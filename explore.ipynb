{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.auto as tqdm\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "from jaxtyping import Float\n",
    "from functools import partial\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE=\"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CTX = 64\n",
    "D_VOCAB = 11  # 10 digits and a comma\n",
    "cfg = transformer_lens.HookedTransformerConfig(\n",
    "    n_layers=1,\n",
    "    d_model=128,\n",
    "    n_ctx=N_CTX,\n",
    "    d_head=64,\n",
    "    n_heads=1,\n",
    "    d_mlp=128,\n",
    "    d_vocab=D_VOCAB,\n",
    "    act_fn=\"relu\",\n",
    "    seed=42,\n",
    "    device=DEVICE,\n",
    "    attn_only=False,\n",
    ")\n",
    "model = transformer_lens.HookedTransformer(cfg, move_to_device=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_state(model: transformer_lens.HookedTransformer, filename: str):\n",
    "    assert os.path.isdir(\"models\"), \"Make a directory `models` with model state dicts\"\n",
    "    if not filename.startswith(\"models/\"):\n",
    "        filename = f\"models/{filename}\"\n",
    "    with open(filename, \"rb\") as f:\n",
    "        state_dict = pickle.load(f)\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "load_model_state(model, \"addition_model_state_dict_2024-06-19T16-18.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(c: str):\n",
    "    return ord(c) - ord(\"0\") if c.isdigit() else 10  # 10 is comma\n",
    "\n",
    "def str_to_tokens(seq_str):\n",
    "    return torch.tensor([tokenize(c) for c in seq_str], device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "         1,  2,  3,  4, 10,  0,  3,  7,  3, 10], device='cuda:0')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_example = \"1234,0373,\"\n",
    "example_toks = str_to_tokens(test_example)\n",
    "if len(example_toks) < cfg.n_ctx:\n",
    "    example_toks = F.pad(example_toks, (cfg.n_ctx - len(example_toks), 0), value=10)\n",
    "example_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1], device='cuda:0')\n",
      "tensor([6], device='cuda:0')\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([7], device='cuda:0')\n",
      "tensor([8], device='cuda:0')\n",
      "tensor([2], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "logits = model(example_toks)[:, -1, :]\n",
    "pred = logits.argmax(dim=-1)\n",
    "print(pred)\n",
    "\n",
    "for _ in range(5):\n",
    "    example_toks = torch.cat([example_toks[1:], pred], dim=0)\n",
    "    logits = model(example_toks)[:, -1, :]\n",
    "    pred = logits.argmax(dim=-1)\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
