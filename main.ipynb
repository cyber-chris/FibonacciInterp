{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import transformer_lens\n",
    "except:\n",
    "    %pip install git+https://github.com/neelnanda-io/TransformerLens\n",
    "    %pip install circuitsvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.auto as tqdm\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from jaxtyping import Float\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: scale up model, it's struggling to learn\n",
    "N_CTX = 128\n",
    "D_VOCAB = 11 # 10 digits and a comma\n",
    "cfg = transformer_lens.HookedTransformerConfig(\n",
    "    n_layers=1,\n",
    "    d_model=128,\n",
    "    n_ctx=N_CTX,\n",
    "    d_head=32,\n",
    "    n_heads=1,\n",
    "    d_mlp=None,\n",
    "    d_vocab=D_VOCAB,\n",
    "    act_fn=\"relu\",\n",
    "    seed=42,\n",
    "    device=DEVICE,\n",
    "    attn_only=True,\n",
    ")\n",
    "model = transformer_lens.HookedTransformer(cfg, move_to_device=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([803, 128]) torch.Size([803, 128])\n",
      "torch.Size([3, 128]) torch.Size([3, 128])\n"
     ]
    }
   ],
   "source": [
    "# data generation\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def fib_sequence(seq_length: int, a=0, b=1) -> np.ndarray:\n",
    "    output = [a, b]\n",
    "    while len(output) < seq_length:\n",
    "        output.append(output[-2] + output[-1])\n",
    "    for x in reversed(output):\n",
    "        if x > 9_223_372_036_854_775_808:\n",
    "            print(\"Warning large num\", a, b, x)\n",
    "        else:\n",
    "            break\n",
    "    return np.array(output)\n",
    "\n",
    "\n",
    "def tokenize(c: str):\n",
    "    return ord(c) - ord(\"0\") if c.isdigit() else 10  # 10 is comma\n",
    "\n",
    "\n",
    "def untokenize(toks) -> str:\n",
    "    return \"\".join([chr(tok + ord(\"0\")) if tok < 10 else \",\" for tok in toks])\n",
    "\n",
    "\n",
    "def seq_to_tokens(seq: np.ndarray) -> torch.Tensor:\n",
    "    seq_str = \",\".join(seq.astype(str))\n",
    "    return torch.tensor([tokenize(c) for c in seq_str], device=DEVICE)\n",
    "\n",
    "\n",
    "def generate_data(sequences: np.ndarray):\n",
    "    # generates x,y items from sequences\n",
    "    X, y = [], []\n",
    "    for seq in sequences:\n",
    "        # This could be a suboptimal way to do it but we're just showing it the next digit.\n",
    "        # e.g. for x = 1,1,2,3,5,8,12, y=2 (rather than 20)\n",
    "        tokens = seq_to_tokens(seq)\n",
    "        x_width = cfg.n_ctx\n",
    "        if len(tokens) < x_width + 1:\n",
    "            raise Exception(len(tokens))\n",
    "        \n",
    "        chunks = tokens.split(x_width + 1)\n",
    "        for chunk in chunks:\n",
    "            if len(chunk) < x_width + 1:\n",
    "                continue\n",
    "            curr_x = chunk[:-1]\n",
    "            curr_y = chunk[1:]\n",
    "            X.append(curr_x)\n",
    "            y.append(curr_y)\n",
    "\n",
    "        last_x = tokens[len(tokens)-(x_width+1):-1]\n",
    "        last_y = tokens[len(tokens)-(x_width+1)+1:]\n",
    "        X.append(last_x)\n",
    "        y.append(last_y)\n",
    "    return torch.row_stack(X), torch.row_stack(y)\n",
    "\n",
    "\n",
    "# We're not really trying to test it's ability to add, so avoiding long sequences.\n",
    "# Instead trying different start points to get more train data.\n",
    "raw_sequences = [fib_sequence(40, a=a, b=b) for a in range(0, 25) for b in range(a+1, 25)]\n",
    "sequences = np.row_stack([raw_sequences])\n",
    "X_train, y_train = generate_data(sequences=sequences)\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "val_sequences = np.row_stack([fib_sequence(40, a=a, b=a + 1) for a in range(25, 26)])\n",
    "X_val, y_val = generate_data(sequences=val_sequences)\n",
    "print(X_val.shape, y_val.shape)\n",
    "\n",
    "train_dataloader = DataLoader(TensorDataset(X_train, y_train), batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,1,2,3,5,8,13,21,34,55,89,144,233,377,610,987,1597,2584,4181,6765,10946,17711,28657,46368,75025,121393,196418,317811,514229,8 predicts ,1,1,2,3,5,8,13,21,34,55,89,144,233,377,610,987,1597,2584,4181,6765,10946,17711,28657,46368,75025,121393,196418,317811,514229,83\n",
      ",28657,46368,75025,121393,196418,317811,514229,832040,1346269,2178309,3524578,5702887,9227465,14930352,24157817,39088169,6324598 predicts 28657,46368,75025,121393,196418,317811,514229,832040,1346269,2178309,3524578,5702887,9227465,14930352,24157817,39088169,63245986\n",
      "0,2,2,4,6,10,16,26,42,68,110,178,288,466,754,1220,1974,3194,5168,8362,13530,21892,35422,57314,92736,150050,242786,392836,635622, predicts ,2,2,4,6,10,16,26,42,68,110,178,288,466,754,1220,1974,3194,5168,8362,13530,21892,35422,57314,92736,150050,242786,392836,635622,1\n",
      ",92736,150050,242786,392836,635622,1028458,1664080,2692538,4356618,7049156,11405774,18454930,29860704,48315634,78176338,12649197 predicts 92736,150050,242786,392836,635622,1028458,1664080,2692538,4356618,7049156,11405774,18454930,29860704,48315634,78176338,126491972\n",
      "0,3,3,6,9,15,24,39,63,102,165,267,432,699,1131,1830,2961,4791,7752,12543,20295,32838,53133,85971,139104,225075,364179,589254,953 predicts ,3,3,6,9,15,24,39,63,102,165,267,432,699,1131,1830,2961,4791,7752,12543,20295,32838,53133,85971,139104,225075,364179,589254,9534\n",
      "9104,225075,364179,589254,953433,1542687,2496120,4038807,6534927,10573734,17108661,27682395,44791056,72473451,117264507,18973795 predicts 104,225075,364179,589254,953433,1542687,2496120,4038807,6534927,10573734,17108661,27682395,44791056,72473451,117264507,189737958\n",
      "0,4,4,8,12,20,32,52,84,136,220,356,576,932,1508,2440,3948,6388,10336,16724,27060,43784,70844,114628,185472,300100,485572,785672, predicts ,4,4,8,12,20,32,52,84,136,220,356,576,932,1508,2440,3948,6388,10336,16724,27060,43784,70844,114628,185472,300100,485572,785672,1\n",
      "472,300100,485572,785672,1271244,2056916,3328160,5385076,8713236,14098312,22811548,36909860,59721408,96631268,156352676,25298394 predicts 72,300100,485572,785672,1271244,2056916,3328160,5385076,8713236,14098312,22811548,36909860,59721408,96631268,156352676,252983944\n",
      "0,5,5,10,15,25,40,65,105,170,275,445,720,1165,1885,3050,4935,7985,12920,20905,33825,54730,88555,143285,231840,375125,606965,9820 predicts ,5,5,10,15,25,40,65,105,170,275,445,720,1165,1885,3050,4935,7985,12920,20905,33825,54730,88555,143285,231840,375125,606965,98209\n",
      "0,375125,606965,982090,1589055,2571145,4160200,6731345,10891545,17622890,28514435,46137325,74651760,120789085,195440845,31622993 predicts ,375125,606965,982090,1589055,2571145,4160200,6731345,10891545,17622890,28514435,46137325,74651760,120789085,195440845,316229930\n",
      "0,6,6,12,18,30,48,78,126,204,330,534,864,1398,2262,3660,5922,9582,15504,25086,40590,65676,106266,171942,278208,450150,728358,117 predicts ,6,6,12,18,30,48,78,126,204,330,534,864,1398,2262,3660,5922,9582,15504,25086,40590,65676,106266,171942,278208,450150,728358,1178\n",
      ",450150,728358,1178508,1906866,3085374,4992240,8077614,13069854,21147468,34217322,55364790,89582112,144946902,234529014,37947591 predicts 450150,728358,1178508,1906866,3085374,4992240,8077614,13069854,21147468,34217322,55364790,89582112,144946902,234529014,379475916\n",
      "0,7,7,14,21,35,56,91,147,238,385,623,1008,1631,2639,4270,6909,11179,18088,29267,47355,76622,123977,200599,324576,525175,849751,1 predicts ,7,7,14,21,35,56,91,147,238,385,623,1008,1631,2639,4270,6909,11179,18088,29267,47355,76622,123977,200599,324576,525175,849751,13\n",
      "525175,849751,1374926,2224677,3599603,5824280,9423883,15248163,24672046,39920209,64592255,104512464,169104719,273617183,44272190 predicts 25175,849751,1374926,2224677,3599603,5824280,9423883,15248163,24672046,39920209,64592255,104512464,169104719,273617183,442721902\n",
      "0,8,8,16,24,40,64,104,168,272,440,712,1152,1864,3016,4880,7896,12776,20672,33448,54120,87568,141688,229256,370944,600200,971144, predicts ,8,8,16,24,40,64,104,168,272,440,712,1152,1864,3016,4880,7896,12776,20672,33448,54120,87568,141688,229256,370944,600200,971144,1\n",
      "00200,971144,1571344,2542488,4113832,6656320,10770152,17426472,28196624,45623096,73819720,119442816,193262536,312705352,50596788 predicts 0200,971144,1571344,2542488,4113832,6656320,10770152,17426472,28196624,45623096,73819720,119442816,193262536,312705352,505967888\n",
      "0,9,9,18,27,45,72,117,189,306,495,801,1296,2097,3393,5490,8883,14373,23256,37629,60885,98514,159399,257913,417312,675225,1092537 predicts ,9,9,18,27,45,72,117,189,306,495,801,1296,2097,3393,5490,8883,14373,23256,37629,60885,98514,159399,257913,417312,675225,1092537,\n",
      "5225,1092537,1767762,2860299,4628061,7488360,12116421,19604781,31721202,51325983,83047185,134373168,217420353,351793521,56921387 predicts 225,1092537,1767762,2860299,4628061,7488360,12116421,19604781,31721202,51325983,83047185,134373168,217420353,351793521,569213874\n",
      "0,10,10,20,30,50,80,130,210,340,550,890,1440,2330,3770,6100,9870,15970,25840,41810,67650,109460,177110,286570,463680,750250,1213 predicts ,10,10,20,30,50,80,130,210,340,550,890,1440,2330,3770,6100,9870,15970,25840,41810,67650,109460,177110,286570,463680,750250,12139\n",
      "0250,1213930,1964180,3178110,5142290,8320400,13462690,21783090,35245780,57028870,92274650,149303520,241578170,390881690,63245986 predicts 250,1213930,1964180,3178110,5142290,8320400,13462690,21783090,35245780,57028870,92274650,149303520,241578170,390881690,632459860\n",
      "0,11,11,22,33,55,88,143,231,374,605,979,1584,2563,4147,6710,10857,17567,28424,45991,74415,120406,194821,315227,510048,825275,133 predicts ,11,11,22,33,55,88,143,231,374,605,979,1584,2563,4147,6710,10857,17567,28424,45991,74415,120406,194821,315227,510048,825275,1335\n",
      "275,1335323,2160598,3495921,5656519,9152440,14808959,23961399,38770358,62731757,101502115,164233872,265735987,429969859,69570584 predicts 75,1335323,2160598,3495921,5656519,9152440,14808959,23961399,38770358,62731757,101502115,164233872,265735987,429969859,695705846\n",
      "0,12,12,24,36,60,96,156,252,408,660,1068,1728,2796,4524,7320,11844,19164,31008,50172,81180,131352,212532,343884,556416,900300,14 predicts ,12,12,24,36,60,96,156,252,408,660,1068,1728,2796,4524,7320,11844,19164,31008,50172,81180,131352,212532,343884,556416,900300,145\n",
      "300,1456716,2357016,3813732,6170748,9984480,16155228,26139708,42294936,68434644,110729580,179164224,289893804,469058028,75895183 predicts 00,1456716,2357016,3813732,6170748,9984480,16155228,26139708,42294936,68434644,110729580,179164224,289893804,469058028,758951832\n",
      "0,13,13,26,39,65,104,169,273,442,715,1157,1872,3029,4901,7930,12831,20761,33592,54353,87945,142298,230243,372541,602784,975325,1 predicts ,13,13,26,39,65,104,169,273,442,715,1157,1872,3029,4901,7930,12831,20761,33592,54353,87945,142298,230243,372541,602784,975325,15\n",
      "25,1578109,2553434,4131543,6684977,10816520,17501497,28318017,45819514,74137531,119957045,194094576,314051621,508146197,82219781 predicts 5,1578109,2553434,4131543,6684977,10816520,17501497,28318017,45819514,74137531,119957045,194094576,314051621,508146197,822197818\n",
      "0,14,14,28,42,70,112,182,294,476,770,1246,2016,3262,5278,8540,13818,22358,36176,58534,94710,153244,247954,401198,649152,1050350, predicts ,14,14,28,42,70,112,182,294,476,770,1246,2016,3262,5278,8540,13818,22358,36176,58534,94710,153244,247954,401198,649152,1050350,1\n",
      "50,1699502,2749852,4449354,7199206,11648560,18847766,30496326,49344092,79840418,129184510,209024928,338209438,547234366,88544380 predicts 0,1699502,2749852,4449354,7199206,11648560,18847766,30496326,49344092,79840418,129184510,209024928,338209438,547234366,885443804\n",
      "0,15,15,30,45,75,120,195,315,510,825,1335,2160,3495,5655,9150,14805,23955,38760,62715,101475,164190,265665,429855,695520,1125375 predicts ,15,15,30,45,75,120,195,315,510,825,1335,2160,3495,5655,9150,14805,23955,38760,62715,101475,164190,265665,429855,695520,1125375,\n",
      "75,1820895,2946270,4767165,7713435,12480600,20194035,32674635,52868670,85543305,138411975,223955280,362367255,586322535,94868979 predicts 5,1820895,2946270,4767165,7713435,12480600,20194035,32674635,52868670,85543305,138411975,223955280,362367255,586322535,948689790\n",
      "0,16,16,32,48,80,128,208,336,544,880,1424,2304,3728,6032,9760,15792,25552,41344,66896,108240,175136,283376,458512,741888,1200400 predicts ,16,16,32,48,80,128,208,336,544,880,1424,2304,3728,6032,9760,15792,25552,41344,66896,108240,175136,283376,458512,741888,1200400,\n",
      "0,1942288,3142688,5084976,8227664,13312640,21540304,34852944,56393248,91246192,147639440,238885632,386525072,625410704,101193577 predicts ,1942288,3142688,5084976,8227664,13312640,21540304,34852944,56393248,91246192,147639440,238885632,386525072,625410704,1011935776\n",
      "0,17,17,34,51,85,136,221,357,578,935,1513,2448,3961,6409,10370,16779,27149,43928,71077,115005,186082,301087,487169,788256,127542 predicts ,17,17,34,51,85,136,221,357,578,935,1513,2448,3961,6409,10370,16779,27149,43928,71077,115005,186082,301087,487169,788256,1275425\n",
      "5,2063681,3339106,5402787,8741893,14144680,22886573,37031253,59917826,96949079,156866905,253815984,410682889,664498873,107518176 predicts ,2063681,3339106,5402787,8741893,14144680,22886573,37031253,59917826,96949079,156866905,253815984,410682889,664498873,1075181762\n",
      "0,18,18,36,54,90,144,234,378,612,990,1602,2592,4194,6786,10980,17766,28746,46512,75258,121770,197028,318798,515826,834624,135045 predicts ,18,18,36,54,90,144,234,378,612,990,1602,2592,4194,6786,10980,17766,28746,46512,75258,121770,197028,318798,515826,834624,1350450\n",
      ",2185074,3535524,5720598,9256122,14976720,24232842,39209562,63442404,102651966,166094370,268746336,434840706,703587042,113842774 predicts 2185074,3535524,5720598,9256122,14976720,24232842,39209562,63442404,102651966,166094370,268746336,434840706,703587042,1138427748\n",
      ",2185074,3535524,5720598,9256122,14976720,24232842,39209562,63442404,102651966,166094370,268746336,434840706,703587042,113842774 predicts 2185074,3535524,5720598,9256122,14976720,24232842,39209562,63442404,102651966,166094370,268746336,434840706,703587042,1138427748\n",
      "0,19,19,38,57,95,152,247,399,646,1045,1691,2736,4427,7163,11590,18753,30343,49096,79439,128535,207974,336509,544483,880992,14254 predicts ,19,19,38,57,95,152,247,399,646,1045,1691,2736,4427,7163,11590,18753,30343,49096,79439,128535,207974,336509,544483,880992,142547\n",
      "5,2306467,3731942,6038409,9770351,15808760,25579111,41387871,66966982,108354853,175321835,283676688,458998523,742675211,12016737 predicts ,2306467,3731942,6038409,9770351,15808760,25579111,41387871,66966982,108354853,175321835,283676688,458998523,742675211,120167373\n",
      ",2306467,3731942,6038409,9770351,15808760,25579111,41387871,66966982,108354853,175321835,283676688,458998523,742675211,120167373 predicts 2306467,3731942,6038409,9770351,15808760,25579111,41387871,66966982,108354853,175321835,283676688,458998523,742675211,1201673734\n",
      "0,20,20,40,60,100,160,260,420,680,1100,1780,2880,4660,7540,12200,19740,31940,51680,83620,135300,218920,354220,573140,927360,1500 predicts ,20,20,40,60,100,160,260,420,680,1100,1780,2880,4660,7540,12200,19740,31940,51680,83620,135300,218920,354220,573140,927360,15005\n",
      "00,2427860,3928360,6356220,10284580,16640800,26925380,43566180,70491560,114057740,184549300,298607040,483156340,781763380,126491 predicts 0,2427860,3928360,6356220,10284580,16640800,26925380,43566180,70491560,114057740,184549300,298607040,483156340,781763380,1264919\n",
      "2427860,3928360,6356220,10284580,16640800,26925380,43566180,70491560,114057740,184549300,298607040,483156340,781763380,126491972 predicts 427860,3928360,6356220,10284580,16640800,26925380,43566180,70491560,114057740,184549300,298607040,483156340,781763380,1264919720\n",
      "0,21,21,42,63,105,168,273,441,714,1155,1869,3024,4893,7917,12810,20727,33537,54264,87801,142065,229866,371931,601797,973728,1575 predicts ,21,21,42,63,105,168,273,441,714,1155,1869,3024,4893,7917,12810,20727,33537,54264,87801,142065,229866,371931,601797,973728,15755\n",
      "25,2549253,4124778,6674031,10798809,17472840,28271649,45744489,74016138,119760627,193776765,313537392,507314157,820851549,132816 predicts 5,2549253,4124778,6674031,10798809,17472840,28271649,45744489,74016138,119760627,193776765,313537392,507314157,820851549,1328165\n",
      "2549253,4124778,6674031,10798809,17472840,28271649,45744489,74016138,119760627,193776765,313537392,507314157,820851549,132816570 predicts 549253,4124778,6674031,10798809,17472840,28271649,45744489,74016138,119760627,193776765,313537392,507314157,820851549,1328165706\n",
      "0,22,22,44,66,110,176,286,462,748,1210,1958,3168,5126,8294,13420,21714,35134,56848,91982,148830,240812,389642,630454,1020096,165 predicts ,22,22,44,66,110,176,286,462,748,1210,1958,3168,5126,8294,13420,21714,35134,56848,91982,148830,240812,389642,630454,1020096,1650\n",
      "550,2670646,4321196,6991842,11313038,18304880,29617918,47922798,77540716,125463514,203004230,328467744,531471974,859939718,13914 predicts 50,2670646,4321196,6991842,11313038,18304880,29617918,47922798,77540716,125463514,203004230,328467744,531471974,859939718,139141\n",
      "2670646,4321196,6991842,11313038,18304880,29617918,47922798,77540716,125463514,203004230,328467744,531471974,859939718,139141169 predicts 670646,4321196,6991842,11313038,18304880,29617918,47922798,77540716,125463514,203004230,328467744,531471974,859939718,1391411692\n",
      "0,23,23,46,69,115,184,299,483,782,1265,2047,3312,5359,8671,14030,22701,36731,59432,96163,155595,251758,407353,659111,1066464,172 predicts ,23,23,46,69,115,184,299,483,782,1265,2047,3312,5359,8671,14030,22701,36731,59432,96163,155595,251758,407353,659111,1066464,1725\n",
      "575,2792039,4517614,7309653,11827267,19136920,30964187,50101107,81065294,131166401,212231695,343398096,555629791,899027887,14546 predicts 75,2792039,4517614,7309653,11827267,19136920,30964187,50101107,81065294,131166401,212231695,343398096,555629791,899027887,145465\n",
      "2792039,4517614,7309653,11827267,19136920,30964187,50101107,81065294,131166401,212231695,343398096,555629791,899027887,145465767 predicts 792039,4517614,7309653,11827267,19136920,30964187,50101107,81065294,131166401,212231695,343398096,555629791,899027887,1454657678\n",
      "0,24,24,48,72,120,192,312,504,816,1320,2136,3456,5592,9048,14640,23688,38328,62016,100344,162360,262704,425064,687768,1112832,18 predicts ,24,24,48,72,120,192,312,504,816,1320,2136,3456,5592,9048,14640,23688,38328,62016,100344,162360,262704,425064,687768,1112832,180\n",
      "0600,2913432,4714032,7627464,12341496,19968960,32310456,52279416,84589872,136869288,221459160,358328448,579787608,938116056,1517 predicts 600,2913432,4714032,7627464,12341496,19968960,32310456,52279416,84589872,136869288,221459160,358328448,579787608,938116056,15179\n",
      "2913432,4714032,7627464,12341496,19968960,32310456,52279416,84589872,136869288,221459160,358328448,579787608,938116056,151790366 predicts 913432,4714032,7627464,12341496,19968960,32310456,52279416,84589872,136869288,221459160,358328448,579787608,938116056,1517903664\n",
      "1,2,3,5,8,13,21,34,55,89,144,233,377,610,987,1597,2584,4181,6765,10946,17711,28657,46368,75025,121393,196418,317811,514229,83204 predicts ,2,3,5,8,13,21,34,55,89,144,233,377,610,987,1597,2584,4181,6765,10946,17711,28657,46368,75025,121393,196418,317811,514229,832040\n",
      "21393,196418,317811,514229,832040,1346269,2178309,3524578,5702887,9227465,14930352,24157817,39088169,63245986,102334155,16558014 predicts 1393,196418,317811,514229,832040,1346269,2178309,3524578,5702887,9227465,14930352,24157817,39088169,63245986,102334155,165580141\n",
      "1,3,4,7,11,18,29,47,76,123,199,322,521,843,1364,2207,3571,5778,9349,15127,24476,39603,64079,103682,167761,271443,439204,710647,1 predicts ,3,4,7,11,18,29,47,76,123,199,322,521,843,1364,2207,3571,5778,9349,15127,24476,39603,64079,103682,167761,271443,439204,710647,11\n",
      "761,271443,439204,710647,1149851,1860498,3010349,4870847,7881196,12752043,20633239,33385282,54018521,87403803,141422324,22882612 predicts 61,271443,439204,710647,1149851,1860498,3010349,4870847,7881196,12752043,20633239,33385282,54018521,87403803,141422324,228826127\n",
      "1,4,5,9,14,23,37,60,97,157,254,411,665,1076,1741,2817,4558,7375,11933,19308,31241,50549,81790,132339,214129,346468,560597,907065 predicts ,4,5,9,14,23,37,60,97,157,254,411,665,1076,1741,2817,4558,7375,11933,19308,31241,50549,81790,132339,214129,346468,560597,907065,\n",
      "9,346468,560597,907065,1467662,2374727,3842389,6217116,10059505,16276621,26336126,42612747,68948873,111561620,180510493,29207211 predicts ,346468,560597,907065,1467662,2374727,3842389,6217116,10059505,16276621,26336126,42612747,68948873,111561620,180510493,292072113\n",
      "1,5,6,11,17,28,45,73,118,191,309,500,809,1309,2118,3427,5545,8972,14517,23489,38006,61495,99501,160996,260497,421493,681990,1103 predicts ,5,6,11,17,28,45,73,118,191,309,500,809,1309,2118,3427,5545,8972,14517,23489,38006,61495,99501,160996,260497,421493,681990,11034\n",
      ",421493,681990,1103483,1785473,2888956,4674429,7563385,12237814,19801199,32039013,51840212,83879225,135719437,219598662,35531809 predicts 421493,681990,1103483,1785473,2888956,4674429,7563385,12237814,19801199,32039013,51840212,83879225,135719437,219598662,355318099\n",
      "1,6,7,13,20,33,53,86,139,225,364,589,953,1542,2495,4037,6532,10569,17101,27670,44771,72441,117212,189653,306865,496518,803383,12 predicts ,6,7,13,20,33,53,86,139,225,364,589,953,1542,2495,4037,6532,10569,17101,27670,44771,72441,117212,189653,306865,496518,803383,129\n"
     ]
    }
   ],
   "source": [
    "# Training examples\n",
    "for X, y in train_dataloader:\n",
    "    for b, seq in enumerate(X):\n",
    "        print(f\"{untokenize(seq)} predicts {untokenize(y[b])}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function (cross entropy loss)\n",
    "\n",
    "def loss_fn(\n",
    "    logits: torch.Tensor,  # [batch, pos, d_vocab]\n",
    "    targets: torch.Tensor,  # [batch, pos]\n",
    "):\n",
    "    B, P, _ = logits.shape\n",
    "    B, P = targets.shape\n",
    "    return torch.nn.functional.cross_entropy(logits.view(B*P, -1), targets.view(B*P))\n",
    "\n",
    "def accuracy(\n",
    "    logits: torch.Tensor,  # [batch, pos, d_vocab]\n",
    "    targets: torch.Tensor,  # [batch, pos]\n",
    "):\n",
    "    # using this as accuracy of the batch\n",
    "    probs = logits.softmax(dim=-1)\n",
    "    predictions = probs.argmax(dim=-1)\n",
    "    assert predictions.shape == targets.shape\n",
    "    acc = (predictions == targets).float().mean()\n",
    "    assert 0 <= acc <= 1\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0) loss = 2.7002, batch accuracy = 0.0938720703125, val acc = 0.1171875\n",
      "(1) loss = 2.3354, batch accuracy = 0.192138671875, val acc = 0.125\n",
      "(2) loss = 2.2961, batch accuracy = 0.199951171875, val acc = 0.1510416716337204\n",
      "(3) loss = 2.2729, batch accuracy = 0.2001953125, val acc = 0.15625\n",
      "(4) loss = 2.2573, batch accuracy = 0.202392578125, val acc = 0.1588541716337204\n",
      "(5) loss = 2.2518, batch accuracy = 0.2066650390625, val acc = 0.1432291716337204\n",
      "(6) loss = 2.2460, batch accuracy = 0.2099609375, val acc = 0.1458333432674408\n",
      "(7) loss = 2.2373, batch accuracy = 0.208984375, val acc = 0.1510416716337204\n",
      "(8) loss = 2.2257, batch accuracy = 0.213134765625, val acc = 0.1510416716337204\n",
      "(9) loss = 2.2157, batch accuracy = 0.216552734375, val acc = 0.15625\n",
      "(10) loss = 2.2093, batch accuracy = 0.21923828125, val acc = 0.15625\n",
      "(11) loss = 2.1929, batch accuracy = 0.2303466796875, val acc = 0.1614583432674408\n",
      "(12) loss = 2.1774, batch accuracy = 0.230224609375, val acc = 0.1692708432674408\n",
      "(13) loss = 2.1682, batch accuracy = 0.234619140625, val acc = 0.1666666716337204\n",
      "(14) loss = 2.1605, batch accuracy = 0.2398681640625, val acc = 0.1848958432674408\n",
      "(15) loss = 2.1537, batch accuracy = 0.24658203125, val acc = 0.1822916716337204\n",
      "(16) loss = 2.1555, batch accuracy = 0.2509765625, val acc = 0.1875\n",
      "(17) loss = 2.1516, batch accuracy = 0.25, val acc = 0.1927083432674408\n",
      "(18) loss = 2.1397, batch accuracy = 0.254150390625, val acc = 0.1875\n",
      "(19) loss = 2.1346, batch accuracy = 0.2535400390625, val acc = 0.1848958432674408\n",
      "(20) loss = 2.1219, batch accuracy = 0.256591796875, val acc = 0.1901041716337204\n",
      "(21) loss = 2.1253, batch accuracy = 0.2545166015625, val acc = 0.1979166716337204\n",
      "(22) loss = 2.1268, batch accuracy = 0.2545166015625, val acc = 0.2005208432674408\n",
      "(23) loss = 2.1186, batch accuracy = 0.25732421875, val acc = 0.1848958432674408\n",
      "(24) loss = 2.1112, batch accuracy = 0.2607421875, val acc = 0.1901041716337204\n",
      "(25) loss = 2.1015, batch accuracy = 0.2591552734375, val acc = 0.1822916716337204\n",
      "(26) loss = 2.1078, batch accuracy = 0.2615966796875, val acc = 0.1979166716337204\n",
      "(27) loss = 2.0961, batch accuracy = 0.2630615234375, val acc = 0.1875\n",
      "(28) loss = 2.1091, batch accuracy = 0.264404296875, val acc = 0.2005208432674408\n",
      "(29) loss = 2.0914, batch accuracy = 0.26513671875, val acc = 0.1901041716337204\n",
      "(30) loss = 2.0950, batch accuracy = 0.265869140625, val acc = 0.2005208432674408\n",
      "(31) loss = 2.0819, batch accuracy = 0.267578125, val acc = 0.203125\n",
      "(32) loss = 2.1003, batch accuracy = 0.26513671875, val acc = 0.203125\n",
      "(33) loss = 2.0824, batch accuracy = 0.2698974609375, val acc = 0.2005208432674408\n",
      "(34) loss = 2.0975, batch accuracy = 0.263916015625, val acc = 0.1901041716337204\n",
      "(35) loss = 2.0779, batch accuracy = 0.2703857421875, val acc = 0.1979166716337204\n",
      "(36) loss = 2.0838, batch accuracy = 0.26611328125, val acc = 0.1848958432674408\n",
      "(37) loss = 2.0716, batch accuracy = 0.2718505859375, val acc = 0.2005208432674408\n",
      "(38) loss = 2.0792, batch accuracy = 0.26904296875, val acc = 0.1953125\n",
      "(39) loss = 2.0632, batch accuracy = 0.274169921875, val acc = 0.203125\n",
      "(40) loss = 2.0698, batch accuracy = 0.2723388671875, val acc = 0.203125\n",
      "(41) loss = 2.0605, batch accuracy = 0.273193359375, val acc = 0.2005208432674408\n",
      "(42) loss = 2.0707, batch accuracy = 0.2703857421875, val acc = 0.2005208432674408\n",
      "(43) loss = 2.0535, batch accuracy = 0.2767333984375, val acc = 0.2057291716337204\n",
      "(44) loss = 2.0663, batch accuracy = 0.2698974609375, val acc = 0.1901041716337204\n",
      "(45) loss = 2.0483, batch accuracy = 0.2786865234375, val acc = 0.2057291716337204\n",
      "(46) loss = 2.0597, batch accuracy = 0.2733154296875, val acc = 0.1875\n",
      "(47) loss = 2.0464, batch accuracy = 0.2783203125, val acc = 0.2161458432674408\n",
      "(48) loss = 2.0587, batch accuracy = 0.27490234375, val acc = 0.2083333432674408\n",
      "(49) loss = 2.0545, batch accuracy = 0.2742919921875, val acc = 0.1927083432674408\n",
      "(50) loss = 2.0687, batch accuracy = 0.271484375, val acc = 0.1979166716337204\n",
      "(51) loss = 2.0602, batch accuracy = 0.27294921875, val acc = 0.1848958432674408\n",
      "(52) loss = 2.0614, batch accuracy = 0.2735595703125, val acc = 0.1979166716337204\n",
      "(53) loss = 2.0446, batch accuracy = 0.2774658203125, val acc = 0.2057291716337204\n",
      "(54) loss = 2.0468, batch accuracy = 0.27490234375, val acc = 0.1979166716337204\n",
      "(55) loss = 2.0485, batch accuracy = 0.2783203125, val acc = 0.2135416716337204\n",
      "(56) loss = 2.0519, batch accuracy = 0.275390625, val acc = 0.2057291716337204\n",
      "(57) loss = 2.0437, batch accuracy = 0.278564453125, val acc = 0.203125\n",
      "(58) loss = 2.0471, batch accuracy = 0.275634765625, val acc = 0.203125\n",
      "(59) loss = 2.0458, batch accuracy = 0.27685546875, val acc = 0.2083333432674408\n",
      "(60) loss = 2.0426, batch accuracy = 0.2752685546875, val acc = 0.1927083432674408\n",
      "(61) loss = 2.0345, batch accuracy = 0.27783203125, val acc = 0.2083333432674408\n",
      "(62) loss = 2.0343, batch accuracy = 0.27783203125, val acc = 0.1953125\n",
      "(63) loss = 2.0400, batch accuracy = 0.27783203125, val acc = 0.2057291716337204\n",
      "(64) loss = 2.0529, batch accuracy = 0.2813720703125, val acc = 0.2005208432674408\n",
      "(65) loss = 2.0361, batch accuracy = 0.2789306640625, val acc = 0.2161458432674408\n",
      "(66) loss = 2.0404, batch accuracy = 0.277587890625, val acc = 0.2161458432674408\n",
      "(67) loss = 2.0442, batch accuracy = 0.27685546875, val acc = 0.1979166716337204\n",
      "(68) loss = 2.0388, batch accuracy = 0.27783203125, val acc = 0.2083333432674408\n",
      "(69) loss = 2.0311, batch accuracy = 0.28271484375, val acc = 0.1953125\n",
      "(70) loss = 2.0412, batch accuracy = 0.280517578125, val acc = 0.2083333432674408\n",
      "(71) loss = 2.0321, batch accuracy = 0.281005859375, val acc = 0.1953125\n",
      "(72) loss = 2.0319, batch accuracy = 0.2828369140625, val acc = 0.2135416716337204\n",
      "(73) loss = 2.0287, batch accuracy = 0.28125, val acc = 0.1953125\n",
      "(74) loss = 2.0321, batch accuracy = 0.2841796875, val acc = 0.2161458432674408\n",
      "(75) loss = 2.0196, batch accuracy = 0.284423828125, val acc = 0.1979166716337204\n",
      "(76) loss = 2.0316, batch accuracy = 0.283203125, val acc = 0.2109375\n",
      "(77) loss = 2.0322, batch accuracy = 0.28173828125, val acc = 0.1979166716337204\n",
      "(78) loss = 2.0258, batch accuracy = 0.282958984375, val acc = 0.203125\n",
      "(79) loss = 2.0230, batch accuracy = 0.284423828125, val acc = 0.1979166716337204\n",
      "(80) loss = 2.0242, batch accuracy = 0.2864990234375, val acc = 0.2135416716337204\n",
      "(81) loss = 2.0202, batch accuracy = 0.2872314453125, val acc = 0.1979166716337204\n",
      "(82) loss = 2.0237, batch accuracy = 0.282958984375, val acc = 0.2109375\n",
      "(83) loss = 2.0196, batch accuracy = 0.2840576171875, val acc = 0.1927083432674408\n",
      "(84) loss = 2.0332, batch accuracy = 0.284423828125, val acc = 0.2135416716337204\n",
      "(85) loss = 2.0172, batch accuracy = 0.2877197265625, val acc = 0.1979166716337204\n",
      "(86) loss = 2.0240, batch accuracy = 0.288330078125, val acc = 0.2083333432674408\n",
      "(87) loss = 2.0219, batch accuracy = 0.287109375, val acc = 0.1927083432674408\n",
      "(88) loss = 2.0239, batch accuracy = 0.2845458984375, val acc = 0.21875\n",
      "(89) loss = 2.0265, batch accuracy = 0.287109375, val acc = 0.1979166716337204\n",
      "(90) loss = 2.0273, batch accuracy = 0.2864990234375, val acc = 0.2161458432674408\n",
      "(91) loss = 2.0238, batch accuracy = 0.2860107421875, val acc = 0.203125\n",
      "(92) loss = 2.0347, batch accuracy = 0.28759765625, val acc = 0.203125\n",
      "(93) loss = 2.0254, batch accuracy = 0.28466796875, val acc = 0.203125\n",
      "(94) loss = 2.0253, batch accuracy = 0.2861328125, val acc = 0.1979166716337204\n",
      "(95) loss = 2.0235, batch accuracy = 0.287841796875, val acc = 0.203125\n",
      "(96) loss = 2.0111, batch accuracy = 0.2840576171875, val acc = 0.2135416716337204\n",
      "(97) loss = 2.0096, batch accuracy = 0.2933349609375, val acc = 0.2057291716337204\n",
      "(98) loss = 2.0087, batch accuracy = 0.2889404296875, val acc = 0.2109375\n",
      "(99) loss = 2.0032, batch accuracy = 0.2911376953125, val acc = 0.2109375\n",
      "(100) loss = 2.0006, batch accuracy = 0.289794921875, val acc = 0.2213541716337204\n",
      "(101) loss = 1.9991, batch accuracy = 0.2926025390625, val acc = 0.2109375\n",
      "(102) loss = 1.9939, batch accuracy = 0.2908935546875, val acc = 0.2109375\n",
      "(103) loss = 1.9934, batch accuracy = 0.2938232421875, val acc = 0.2109375\n",
      "(104) loss = 2.0063, batch accuracy = 0.291259765625, val acc = 0.2005208432674408\n",
      "(105) loss = 2.0037, batch accuracy = 0.2911376953125, val acc = 0.203125\n",
      "(106) loss = 2.0038, batch accuracy = 0.2869873046875, val acc = 0.203125\n",
      "(107) loss = 2.0091, batch accuracy = 0.2877197265625, val acc = 0.2005208432674408\n",
      "(108) loss = 2.0115, batch accuracy = 0.287841796875, val acc = 0.1927083432674408\n",
      "(109) loss = 2.0084, batch accuracy = 0.287841796875, val acc = 0.2083333432674408\n",
      "(110) loss = 2.0143, batch accuracy = 0.2845458984375, val acc = 0.1979166716337204\n",
      "(111) loss = 2.0181, batch accuracy = 0.2860107421875, val acc = 0.1901041716337204\n",
      "(112) loss = 2.0271, batch accuracy = 0.282958984375, val acc = 0.203125\n",
      "(113) loss = 2.0335, batch accuracy = 0.28271484375, val acc = 0.2083333432674408\n",
      "(114) loss = 2.0196, batch accuracy = 0.2823486328125, val acc = 0.1979166716337204\n",
      "(115) loss = 2.0106, batch accuracy = 0.2874755859375, val acc = 0.203125\n",
      "(116) loss = 2.0081, batch accuracy = 0.2879638671875, val acc = 0.1979166716337204\n",
      "(117) loss = 1.9970, batch accuracy = 0.290771484375, val acc = 0.1953125\n",
      "(118) loss = 1.9964, batch accuracy = 0.291748046875, val acc = 0.2109375\n",
      "(119) loss = 2.0024, batch accuracy = 0.28857421875, val acc = 0.1901041716337204\n",
      "(120) loss = 2.0080, batch accuracy = 0.2933349609375, val acc = 0.2057291716337204\n",
      "(121) loss = 1.9977, batch accuracy = 0.289306640625, val acc = 0.2005208432674408\n",
      "(122) loss = 2.0047, batch accuracy = 0.2919921875, val acc = 0.2083333432674408\n",
      "(123) loss = 2.0041, batch accuracy = 0.2894287109375, val acc = 0.2057291716337204\n",
      "(124) loss = 2.0063, batch accuracy = 0.2911376953125, val acc = 0.2057291716337204\n",
      "(125) loss = 1.9953, batch accuracy = 0.290283203125, val acc = 0.1927083432674408\n",
      "(126) loss = 2.0104, batch accuracy = 0.2900390625, val acc = 0.1901041716337204\n",
      "(127) loss = 2.0012, batch accuracy = 0.2886962890625, val acc = 0.1953125\n",
      "(128) loss = 2.0002, batch accuracy = 0.2940673828125, val acc = 0.1875\n",
      "(129) loss = 2.0005, batch accuracy = 0.2906494140625, val acc = 0.2057291716337204\n",
      "(130) loss = 1.9923, batch accuracy = 0.2938232421875, val acc = 0.1927083432674408\n",
      "(131) loss = 1.9975, batch accuracy = 0.2926025390625, val acc = 0.203125\n",
      "(132) loss = 1.9934, batch accuracy = 0.2891845703125, val acc = 0.1927083432674408\n",
      "(133) loss = 1.9980, batch accuracy = 0.29345703125, val acc = 0.1901041716337204\n",
      "(134) loss = 1.9926, batch accuracy = 0.29248046875, val acc = 0.2005208432674408\n",
      "(135) loss = 1.9953, batch accuracy = 0.289794921875, val acc = 0.2057291716337204\n",
      "(136) loss = 1.9938, batch accuracy = 0.29345703125, val acc = 0.1953125\n",
      "(137) loss = 1.9870, batch accuracy = 0.2939453125, val acc = 0.1927083432674408\n",
      "(138) loss = 1.9882, batch accuracy = 0.2938232421875, val acc = 0.1848958432674408\n",
      "(139) loss = 1.9948, batch accuracy = 0.29443359375, val acc = 0.1901041716337204\n",
      "(140) loss = 1.9804, batch accuracy = 0.2947998046875, val acc = 0.2005208432674408\n",
      "(141) loss = 1.9974, batch accuracy = 0.295166015625, val acc = 0.1953125\n",
      "(142) loss = 1.9840, batch accuracy = 0.2939453125, val acc = 0.1927083432674408\n",
      "(143) loss = 1.9871, batch accuracy = 0.2957763671875, val acc = 0.1796875\n",
      "(144) loss = 1.9855, batch accuracy = 0.29736328125, val acc = 0.2005208432674408\n",
      "(145) loss = 1.9853, batch accuracy = 0.2939453125, val acc = 0.1901041716337204\n",
      "(146) loss = 1.9807, batch accuracy = 0.29345703125, val acc = 0.1979166716337204\n",
      "(147) loss = 1.9794, batch accuracy = 0.2933349609375, val acc = 0.1927083432674408\n",
      "(148) loss = 1.9822, batch accuracy = 0.2957763671875, val acc = 0.1953125\n",
      "(149) loss = 1.9920, batch accuracy = 0.296875, val acc = 0.1901041716337204\n",
      "(150) loss = 1.9811, batch accuracy = 0.2950439453125, val acc = 0.1927083432674408\n",
      "(151) loss = 1.9885, batch accuracy = 0.29541015625, val acc = 0.1927083432674408\n",
      "(152) loss = 1.9814, batch accuracy = 0.2938232421875, val acc = 0.203125\n",
      "(153) loss = 1.9876, batch accuracy = 0.2943115234375, val acc = 0.1979166716337204\n",
      "(154) loss = 1.9861, batch accuracy = 0.292724609375, val acc = 0.2005208432674408\n",
      "(155) loss = 1.9875, batch accuracy = 0.2911376953125, val acc = 0.1875\n",
      "(156) loss = 1.9836, batch accuracy = 0.295654296875, val acc = 0.2005208432674408\n",
      "(157) loss = 1.9802, batch accuracy = 0.2935791015625, val acc = 0.1979166716337204\n",
      "(158) loss = 1.9740, batch accuracy = 0.2960205078125, val acc = 0.2135416716337204\n",
      "(159) loss = 1.9812, batch accuracy = 0.295654296875, val acc = 0.1848958432674408\n",
      "(160) loss = 1.9682, batch accuracy = 0.29638671875, val acc = 0.2005208432674408\n",
      "(161) loss = 1.9832, batch accuracy = 0.2938232421875, val acc = 0.1822916716337204\n",
      "(162) loss = 1.9716, batch accuracy = 0.2955322265625, val acc = 0.2005208432674408\n",
      "(163) loss = 1.9818, batch accuracy = 0.29443359375, val acc = 0.1979166716337204\n",
      "(164) loss = 1.9775, batch accuracy = 0.2947998046875, val acc = 0.1953125\n",
      "(165) loss = 1.9757, batch accuracy = 0.2979736328125, val acc = 0.2005208432674408\n",
      "(166) loss = 1.9763, batch accuracy = 0.2967529296875, val acc = 0.203125\n",
      "(167) loss = 1.9761, batch accuracy = 0.296630859375, val acc = 0.1901041716337204\n",
      "(168) loss = 1.9774, batch accuracy = 0.29638671875, val acc = 0.1953125\n",
      "(169) loss = 1.9737, batch accuracy = 0.2957763671875, val acc = 0.1953125\n",
      "(170) loss = 1.9762, batch accuracy = 0.2940673828125, val acc = 0.203125\n",
      "(171) loss = 1.9874, batch accuracy = 0.2930908203125, val acc = 0.1953125\n",
      "(172) loss = 1.9788, batch accuracy = 0.2935791015625, val acc = 0.203125\n",
      "(173) loss = 1.9899, batch accuracy = 0.29345703125, val acc = 0.1953125\n",
      "(174) loss = 1.9927, batch accuracy = 0.2901611328125, val acc = 0.203125\n",
      "(175) loss = 1.9926, batch accuracy = 0.293212890625, val acc = 0.1901041716337204\n",
      "(176) loss = 1.9786, batch accuracy = 0.292724609375, val acc = 0.1953125\n",
      "(177) loss = 1.9897, batch accuracy = 0.293212890625, val acc = 0.2005208432674408\n",
      "(178) loss = 1.9801, batch accuracy = 0.2930908203125, val acc = 0.1901041716337204\n",
      "(179) loss = 2.0022, batch accuracy = 0.2939453125, val acc = 0.1848958432674408\n",
      "(180) loss = 2.0005, batch accuracy = 0.2896728515625, val acc = 0.1848958432674408\n",
      "(181) loss = 1.9938, batch accuracy = 0.292724609375, val acc = 0.1822916716337204\n",
      "(182) loss = 1.9858, batch accuracy = 0.288818359375, val acc = 0.1796875\n",
      "(183) loss = 1.9809, batch accuracy = 0.2967529296875, val acc = 0.1822916716337204\n",
      "(184) loss = 1.9820, batch accuracy = 0.2947998046875, val acc = 0.1953125\n",
      "(185) loss = 1.9743, batch accuracy = 0.2957763671875, val acc = 0.1848958432674408\n",
      "(186) loss = 1.9731, batch accuracy = 0.296142578125, val acc = 0.1953125\n",
      "(187) loss = 1.9746, batch accuracy = 0.29443359375, val acc = 0.1901041716337204\n",
      "(188) loss = 1.9794, batch accuracy = 0.2969970703125, val acc = 0.1927083432674408\n",
      "(189) loss = 1.9781, batch accuracy = 0.296142578125, val acc = 0.1848958432674408\n",
      "(190) loss = 1.9732, batch accuracy = 0.2943115234375, val acc = 0.1927083432674408\n",
      "(191) loss = 1.9648, batch accuracy = 0.29833984375, val acc = 0.1901041716337204\n",
      "(192) loss = 1.9662, batch accuracy = 0.3011474609375, val acc = 0.1848958432674408\n",
      "(193) loss = 1.9731, batch accuracy = 0.300537109375, val acc = 0.1953125\n",
      "(194) loss = 1.9748, batch accuracy = 0.2994384765625, val acc = 0.1875\n",
      "(195) loss = 1.9687, batch accuracy = 0.297607421875, val acc = 0.1927083432674408\n",
      "(196) loss = 1.9709, batch accuracy = 0.30029296875, val acc = 0.1796875\n",
      "(197) loss = 1.9688, batch accuracy = 0.2987060546875, val acc = 0.1901041716337204\n",
      "(198) loss = 1.9699, batch accuracy = 0.3023681640625, val acc = 0.1953125\n",
      "(199) loss = 1.9686, batch accuracy = 0.297607421875, val acc = 0.1953125\n",
      "(200) loss = 1.9729, batch accuracy = 0.298828125, val acc = 0.1953125\n",
      "(201) loss = 1.9637, batch accuracy = 0.300048828125, val acc = 0.1901041716337204\n",
      "(202) loss = 1.9644, batch accuracy = 0.30126953125, val acc = 0.1979166716337204\n",
      "(203) loss = 1.9736, batch accuracy = 0.2994384765625, val acc = 0.2005208432674408\n",
      "(204) loss = 1.9731, batch accuracy = 0.301513671875, val acc = 0.1848958432674408\n",
      "(205) loss = 1.9682, batch accuracy = 0.30078125, val acc = 0.2005208432674408\n",
      "(206) loss = 1.9665, batch accuracy = 0.29931640625, val acc = 0.1953125\n",
      "(207) loss = 1.9708, batch accuracy = 0.30029296875, val acc = 0.1953125\n",
      "(208) loss = 1.9702, batch accuracy = 0.29931640625, val acc = 0.2083333432674408\n",
      "(209) loss = 1.9755, batch accuracy = 0.3006591796875, val acc = 0.1927083432674408\n",
      "(210) loss = 1.9840, batch accuracy = 0.2955322265625, val acc = 0.2005208432674408\n",
      "(211) loss = 1.9788, batch accuracy = 0.2957763671875, val acc = 0.2005208432674408\n",
      "(212) loss = 1.9726, batch accuracy = 0.2965087890625, val acc = 0.1796875\n",
      "(213) loss = 1.9725, batch accuracy = 0.300537109375, val acc = 0.1953125\n",
      "(214) loss = 1.9641, batch accuracy = 0.3018798828125, val acc = 0.1901041716337204\n",
      "(215) loss = 1.9657, batch accuracy = 0.2999267578125, val acc = 0.1953125\n",
      "(216) loss = 1.9613, batch accuracy = 0.30078125, val acc = 0.2005208432674408\n",
      "(217) loss = 1.9646, batch accuracy = 0.302734375, val acc = 0.1953125\n",
      "(218) loss = 1.9590, batch accuracy = 0.30078125, val acc = 0.1953125\n",
      "(219) loss = 1.9559, batch accuracy = 0.30126953125, val acc = 0.1953125\n",
      "(220) loss = 1.9665, batch accuracy = 0.2972412109375, val acc = 0.1927083432674408\n",
      "(221) loss = 1.9700, batch accuracy = 0.2998046875, val acc = 0.1927083432674408\n",
      "(222) loss = 1.9724, batch accuracy = 0.29833984375, val acc = 0.1927083432674408\n",
      "(223) loss = 1.9569, batch accuracy = 0.30126953125, val acc = 0.1953125\n",
      "(224) loss = 1.9573, batch accuracy = 0.3011474609375, val acc = 0.2005208432674408\n",
      "(225) loss = 1.9657, batch accuracy = 0.2978515625, val acc = 0.2005208432674408\n",
      "(226) loss = 1.9630, batch accuracy = 0.3021240234375, val acc = 0.1953125\n",
      "(227) loss = 1.9671, batch accuracy = 0.301025390625, val acc = 0.1979166716337204\n",
      "(228) loss = 1.9602, batch accuracy = 0.299560546875, val acc = 0.1901041716337204\n",
      "(229) loss = 1.9637, batch accuracy = 0.3048095703125, val acc = 0.1927083432674408\n",
      "(230) loss = 1.9589, batch accuracy = 0.3001708984375, val acc = 0.2083333432674408\n",
      "(231) loss = 1.9573, batch accuracy = 0.3011474609375, val acc = 0.1848958432674408\n",
      "(232) loss = 1.9513, batch accuracy = 0.301513671875, val acc = 0.2057291716337204\n",
      "(233) loss = 1.9539, batch accuracy = 0.3013916015625, val acc = 0.1848958432674408\n",
      "(234) loss = 1.9602, batch accuracy = 0.30126953125, val acc = 0.2057291716337204\n",
      "(235) loss = 1.9573, batch accuracy = 0.3035888671875, val acc = 0.1822916716337204\n",
      "(236) loss = 1.9575, batch accuracy = 0.2991943359375, val acc = 0.1927083432674408\n",
      "(237) loss = 1.9547, batch accuracy = 0.3018798828125, val acc = 0.1979166716337204\n",
      "(238) loss = 1.9661, batch accuracy = 0.302490234375, val acc = 0.2161458432674408\n",
      "(239) loss = 1.9500, batch accuracy = 0.303955078125, val acc = 0.203125\n",
      "(240) loss = 1.9509, batch accuracy = 0.304443359375, val acc = 0.1953125\n",
      "(241) loss = 1.9544, batch accuracy = 0.3023681640625, val acc = 0.203125\n",
      "(242) loss = 1.9664, batch accuracy = 0.30322265625, val acc = 0.1901041716337204\n",
      "(243) loss = 1.9619, batch accuracy = 0.302734375, val acc = 0.1848958432674408\n",
      "(244) loss = 1.9592, batch accuracy = 0.3031005859375, val acc = 0.1901041716337204\n",
      "(245) loss = 1.9565, batch accuracy = 0.30126953125, val acc = 0.1901041716337204\n",
      "(246) loss = 1.9539, batch accuracy = 0.3033447265625, val acc = 0.2005208432674408\n",
      "(247) loss = 1.9504, batch accuracy = 0.3048095703125, val acc = 0.1927083432674408\n",
      "(248) loss = 1.9588, batch accuracy = 0.30126953125, val acc = 0.1953125\n",
      "(249) loss = 1.9535, batch accuracy = 0.301513671875, val acc = 0.203125\n",
      "(250) loss = 1.9507, batch accuracy = 0.3028564453125, val acc = 0.1979166716337204\n",
      "(251) loss = 1.9430, batch accuracy = 0.30517578125, val acc = 0.1875\n",
      "(252) loss = 1.9487, batch accuracy = 0.3046875, val acc = 0.203125\n",
      "(253) loss = 1.9455, batch accuracy = 0.3023681640625, val acc = 0.1927083432674408\n",
      "(254) loss = 1.9418, batch accuracy = 0.3067626953125, val acc = 0.1953125\n",
      "(255) loss = 1.9485, batch accuracy = 0.3050537109375, val acc = 0.1953125\n",
      "(256) loss = 1.9489, batch accuracy = 0.302734375, val acc = 0.1979166716337204\n",
      "(257) loss = 1.9554, batch accuracy = 0.3046875, val acc = 0.2057291716337204\n",
      "(258) loss = 1.9522, batch accuracy = 0.3056640625, val acc = 0.2005208432674408\n",
      "(259) loss = 1.9509, batch accuracy = 0.306884765625, val acc = 0.1875\n",
      "(260) loss = 1.9500, batch accuracy = 0.3079833984375, val acc = 0.1979166716337204\n",
      "(261) loss = 1.9557, batch accuracy = 0.3056640625, val acc = 0.1953125\n",
      "(262) loss = 1.9451, batch accuracy = 0.303955078125, val acc = 0.1953125\n",
      "(263) loss = 1.9498, batch accuracy = 0.304443359375, val acc = 0.1953125\n",
      "(264) loss = 1.9446, batch accuracy = 0.30810546875, val acc = 0.2109375\n",
      "(265) loss = 1.9455, batch accuracy = 0.306396484375, val acc = 0.1875\n",
      "(266) loss = 1.9405, batch accuracy = 0.305908203125, val acc = 0.203125\n",
      "(267) loss = 1.9441, batch accuracy = 0.3035888671875, val acc = 0.1953125\n",
      "(268) loss = 1.9489, batch accuracy = 0.3074951171875, val acc = 0.203125\n",
      "(269) loss = 1.9419, batch accuracy = 0.30419921875, val acc = 0.2109375\n",
      "(270) loss = 1.9645, batch accuracy = 0.3056640625, val acc = 0.1979166716337204\n",
      "(271) loss = 1.9687, batch accuracy = 0.2999267578125, val acc = 0.2005208432674408\n",
      "(272) loss = 1.9391, batch accuracy = 0.304931640625, val acc = 0.1927083432674408\n",
      "(273) loss = 1.9514, batch accuracy = 0.3076171875, val acc = 0.1953125\n",
      "(274) loss = 1.9499, batch accuracy = 0.3045654296875, val acc = 0.203125\n",
      "(275) loss = 1.9479, batch accuracy = 0.3072509765625, val acc = 0.1927083432674408\n",
      "(276) loss = 1.9605, batch accuracy = 0.302978515625, val acc = 0.203125\n",
      "(277) loss = 1.9578, batch accuracy = 0.3035888671875, val acc = 0.1953125\n",
      "(278) loss = 1.9559, batch accuracy = 0.3035888671875, val acc = 0.2083333432674408\n",
      "(279) loss = 1.9566, batch accuracy = 0.303955078125, val acc = 0.2057291716337204\n",
      "(280) loss = 1.9466, batch accuracy = 0.3060302734375, val acc = 0.1848958432674408\n",
      "(281) loss = 1.9433, batch accuracy = 0.304443359375, val acc = 0.1901041716337204\n",
      "(282) loss = 1.9383, batch accuracy = 0.3076171875, val acc = 0.1927083432674408\n",
      "(283) loss = 1.9399, batch accuracy = 0.308349609375, val acc = 0.1979166716337204\n",
      "(284) loss = 1.9425, batch accuracy = 0.3087158203125, val acc = 0.1927083432674408\n",
      "(285) loss = 1.9401, batch accuracy = 0.30517578125, val acc = 0.1953125\n",
      "(286) loss = 1.9465, batch accuracy = 0.308349609375, val acc = 0.203125\n",
      "(287) loss = 1.9364, batch accuracy = 0.31005859375, val acc = 0.2005208432674408\n",
      "(288) loss = 1.9490, batch accuracy = 0.307861328125, val acc = 0.1979166716337204\n",
      "(289) loss = 1.9538, batch accuracy = 0.306396484375, val acc = 0.1927083432674408\n",
      "(290) loss = 1.9490, batch accuracy = 0.3111572265625, val acc = 0.1979166716337204\n",
      "(291) loss = 1.9539, batch accuracy = 0.305419921875, val acc = 0.2005208432674408\n",
      "(292) loss = 1.9535, batch accuracy = 0.30908203125, val acc = 0.2109375\n",
      "(293) loss = 1.9408, batch accuracy = 0.306640625, val acc = 0.1953125\n",
      "(294) loss = 1.9515, batch accuracy = 0.308349609375, val acc = 0.1901041716337204\n",
      "(295) loss = 1.9369, batch accuracy = 0.3079833984375, val acc = 0.203125\n",
      "(296) loss = 1.9360, batch accuracy = 0.3101806640625, val acc = 0.203125\n",
      "(297) loss = 1.9329, batch accuracy = 0.3096923828125, val acc = 0.1927083432674408\n",
      "(298) loss = 1.9408, batch accuracy = 0.3077392578125, val acc = 0.1927083432674408\n",
      "(299) loss = 1.9326, batch accuracy = 0.3104248046875, val acc = 0.1848958432674408\n",
      "(300) loss = 1.9390, batch accuracy = 0.309326171875, val acc = 0.2005208432674408\n",
      "(301) loss = 1.9360, batch accuracy = 0.3118896484375, val acc = 0.1953125\n",
      "(302) loss = 1.9396, batch accuracy = 0.3092041015625, val acc = 0.1979166716337204\n",
      "(303) loss = 1.9357, batch accuracy = 0.307861328125, val acc = 0.2005208432674408\n",
      "(304) loss = 1.9444, batch accuracy = 0.3094482421875, val acc = 0.203125\n",
      "(305) loss = 1.9381, batch accuracy = 0.3055419921875, val acc = 0.1927083432674408\n",
      "(306) loss = 1.9405, batch accuracy = 0.3109130859375, val acc = 0.1875\n",
      "(307) loss = 1.9422, batch accuracy = 0.3101806640625, val acc = 0.203125\n",
      "(308) loss = 1.9372, batch accuracy = 0.30859375, val acc = 0.2005208432674408\n",
      "(309) loss = 1.9347, batch accuracy = 0.308349609375, val acc = 0.1953125\n",
      "(310) loss = 1.9358, batch accuracy = 0.3109130859375, val acc = 0.2005208432674408\n",
      "(311) loss = 1.9344, batch accuracy = 0.309814453125, val acc = 0.1953125\n",
      "(312) loss = 1.9389, batch accuracy = 0.3104248046875, val acc = 0.1953125\n",
      "(313) loss = 1.9387, batch accuracy = 0.3046875, val acc = 0.1953125\n",
      "(314) loss = 1.9506, batch accuracy = 0.3076171875, val acc = 0.1979166716337204\n",
      "(315) loss = 1.9426, batch accuracy = 0.3082275390625, val acc = 0.1848958432674408\n",
      "(316) loss = 1.9433, batch accuracy = 0.3099365234375, val acc = 0.1979166716337204\n",
      "(317) loss = 1.9420, batch accuracy = 0.30859375, val acc = 0.1901041716337204\n",
      "(318) loss = 1.9400, batch accuracy = 0.3060302734375, val acc = 0.1953125\n",
      "(319) loss = 1.9411, batch accuracy = 0.306884765625, val acc = 0.1927083432674408\n",
      "(320) loss = 1.9481, batch accuracy = 0.3060302734375, val acc = 0.1875\n",
      "(321) loss = 1.9349, batch accuracy = 0.308349609375, val acc = 0.1796875\n",
      "(322) loss = 1.9452, batch accuracy = 0.305419921875, val acc = 0.1848958432674408\n",
      "(323) loss = 1.9526, batch accuracy = 0.3087158203125, val acc = 0.1822916716337204\n",
      "(324) loss = 1.9537, batch accuracy = 0.3048095703125, val acc = 0.1927083432674408\n",
      "(325) loss = 1.9593, batch accuracy = 0.300048828125, val acc = 0.1953125\n",
      "(326) loss = 1.9540, batch accuracy = 0.30322265625, val acc = 0.1848958432674408\n",
      "(327) loss = 1.9475, batch accuracy = 0.3065185546875, val acc = 0.1953125\n",
      "(328) loss = 1.9558, batch accuracy = 0.3062744140625, val acc = 0.1901041716337204\n",
      "(329) loss = 1.9496, batch accuracy = 0.3045654296875, val acc = 0.1875\n",
      "(330) loss = 1.9456, batch accuracy = 0.30712890625, val acc = 0.1901041716337204\n",
      "(331) loss = 1.9361, batch accuracy = 0.308837890625, val acc = 0.1875\n",
      "(332) loss = 1.9341, batch accuracy = 0.30810546875, val acc = 0.1875\n",
      "(333) loss = 1.9385, batch accuracy = 0.3067626953125, val acc = 0.1927083432674408\n",
      "(334) loss = 1.9367, batch accuracy = 0.3089599609375, val acc = 0.1953125\n",
      "(335) loss = 1.9339, batch accuracy = 0.3121337890625, val acc = 0.1979166716337204\n",
      "(336) loss = 1.9387, batch accuracy = 0.305908203125, val acc = 0.1901041716337204\n",
      "(337) loss = 1.9424, batch accuracy = 0.31005859375, val acc = 0.1979166716337204\n",
      "(338) loss = 1.9467, batch accuracy = 0.302978515625, val acc = 0.1848958432674408\n",
      "(339) loss = 1.9397, batch accuracy = 0.306640625, val acc = 0.1927083432674408\n",
      "(340) loss = 1.9482, batch accuracy = 0.305419921875, val acc = 0.1927083432674408\n",
      "(341) loss = 1.9493, batch accuracy = 0.3056640625, val acc = 0.1875\n",
      "(342) loss = 1.9393, batch accuracy = 0.309326171875, val acc = 0.1901041716337204\n",
      "(343) loss = 1.9460, batch accuracy = 0.3065185546875, val acc = 0.1927083432674408\n",
      "(344) loss = 1.9399, batch accuracy = 0.30859375, val acc = 0.1901041716337204\n",
      "(345) loss = 1.9463, batch accuracy = 0.3037109375, val acc = 0.203125\n",
      "(346) loss = 1.9525, batch accuracy = 0.304931640625, val acc = 0.1848958432674408\n",
      "(347) loss = 1.9558, batch accuracy = 0.303955078125, val acc = 0.1927083432674408\n",
      "(348) loss = 1.9436, batch accuracy = 0.307373046875, val acc = 0.1875\n",
      "(349) loss = 1.9397, batch accuracy = 0.304931640625, val acc = 0.1796875\n",
      "(350) loss = 1.9512, batch accuracy = 0.3046875, val acc = 0.1875\n",
      "(351) loss = 1.9361, batch accuracy = 0.307861328125, val acc = 0.1822916716337204\n",
      "(352) loss = 1.9465, batch accuracy = 0.30712890625, val acc = 0.1901041716337204\n",
      "(353) loss = 1.9399, batch accuracy = 0.30517578125, val acc = 0.1901041716337204\n",
      "(354) loss = 1.9459, batch accuracy = 0.3079833984375, val acc = 0.1953125\n",
      "(355) loss = 1.9406, batch accuracy = 0.309814453125, val acc = 0.1953125\n",
      "(356) loss = 1.9489, batch accuracy = 0.30712890625, val acc = 0.1927083432674408\n",
      "(357) loss = 1.9516, batch accuracy = 0.304931640625, val acc = 0.1796875\n",
      "(358) loss = 1.9434, batch accuracy = 0.3082275390625, val acc = 0.1875\n",
      "(359) loss = 1.9568, batch accuracy = 0.3033447265625, val acc = 0.1848958432674408\n",
      "(360) loss = 1.9408, batch accuracy = 0.306884765625, val acc = 0.1979166716337204\n",
      "(361) loss = 1.9362, batch accuracy = 0.309814453125, val acc = 0.1979166716337204\n",
      "(362) loss = 1.9343, batch accuracy = 0.309326171875, val acc = 0.1848958432674408\n",
      "(363) loss = 1.9424, batch accuracy = 0.310546875, val acc = 0.1875\n",
      "(364) loss = 1.9490, batch accuracy = 0.30712890625, val acc = 0.1953125\n",
      "(365) loss = 1.9507, batch accuracy = 0.3089599609375, val acc = 0.1927083432674408\n",
      "(366) loss = 1.9504, batch accuracy = 0.307861328125, val acc = 0.1822916716337204\n",
      "(367) loss = 1.9532, batch accuracy = 0.3094482421875, val acc = 0.1901041716337204\n",
      "(368) loss = 1.9483, batch accuracy = 0.306884765625, val acc = 0.1848958432674408\n",
      "(369) loss = 1.9599, batch accuracy = 0.305908203125, val acc = 0.1848958432674408\n",
      "(370) loss = 1.9589, batch accuracy = 0.30859375, val acc = 0.2005208432674408\n",
      "(371) loss = 1.9384, batch accuracy = 0.306396484375, val acc = 0.1770833432674408\n",
      "(372) loss = 1.9323, batch accuracy = 0.3095703125, val acc = 0.1979166716337204\n",
      "(373) loss = 1.9353, batch accuracy = 0.3095703125, val acc = 0.1901041716337204\n",
      "(374) loss = 1.9328, batch accuracy = 0.3109130859375, val acc = 0.1953125\n",
      "(375) loss = 1.9369, batch accuracy = 0.311767578125, val acc = 0.1979166716337204\n",
      "(376) loss = 1.9212, batch accuracy = 0.3123779296875, val acc = 0.1875\n",
      "(377) loss = 1.9317, batch accuracy = 0.3094482421875, val acc = 0.1953125\n",
      "(378) loss = 1.9268, batch accuracy = 0.31591796875, val acc = 0.1901041716337204\n",
      "(379) loss = 1.9326, batch accuracy = 0.3155517578125, val acc = 0.1979166716337204\n",
      "(380) loss = 1.9308, batch accuracy = 0.31591796875, val acc = 0.1822916716337204\n",
      "(381) loss = 1.9378, batch accuracy = 0.312255859375, val acc = 0.1822916716337204\n",
      "(382) loss = 1.9396, batch accuracy = 0.316162109375, val acc = 0.1927083432674408\n",
      "(383) loss = 1.9322, batch accuracy = 0.3115234375, val acc = 0.1875\n",
      "(384) loss = 1.9271, batch accuracy = 0.315185546875, val acc = 0.1953125\n",
      "(385) loss = 1.9297, batch accuracy = 0.3133544921875, val acc = 0.1979166716337204\n",
      "(386) loss = 1.9246, batch accuracy = 0.31396484375, val acc = 0.1901041716337204\n",
      "(387) loss = 1.9349, batch accuracy = 0.31396484375, val acc = 0.1875\n",
      "(388) loss = 1.9321, batch accuracy = 0.314453125, val acc = 0.1822916716337204\n",
      "(389) loss = 1.9293, batch accuracy = 0.315185546875, val acc = 0.1822916716337204\n",
      "(390) loss = 1.9340, batch accuracy = 0.3125, val acc = 0.1848958432674408\n",
      "(391) loss = 1.9234, batch accuracy = 0.312744140625, val acc = 0.1822916716337204\n",
      "(392) loss = 1.9285, batch accuracy = 0.3135986328125, val acc = 0.1901041716337204\n",
      "(393) loss = 1.9372, batch accuracy = 0.3114013671875, val acc = 0.1901041716337204\n",
      "(394) loss = 1.9349, batch accuracy = 0.3128662109375, val acc = 0.1744791716337204\n",
      "(395) loss = 1.9285, batch accuracy = 0.3148193359375, val acc = 0.1822916716337204\n",
      "(396) loss = 1.9282, batch accuracy = 0.317138671875, val acc = 0.1953125\n",
      "(397) loss = 1.9323, batch accuracy = 0.3116455078125, val acc = 0.1927083432674408\n",
      "(398) loss = 1.9281, batch accuracy = 0.3145751953125, val acc = 0.1875\n",
      "(399) loss = 1.9388, batch accuracy = 0.3140869140625, val acc = 0.1796875\n",
      "(400) loss = 1.9253, batch accuracy = 0.3150634765625, val acc = 0.1848958432674408\n",
      "(401) loss = 1.9288, batch accuracy = 0.3131103515625, val acc = 0.1848958432674408\n",
      "(402) loss = 1.9273, batch accuracy = 0.3138427734375, val acc = 0.1796875\n",
      "(403) loss = 1.9285, batch accuracy = 0.3118896484375, val acc = 0.1979166716337204\n",
      "(404) loss = 1.9226, batch accuracy = 0.3089599609375, val acc = 0.1770833432674408\n",
      "(405) loss = 1.9268, batch accuracy = 0.31201171875, val acc = 0.203125\n",
      "(406) loss = 1.9161, batch accuracy = 0.3133544921875, val acc = 0.1770833432674408\n",
      "(407) loss = 1.9223, batch accuracy = 0.3123779296875, val acc = 0.1953125\n",
      "(408) loss = 1.9246, batch accuracy = 0.3134765625, val acc = 0.1848958432674408\n",
      "(409) loss = 1.9247, batch accuracy = 0.3133544921875, val acc = 0.1953125\n",
      "(410) loss = 1.9144, batch accuracy = 0.31396484375, val acc = 0.2005208432674408\n",
      "(411) loss = 1.9208, batch accuracy = 0.314453125, val acc = 0.1875\n",
      "(412) loss = 1.9126, batch accuracy = 0.3148193359375, val acc = 0.1953125\n",
      "(413) loss = 1.9316, batch accuracy = 0.3096923828125, val acc = 0.1848958432674408\n",
      "(414) loss = 1.9201, batch accuracy = 0.3121337890625, val acc = 0.1927083432674408\n",
      "(415) loss = 1.9167, batch accuracy = 0.3140869140625, val acc = 0.1979166716337204\n",
      "(416) loss = 1.9091, batch accuracy = 0.314208984375, val acc = 0.1901041716337204\n",
      "(417) loss = 1.9169, batch accuracy = 0.3145751953125, val acc = 0.203125\n",
      "(418) loss = 1.9147, batch accuracy = 0.3131103515625, val acc = 0.1953125\n",
      "(419) loss = 1.9093, batch accuracy = 0.3153076171875, val acc = 0.2005208432674408\n",
      "(420) loss = 1.9337, batch accuracy = 0.31396484375, val acc = 0.1979166716337204\n",
      "(421) loss = 1.9165, batch accuracy = 0.315673828125, val acc = 0.1848958432674408\n",
      "(422) loss = 1.9212, batch accuracy = 0.3148193359375, val acc = 0.1901041716337204\n",
      "(423) loss = 1.9176, batch accuracy = 0.3138427734375, val acc = 0.1848958432674408\n",
      "(424) loss = 1.9203, batch accuracy = 0.31591796875, val acc = 0.1875\n",
      "(425) loss = 1.9195, batch accuracy = 0.3143310546875, val acc = 0.1979166716337204\n",
      "(426) loss = 1.9254, batch accuracy = 0.3126220703125, val acc = 0.1953125\n",
      "(427) loss = 1.9183, batch accuracy = 0.3121337890625, val acc = 0.1901041716337204\n",
      "(428) loss = 1.9153, batch accuracy = 0.3162841796875, val acc = 0.2057291716337204\n",
      "(429) loss = 1.9183, batch accuracy = 0.31689453125, val acc = 0.1953125\n",
      "(430) loss = 1.9162, batch accuracy = 0.3170166015625, val acc = 0.1848958432674408\n",
      "(431) loss = 1.9242, batch accuracy = 0.312255859375, val acc = 0.1901041716337204\n",
      "(432) loss = 1.9251, batch accuracy = 0.3133544921875, val acc = 0.1901041716337204\n",
      "(433) loss = 1.9279, batch accuracy = 0.3133544921875, val acc = 0.1927083432674408\n",
      "(434) loss = 1.9266, batch accuracy = 0.31298828125, val acc = 0.1901041716337204\n",
      "(435) loss = 1.9445, batch accuracy = 0.31201171875, val acc = 0.2005208432674408\n",
      "(436) loss = 1.9451, batch accuracy = 0.3148193359375, val acc = 0.1796875\n",
      "(437) loss = 1.9411, batch accuracy = 0.309814453125, val acc = 0.1822916716337204\n",
      "(438) loss = 1.9383, batch accuracy = 0.310302734375, val acc = 0.1875\n",
      "(439) loss = 1.9254, batch accuracy = 0.3135986328125, val acc = 0.1875\n",
      "(440) loss = 1.9355, batch accuracy = 0.311767578125, val acc = 0.1979166716337204\n",
      "(441) loss = 1.9266, batch accuracy = 0.315185546875, val acc = 0.1796875\n",
      "(442) loss = 1.9287, batch accuracy = 0.3101806640625, val acc = 0.1875\n",
      "(443) loss = 1.9177, batch accuracy = 0.318115234375, val acc = 0.1953125\n",
      "(444) loss = 1.9091, batch accuracy = 0.31494140625, val acc = 0.1848958432674408\n",
      "(445) loss = 1.9094, batch accuracy = 0.3172607421875, val acc = 0.1901041716337204\n",
      "(446) loss = 1.9097, batch accuracy = 0.314208984375, val acc = 0.1927083432674408\n",
      "(447) loss = 1.9100, batch accuracy = 0.3162841796875, val acc = 0.1927083432674408\n",
      "(448) loss = 1.9187, batch accuracy = 0.312744140625, val acc = 0.1979166716337204\n",
      "(449) loss = 1.9117, batch accuracy = 0.31640625, val acc = 0.1796875\n",
      "(450) loss = 1.8997, batch accuracy = 0.3184814453125, val acc = 0.203125\n",
      "(451) loss = 1.9056, batch accuracy = 0.317138671875, val acc = 0.1927083432674408\n",
      "(452) loss = 1.9125, batch accuracy = 0.318359375, val acc = 0.1901041716337204\n",
      "(453) loss = 1.9169, batch accuracy = 0.3145751953125, val acc = 0.1822916716337204\n",
      "(454) loss = 1.9227, batch accuracy = 0.312255859375, val acc = 0.1953125\n",
      "(455) loss = 1.9235, batch accuracy = 0.3138427734375, val acc = 0.1796875\n",
      "(456) loss = 1.9132, batch accuracy = 0.3165283203125, val acc = 0.2005208432674408\n",
      "(457) loss = 1.9113, batch accuracy = 0.3157958984375, val acc = 0.1822916716337204\n",
      "(458) loss = 1.9155, batch accuracy = 0.31494140625, val acc = 0.1953125\n",
      "(459) loss = 1.9186, batch accuracy = 0.3143310546875, val acc = 0.1848958432674408\n",
      "(460) loss = 1.9099, batch accuracy = 0.315673828125, val acc = 0.2057291716337204\n",
      "(461) loss = 1.9069, batch accuracy = 0.31494140625, val acc = 0.1953125\n",
      "(462) loss = 1.9213, batch accuracy = 0.31591796875, val acc = 0.1927083432674408\n",
      "(463) loss = 1.9172, batch accuracy = 0.31494140625, val acc = 0.1875\n",
      "(464) loss = 1.9137, batch accuracy = 0.31689453125, val acc = 0.1875\n",
      "(465) loss = 1.9176, batch accuracy = 0.317138671875, val acc = 0.1901041716337204\n",
      "(466) loss = 1.9214, batch accuracy = 0.3118896484375, val acc = 0.2005208432674408\n",
      "(467) loss = 1.9185, batch accuracy = 0.3126220703125, val acc = 0.1953125\n",
      "(468) loss = 1.9174, batch accuracy = 0.3106689453125, val acc = 0.1927083432674408\n",
      "(469) loss = 1.9104, batch accuracy = 0.3143310546875, val acc = 0.1927083432674408\n",
      "(470) loss = 1.9109, batch accuracy = 0.3143310546875, val acc = 0.1875\n",
      "(471) loss = 1.9088, batch accuracy = 0.3167724609375, val acc = 0.1796875\n",
      "(472) loss = 1.9192, batch accuracy = 0.31201171875, val acc = 0.1875\n",
      "(473) loss = 1.9202, batch accuracy = 0.31591796875, val acc = 0.1875\n",
      "(474) loss = 1.9207, batch accuracy = 0.3126220703125, val acc = 0.1848958432674408\n",
      "(475) loss = 1.9168, batch accuracy = 0.31591796875, val acc = 0.1875\n",
      "(476) loss = 1.9367, batch accuracy = 0.311767578125, val acc = 0.1744791716337204\n",
      "(477) loss = 1.9363, batch accuracy = 0.3114013671875, val acc = 0.1875\n",
      "(478) loss = 1.9203, batch accuracy = 0.3155517578125, val acc = 0.1927083432674408\n",
      "(479) loss = 1.9200, batch accuracy = 0.312255859375, val acc = 0.1822916716337204\n",
      "(480) loss = 1.9240, batch accuracy = 0.311279296875, val acc = 0.1796875\n",
      "(481) loss = 1.9292, batch accuracy = 0.3125, val acc = 0.203125\n",
      "(482) loss = 1.9359, batch accuracy = 0.3115234375, val acc = 0.1927083432674408\n",
      "(483) loss = 1.9199, batch accuracy = 0.3153076171875, val acc = 0.1796875\n",
      "(484) loss = 1.9199, batch accuracy = 0.314453125, val acc = 0.1848958432674408\n",
      "(485) loss = 1.9191, batch accuracy = 0.313720703125, val acc = 0.2005208432674408\n",
      "(486) loss = 1.9210, batch accuracy = 0.313720703125, val acc = 0.1901041716337204\n",
      "(487) loss = 1.9149, batch accuracy = 0.314453125, val acc = 0.1901041716337204\n",
      "(488) loss = 1.9172, batch accuracy = 0.31005859375, val acc = 0.1875\n",
      "(489) loss = 1.9222, batch accuracy = 0.315673828125, val acc = 0.1979166716337204\n",
      "(490) loss = 1.9194, batch accuracy = 0.315673828125, val acc = 0.1848958432674408\n",
      "(491) loss = 1.9166, batch accuracy = 0.31494140625, val acc = 0.1901041716337204\n",
      "(492) loss = 1.9194, batch accuracy = 0.313720703125, val acc = 0.1953125\n",
      "(493) loss = 1.9157, batch accuracy = 0.316650390625, val acc = 0.1848958432674408\n",
      "(494) loss = 1.9142, batch accuracy = 0.314208984375, val acc = 0.1979166716337204\n",
      "(495) loss = 1.9162, batch accuracy = 0.3138427734375, val acc = 0.1979166716337204\n",
      "(496) loss = 1.9152, batch accuracy = 0.3140869140625, val acc = 0.1901041716337204\n",
      "(497) loss = 1.9116, batch accuracy = 0.315185546875, val acc = 0.1822916716337204\n",
      "(498) loss = 1.9189, batch accuracy = 0.3126220703125, val acc = 0.1927083432674408\n",
      "(499) loss = 1.9174, batch accuracy = 0.3121337890625, val acc = 0.1744791716337204\n",
      "(500) loss = 1.9207, batch accuracy = 0.310302734375, val acc = 0.1901041716337204\n",
      "(501) loss = 1.9204, batch accuracy = 0.31005859375, val acc = 0.1848958432674408\n",
      "(502) loss = 1.9372, batch accuracy = 0.3082275390625, val acc = 0.1927083432674408\n",
      "(503) loss = 1.9305, batch accuracy = 0.306884765625, val acc = 0.1875\n",
      "(504) loss = 1.9368, batch accuracy = 0.3074951171875, val acc = 0.1927083432674408\n",
      "(505) loss = 1.9346, batch accuracy = 0.308837890625, val acc = 0.2005208432674408\n",
      "(506) loss = 1.9387, batch accuracy = 0.308837890625, val acc = 0.203125\n",
      "(507) loss = 1.9229, batch accuracy = 0.312255859375, val acc = 0.203125\n",
      "(508) loss = 1.9348, batch accuracy = 0.3074951171875, val acc = 0.1979166716337204\n",
      "(509) loss = 1.9219, batch accuracy = 0.313720703125, val acc = 0.1901041716337204\n",
      "(510) loss = 1.9108, batch accuracy = 0.3131103515625, val acc = 0.1979166716337204\n",
      "(511) loss = 1.9148, batch accuracy = 0.3131103515625, val acc = 0.1953125\n",
      "(512) loss = 1.9328, batch accuracy = 0.3131103515625, val acc = 0.1953125\n",
      "(513) loss = 1.9320, batch accuracy = 0.314453125, val acc = 0.2005208432674408\n",
      "(514) loss = 1.9251, batch accuracy = 0.310546875, val acc = 0.1953125\n",
      "(515) loss = 1.9344, batch accuracy = 0.3128662109375, val acc = 0.1927083432674408\n",
      "(516) loss = 1.9280, batch accuracy = 0.3116455078125, val acc = 0.1979166716337204\n",
      "(517) loss = 1.9181, batch accuracy = 0.3133544921875, val acc = 0.1901041716337204\n",
      "(518) loss = 1.9238, batch accuracy = 0.3095703125, val acc = 0.1979166716337204\n",
      "(519) loss = 1.9113, batch accuracy = 0.3153076171875, val acc = 0.1953125\n",
      "(520) loss = 1.9089, batch accuracy = 0.3134765625, val acc = 0.203125\n",
      "(521) loss = 1.9001, batch accuracy = 0.314453125, val acc = 0.203125\n",
      "(522) loss = 1.9138, batch accuracy = 0.313720703125, val acc = 0.1953125\n",
      "(523) loss = 1.9074, batch accuracy = 0.3165283203125, val acc = 0.1875\n",
      "(524) loss = 1.9012, batch accuracy = 0.3187255859375, val acc = 0.2057291716337204\n",
      "(525) loss = 1.9073, batch accuracy = 0.314697265625, val acc = 0.2109375\n",
      "(526) loss = 1.9082, batch accuracy = 0.3165283203125, val acc = 0.2005208432674408\n",
      "(527) loss = 1.9037, batch accuracy = 0.3140869140625, val acc = 0.1927083432674408\n",
      "(528) loss = 1.9150, batch accuracy = 0.3131103515625, val acc = 0.1979166716337204\n",
      "(529) loss = 1.9084, batch accuracy = 0.3135986328125, val acc = 0.2057291716337204\n",
      "(530) loss = 1.9147, batch accuracy = 0.3145751953125, val acc = 0.1979166716337204\n",
      "(531) loss = 1.9047, batch accuracy = 0.3135986328125, val acc = 0.203125\n",
      "(532) loss = 1.9208, batch accuracy = 0.3114013671875, val acc = 0.2005208432674408\n",
      "(533) loss = 1.9169, batch accuracy = 0.3118896484375, val acc = 0.203125\n",
      "(534) loss = 1.9225, batch accuracy = 0.31298828125, val acc = 0.1744791716337204\n",
      "(535) loss = 1.9284, batch accuracy = 0.313232421875, val acc = 0.1979166716337204\n",
      "(536) loss = 1.9190, batch accuracy = 0.314208984375, val acc = 0.1927083432674408\n",
      "(537) loss = 1.9179, batch accuracy = 0.311279296875, val acc = 0.2057291716337204\n",
      "(538) loss = 1.9173, batch accuracy = 0.31689453125, val acc = 0.1901041716337204\n",
      "(539) loss = 1.9245, batch accuracy = 0.3084716796875, val acc = 0.1927083432674408\n",
      "(540) loss = 1.9186, batch accuracy = 0.3170166015625, val acc = 0.1979166716337204\n",
      "(541) loss = 1.9256, batch accuracy = 0.3118896484375, val acc = 0.203125\n",
      "(542) loss = 1.9132, batch accuracy = 0.3145751953125, val acc = 0.1901041716337204\n",
      "(543) loss = 1.9185, batch accuracy = 0.3111572265625, val acc = 0.2005208432674408\n",
      "(544) loss = 1.9168, batch accuracy = 0.3160400390625, val acc = 0.1979166716337204\n",
      "(545) loss = 1.9095, batch accuracy = 0.317138671875, val acc = 0.1901041716337204\n",
      "(546) loss = 1.9156, batch accuracy = 0.3153076171875, val acc = 0.1953125\n",
      "(547) loss = 1.9179, batch accuracy = 0.31396484375, val acc = 0.1822916716337204\n",
      "(548) loss = 1.9116, batch accuracy = 0.3134765625, val acc = 0.1848958432674408\n",
      "(549) loss = 1.9123, batch accuracy = 0.315185546875, val acc = 0.1927083432674408\n",
      "(550) loss = 1.9140, batch accuracy = 0.3172607421875, val acc = 0.1927083432674408\n",
      "(551) loss = 1.9154, batch accuracy = 0.316162109375, val acc = 0.1927083432674408\n",
      "(552) loss = 1.9070, batch accuracy = 0.31689453125, val acc = 0.1953125\n",
      "(553) loss = 1.9149, batch accuracy = 0.3123779296875, val acc = 0.1927083432674408\n",
      "(554) loss = 1.9052, batch accuracy = 0.31396484375, val acc = 0.1927083432674408\n",
      "(555) loss = 1.9009, batch accuracy = 0.31787109375, val acc = 0.1875\n",
      "(556) loss = 1.8999, batch accuracy = 0.3157958984375, val acc = 0.1979166716337204\n",
      "(557) loss = 1.9105, batch accuracy = 0.315673828125, val acc = 0.1953125\n",
      "(558) loss = 1.9159, batch accuracy = 0.3155517578125, val acc = 0.1927083432674408\n",
      "(559) loss = 1.9233, batch accuracy = 0.31640625, val acc = 0.1979166716337204\n",
      "(560) loss = 1.9167, batch accuracy = 0.316650390625, val acc = 0.1875\n",
      "(561) loss = 1.9115, batch accuracy = 0.311767578125, val acc = 0.1979166716337204\n",
      "(562) loss = 1.9069, batch accuracy = 0.31884765625, val acc = 0.1796875\n",
      "(563) loss = 1.9146, batch accuracy = 0.312744140625, val acc = 0.2005208432674408\n",
      "(564) loss = 1.9122, batch accuracy = 0.314453125, val acc = 0.1875\n",
      "(565) loss = 1.9108, batch accuracy = 0.3134765625, val acc = 0.1979166716337204\n",
      "(566) loss = 1.9148, batch accuracy = 0.314697265625, val acc = 0.1875\n",
      "(567) loss = 1.9075, batch accuracy = 0.3150634765625, val acc = 0.1901041716337204\n",
      "(568) loss = 1.9113, batch accuracy = 0.3175048828125, val acc = 0.1796875\n",
      "(569) loss = 1.9068, batch accuracy = 0.318603515625, val acc = 0.1953125\n",
      "(570) loss = 1.9318, batch accuracy = 0.3162841796875, val acc = 0.1875\n",
      "(571) loss = 1.9272, batch accuracy = 0.3153076171875, val acc = 0.1901041716337204\n",
      "(572) loss = 1.9176, batch accuracy = 0.3143310546875, val acc = 0.171875\n",
      "(573) loss = 1.9094, batch accuracy = 0.316650390625, val acc = 0.1927083432674408\n",
      "(574) loss = 1.9123, batch accuracy = 0.316650390625, val acc = 0.1927083432674408\n",
      "(575) loss = 1.9084, batch accuracy = 0.314208984375, val acc = 0.1796875\n",
      "(576) loss = 1.9175, batch accuracy = 0.3173828125, val acc = 0.1848958432674408\n",
      "(577) loss = 1.9164, batch accuracy = 0.315185546875, val acc = 0.171875\n",
      "(578) loss = 1.9112, batch accuracy = 0.314697265625, val acc = 0.1848958432674408\n",
      "(579) loss = 1.9101, batch accuracy = 0.3170166015625, val acc = 0.1848958432674408\n",
      "(580) loss = 1.9113, batch accuracy = 0.315673828125, val acc = 0.1927083432674408\n",
      "(581) loss = 1.9024, batch accuracy = 0.31689453125, val acc = 0.1979166716337204\n",
      "(582) loss = 1.8980, batch accuracy = 0.3184814453125, val acc = 0.1901041716337204\n",
      "(583) loss = 1.8937, batch accuracy = 0.32080078125, val acc = 0.1901041716337204\n",
      "(584) loss = 1.9018, batch accuracy = 0.317626953125, val acc = 0.1848958432674408\n",
      "(585) loss = 1.8923, batch accuracy = 0.319580078125, val acc = 0.1796875\n",
      "(586) loss = 1.8991, batch accuracy = 0.3209228515625, val acc = 0.1979166716337204\n",
      "(587) loss = 1.9038, batch accuracy = 0.3148193359375, val acc = 0.171875\n",
      "(588) loss = 1.8951, batch accuracy = 0.318115234375, val acc = 0.1692708432674408\n",
      "(589) loss = 1.9043, batch accuracy = 0.318359375, val acc = 0.1927083432674408\n",
      "(590) loss = 1.8975, batch accuracy = 0.320068359375, val acc = 0.1822916716337204\n",
      "(591) loss = 1.9016, batch accuracy = 0.3172607421875, val acc = 0.1875\n",
      "(592) loss = 1.8986, batch accuracy = 0.3179931640625, val acc = 0.1848958432674408\n",
      "(593) loss = 1.9101, batch accuracy = 0.315185546875, val acc = 0.1927083432674408\n",
      "(594) loss = 1.8967, batch accuracy = 0.3192138671875, val acc = 0.1744791716337204\n",
      "(595) loss = 1.9130, batch accuracy = 0.3153076171875, val acc = 0.1979166716337204\n",
      "(596) loss = 1.9132, batch accuracy = 0.314453125, val acc = 0.1744791716337204\n",
      "(597) loss = 1.9222, batch accuracy = 0.3121337890625, val acc = 0.1875\n",
      "(598) loss = 1.9014, batch accuracy = 0.3160400390625, val acc = 0.1822916716337204\n",
      "(599) loss = 1.9114, batch accuracy = 0.31787109375, val acc = 0.1875\n",
      "(600) loss = 1.9019, batch accuracy = 0.3189697265625, val acc = 0.1848958432674408\n",
      "(601) loss = 1.9033, batch accuracy = 0.3165283203125, val acc = 0.1901041716337204\n",
      "(602) loss = 1.9093, batch accuracy = 0.3153076171875, val acc = 0.1927083432674408\n",
      "(603) loss = 1.9103, batch accuracy = 0.312744140625, val acc = 0.1848958432674408\n",
      "(604) loss = 1.9061, batch accuracy = 0.3175048828125, val acc = 0.2005208432674408\n",
      "(605) loss = 1.9074, batch accuracy = 0.3118896484375, val acc = 0.1927083432674408\n",
      "(606) loss = 1.9172, batch accuracy = 0.3114013671875, val acc = 0.1875\n",
      "(607) loss = 1.9130, batch accuracy = 0.316162109375, val acc = 0.2005208432674408\n",
      "(608) loss = 1.9181, batch accuracy = 0.3157958984375, val acc = 0.1848958432674408\n",
      "(609) loss = 1.9098, batch accuracy = 0.3126220703125, val acc = 0.1770833432674408\n",
      "(610) loss = 1.9146, batch accuracy = 0.31591796875, val acc = 0.1744791716337204\n",
      "(611) loss = 1.9158, batch accuracy = 0.3133544921875, val acc = 0.1822916716337204\n",
      "(612) loss = 1.9067, batch accuracy = 0.3150634765625, val acc = 0.1953125\n",
      "(613) loss = 1.9158, batch accuracy = 0.3157958984375, val acc = 0.1927083432674408\n",
      "(614) loss = 1.9028, batch accuracy = 0.3121337890625, val acc = 0.1692708432674408\n",
      "(615) loss = 1.9063, batch accuracy = 0.31591796875, val acc = 0.1953125\n",
      "(616) loss = 1.9055, batch accuracy = 0.31005859375, val acc = 0.2057291716337204\n",
      "(617) loss = 1.9054, batch accuracy = 0.3193359375, val acc = 0.1927083432674408\n",
      "(618) loss = 1.9075, batch accuracy = 0.3182373046875, val acc = 0.1979166716337204\n",
      "(619) loss = 1.9055, batch accuracy = 0.3165283203125, val acc = 0.1796875\n",
      "(620) loss = 1.8969, batch accuracy = 0.31640625, val acc = 0.1875\n",
      "(621) loss = 1.8980, batch accuracy = 0.3206787109375, val acc = 0.1822916716337204\n",
      "(622) loss = 1.8983, batch accuracy = 0.319091796875, val acc = 0.1822916716337204\n",
      "(623) loss = 1.8977, batch accuracy = 0.3182373046875, val acc = 0.1822916716337204\n",
      "(624) loss = 1.8936, batch accuracy = 0.320068359375, val acc = 0.1875\n",
      "(625) loss = 1.9102, batch accuracy = 0.3177490234375, val acc = 0.1875\n",
      "(626) loss = 1.9035, batch accuracy = 0.3228759765625, val acc = 0.1822916716337204\n",
      "(627) loss = 1.9094, batch accuracy = 0.316162109375, val acc = 0.1770833432674408\n",
      "(628) loss = 1.9067, batch accuracy = 0.3189697265625, val acc = 0.1901041716337204\n",
      "(629) loss = 1.9175, batch accuracy = 0.31787109375, val acc = 0.1901041716337204\n",
      "(630) loss = 1.9150, batch accuracy = 0.3133544921875, val acc = 0.1848958432674408\n",
      "(631) loss = 1.9103, batch accuracy = 0.3138427734375, val acc = 0.1770833432674408\n",
      "(632) loss = 1.8974, batch accuracy = 0.3203125, val acc = 0.1822916716337204\n",
      "(633) loss = 1.9162, batch accuracy = 0.31640625, val acc = 0.1848958432674408\n",
      "(634) loss = 1.9045, batch accuracy = 0.318115234375, val acc = 0.2005208432674408\n",
      "(635) loss = 1.9048, batch accuracy = 0.319091796875, val acc = 0.1822916716337204\n",
      "(636) loss = 1.8992, batch accuracy = 0.322509765625, val acc = 0.1848958432674408\n",
      "(637) loss = 1.8957, batch accuracy = 0.3179931640625, val acc = 0.1848958432674408\n",
      "(638) loss = 1.9023, batch accuracy = 0.318603515625, val acc = 0.1875\n",
      "(639) loss = 1.9002, batch accuracy = 0.3150634765625, val acc = 0.1901041716337204\n",
      "(640) loss = 1.8990, batch accuracy = 0.3197021484375, val acc = 0.1744791716337204\n",
      "(641) loss = 1.9032, batch accuracy = 0.3175048828125, val acc = 0.1796875\n",
      "(642) loss = 1.8962, batch accuracy = 0.318359375, val acc = 0.1640625\n",
      "(643) loss = 1.9024, batch accuracy = 0.318603515625, val acc = 0.1796875\n",
      "(644) loss = 1.8918, batch accuracy = 0.3209228515625, val acc = 0.1640625\n",
      "(645) loss = 1.9080, batch accuracy = 0.3153076171875, val acc = 0.1796875\n",
      "(646) loss = 1.9009, batch accuracy = 0.318359375, val acc = 0.1744791716337204\n",
      "(647) loss = 1.8973, batch accuracy = 0.318603515625, val acc = 0.1927083432674408\n",
      "(648) loss = 1.8949, batch accuracy = 0.3192138671875, val acc = 0.1796875\n",
      "(649) loss = 1.9001, batch accuracy = 0.319091796875, val acc = 0.1744791716337204\n",
      "(650) loss = 1.8971, batch accuracy = 0.3201904296875, val acc = 0.1927083432674408\n",
      "(651) loss = 1.9080, batch accuracy = 0.316162109375, val acc = 0.1770833432674408\n",
      "(652) loss = 1.9061, batch accuracy = 0.3192138671875, val acc = 0.171875\n",
      "(653) loss = 1.9063, batch accuracy = 0.318359375, val acc = 0.171875\n",
      "(654) loss = 1.9045, batch accuracy = 0.3182373046875, val acc = 0.1875\n",
      "(655) loss = 1.9052, batch accuracy = 0.31884765625, val acc = 0.1875\n",
      "(656) loss = 1.9087, batch accuracy = 0.3194580078125, val acc = 0.1822916716337204\n",
      "(657) loss = 1.9091, batch accuracy = 0.318115234375, val acc = 0.1875\n",
      "(658) loss = 1.9074, batch accuracy = 0.3167724609375, val acc = 0.1822916716337204\n",
      "(659) loss = 1.9083, batch accuracy = 0.3160400390625, val acc = 0.1875\n",
      "(660) loss = 1.9034, batch accuracy = 0.319091796875, val acc = 0.1770833432674408\n",
      "(661) loss = 1.9065, batch accuracy = 0.3167724609375, val acc = 0.1744791716337204\n",
      "(662) loss = 1.9014, batch accuracy = 0.318603515625, val acc = 0.1848958432674408\n",
      "(663) loss = 1.8984, batch accuracy = 0.3194580078125, val acc = 0.1770833432674408\n",
      "(664) loss = 1.8998, batch accuracy = 0.3160400390625, val acc = 0.2005208432674408\n",
      "(665) loss = 1.9046, batch accuracy = 0.3150634765625, val acc = 0.1848958432674408\n",
      "(666) loss = 1.9079, batch accuracy = 0.3155517578125, val acc = 0.1927083432674408\n",
      "(667) loss = 1.9015, batch accuracy = 0.3179931640625, val acc = 0.1692708432674408\n",
      "(668) loss = 1.9064, batch accuracy = 0.3155517578125, val acc = 0.1875\n",
      "(669) loss = 1.9073, batch accuracy = 0.318115234375, val acc = 0.1796875\n",
      "(670) loss = 1.9112, batch accuracy = 0.314453125, val acc = 0.1927083432674408\n",
      "(671) loss = 1.9178, batch accuracy = 0.3116455078125, val acc = 0.171875\n",
      "(672) loss = 1.9123, batch accuracy = 0.3170166015625, val acc = 0.1848958432674408\n",
      "(673) loss = 1.9142, batch accuracy = 0.315673828125, val acc = 0.1927083432674408\n",
      "(674) loss = 1.9142, batch accuracy = 0.31494140625, val acc = 0.1796875\n",
      "(675) loss = 1.9138, batch accuracy = 0.3148193359375, val acc = 0.203125\n",
      "(676) loss = 1.9110, batch accuracy = 0.31494140625, val acc = 0.1796875\n",
      "(677) loss = 1.9133, batch accuracy = 0.3128662109375, val acc = 0.1744791716337204\n",
      "(678) loss = 1.8988, batch accuracy = 0.3201904296875, val acc = 0.1796875\n",
      "(679) loss = 1.8971, batch accuracy = 0.3173828125, val acc = 0.1927083432674408\n",
      "(680) loss = 1.8945, batch accuracy = 0.3150634765625, val acc = 0.1901041716337204\n",
      "(681) loss = 1.8926, batch accuracy = 0.31787109375, val acc = 0.1770833432674408\n",
      "(682) loss = 1.9033, batch accuracy = 0.320068359375, val acc = 0.1666666716337204\n",
      "(683) loss = 1.8901, batch accuracy = 0.3189697265625, val acc = 0.1770833432674408\n",
      "(684) loss = 1.8989, batch accuracy = 0.3214111328125, val acc = 0.1822916716337204\n",
      "(685) loss = 1.8951, batch accuracy = 0.322265625, val acc = 0.171875\n",
      "(686) loss = 1.8931, batch accuracy = 0.3232421875, val acc = 0.171875\n",
      "(687) loss = 1.8932, batch accuracy = 0.32275390625, val acc = 0.1901041716337204\n",
      "(688) loss = 1.8880, batch accuracy = 0.320068359375, val acc = 0.1770833432674408\n",
      "(689) loss = 1.9053, batch accuracy = 0.3194580078125, val acc = 0.171875\n",
      "(690) loss = 1.8892, batch accuracy = 0.3221435546875, val acc = 0.1692708432674408\n",
      "(691) loss = 1.8883, batch accuracy = 0.3206787109375, val acc = 0.1770833432674408\n",
      "(692) loss = 1.8921, batch accuracy = 0.32080078125, val acc = 0.1770833432674408\n",
      "(693) loss = 1.8900, batch accuracy = 0.320556640625, val acc = 0.1796875\n",
      "(694) loss = 1.8975, batch accuracy = 0.3175048828125, val acc = 0.1822916716337204\n",
      "(695) loss = 1.8999, batch accuracy = 0.3199462890625, val acc = 0.171875\n",
      "(696) loss = 1.8935, batch accuracy = 0.3199462890625, val acc = 0.1692708432674408\n",
      "(697) loss = 1.8908, batch accuracy = 0.321533203125, val acc = 0.1744791716337204\n",
      "(698) loss = 1.8980, batch accuracy = 0.320556640625, val acc = 0.1744791716337204\n",
      "(699) loss = 1.8910, batch accuracy = 0.322509765625, val acc = 0.1744791716337204\n",
      "(700) loss = 1.8959, batch accuracy = 0.3187255859375, val acc = 0.171875\n",
      "(701) loss = 1.8957, batch accuracy = 0.322998046875, val acc = 0.1770833432674408\n",
      "(702) loss = 1.9027, batch accuracy = 0.317626953125, val acc = 0.1822916716337204\n",
      "(703) loss = 1.8926, batch accuracy = 0.32275390625, val acc = 0.1770833432674408\n",
      "(704) loss = 1.8932, batch accuracy = 0.3209228515625, val acc = 0.1796875\n",
      "(705) loss = 1.8917, batch accuracy = 0.3204345703125, val acc = 0.1796875\n",
      "(706) loss = 1.8933, batch accuracy = 0.3226318359375, val acc = 0.1848958432674408\n",
      "(707) loss = 1.9013, batch accuracy = 0.3192138671875, val acc = 0.1927083432674408\n",
      "(708) loss = 1.9109, batch accuracy = 0.31982421875, val acc = 0.1953125\n",
      "(709) loss = 1.9203, batch accuracy = 0.318603515625, val acc = 0.1953125\n",
      "(710) loss = 1.9315, batch accuracy = 0.3172607421875, val acc = 0.1953125\n",
      "(711) loss = 1.9185, batch accuracy = 0.3167724609375, val acc = 0.1796875\n",
      "(712) loss = 1.9053, batch accuracy = 0.318359375, val acc = 0.1848958432674408\n",
      "(713) loss = 1.9015, batch accuracy = 0.3148193359375, val acc = 0.171875\n",
      "(714) loss = 1.8955, batch accuracy = 0.3184814453125, val acc = 0.1744791716337204\n",
      "(715) loss = 1.8899, batch accuracy = 0.321533203125, val acc = 0.1770833432674408\n",
      "(716) loss = 1.8884, batch accuracy = 0.320068359375, val acc = 0.1796875\n",
      "(717) loss = 1.8851, batch accuracy = 0.3226318359375, val acc = 0.1744791716337204\n",
      "(718) loss = 1.8883, batch accuracy = 0.319580078125, val acc = 0.1848958432674408\n",
      "(719) loss = 1.8898, batch accuracy = 0.3218994140625, val acc = 0.1848958432674408\n",
      "(720) loss = 1.8910, batch accuracy = 0.3199462890625, val acc = 0.1848958432674408\n",
      "(721) loss = 1.8936, batch accuracy = 0.320068359375, val acc = 0.1822916716337204\n",
      "(722) loss = 1.8858, batch accuracy = 0.3211669921875, val acc = 0.1822916716337204\n",
      "(723) loss = 1.8870, batch accuracy = 0.3238525390625, val acc = 0.1901041716337204\n",
      "(724) loss = 1.8981, batch accuracy = 0.3216552734375, val acc = 0.171875\n",
      "(725) loss = 1.8999, batch accuracy = 0.3206787109375, val acc = 0.1875\n",
      "(726) loss = 1.8936, batch accuracy = 0.3204345703125, val acc = 0.1770833432674408\n",
      "(727) loss = 1.8946, batch accuracy = 0.3206787109375, val acc = 0.1901041716337204\n",
      "(728) loss = 1.8887, batch accuracy = 0.3206787109375, val acc = 0.1770833432674408\n",
      "(729) loss = 1.8993, batch accuracy = 0.3197021484375, val acc = 0.1875\n",
      "(730) loss = 1.8897, batch accuracy = 0.319580078125, val acc = 0.1901041716337204\n",
      "(731) loss = 1.8887, batch accuracy = 0.322509765625, val acc = 0.1796875\n",
      "(732) loss = 1.8942, batch accuracy = 0.32080078125, val acc = 0.1848958432674408\n",
      "(733) loss = 1.8961, batch accuracy = 0.321044921875, val acc = 0.1927083432674408\n",
      "(734) loss = 1.8971, batch accuracy = 0.3206787109375, val acc = 0.1901041716337204\n",
      "(735) loss = 1.8887, batch accuracy = 0.3199462890625, val acc = 0.1796875\n",
      "(736) loss = 1.8986, batch accuracy = 0.318603515625, val acc = 0.1848958432674408\n",
      "(737) loss = 1.9042, batch accuracy = 0.3179931640625, val acc = 0.1770833432674408\n",
      "(738) loss = 1.9023, batch accuracy = 0.3175048828125, val acc = 0.1770833432674408\n",
      "(739) loss = 1.8915, batch accuracy = 0.323486328125, val acc = 0.1822916716337204\n",
      "(740) loss = 1.8929, batch accuracy = 0.322265625, val acc = 0.1796875\n",
      "(741) loss = 1.8936, batch accuracy = 0.31982421875, val acc = 0.1744791716337204\n",
      "(742) loss = 1.8939, batch accuracy = 0.3204345703125, val acc = 0.1796875\n",
      "(743) loss = 1.8976, batch accuracy = 0.317626953125, val acc = 0.171875\n",
      "(744) loss = 1.8979, batch accuracy = 0.320068359375, val acc = 0.1770833432674408\n",
      "(745) loss = 1.9105, batch accuracy = 0.31787109375, val acc = 0.1796875\n",
      "(746) loss = 1.9081, batch accuracy = 0.319091796875, val acc = 0.1796875\n",
      "(747) loss = 1.9129, batch accuracy = 0.3157958984375, val acc = 0.1744791716337204\n",
      "(748) loss = 1.9145, batch accuracy = 0.3140869140625, val acc = 0.1770833432674408\n",
      "(749) loss = 1.9151, batch accuracy = 0.3148193359375, val acc = 0.1796875\n",
      "(750) loss = 1.9158, batch accuracy = 0.3128662109375, val acc = 0.1796875\n",
      "(751) loss = 1.9120, batch accuracy = 0.31396484375, val acc = 0.1744791716337204\n",
      "(752) loss = 1.9090, batch accuracy = 0.316650390625, val acc = 0.1770833432674408\n",
      "(753) loss = 1.9093, batch accuracy = 0.314453125, val acc = 0.1796875\n",
      "(754) loss = 1.9191, batch accuracy = 0.3123779296875, val acc = 0.1770833432674408\n",
      "(755) loss = 1.9013, batch accuracy = 0.31396484375, val acc = 0.1822916716337204\n",
      "(756) loss = 1.8887, batch accuracy = 0.3228759765625, val acc = 0.1796875\n",
      "(757) loss = 1.9023, batch accuracy = 0.3197021484375, val acc = 0.1979166716337204\n",
      "(758) loss = 1.8896, batch accuracy = 0.3214111328125, val acc = 0.1875\n",
      "(759) loss = 1.8960, batch accuracy = 0.3206787109375, val acc = 0.1744791716337204\n",
      "(760) loss = 1.8992, batch accuracy = 0.3187255859375, val acc = 0.1822916716337204\n",
      "(761) loss = 1.8977, batch accuracy = 0.31982421875, val acc = 0.1927083432674408\n",
      "(762) loss = 1.9061, batch accuracy = 0.3203125, val acc = 0.1848958432674408\n",
      "(763) loss = 1.9002, batch accuracy = 0.3201904296875, val acc = 0.1953125\n",
      "(764) loss = 1.9038, batch accuracy = 0.3192138671875, val acc = 0.1901041716337204\n",
      "(765) loss = 1.9009, batch accuracy = 0.318603515625, val acc = 0.1822916716337204\n",
      "(766) loss = 1.8895, batch accuracy = 0.319091796875, val acc = 0.1875\n",
      "(767) loss = 1.9014, batch accuracy = 0.3228759765625, val acc = 0.1901041716337204\n",
      "(768) loss = 1.9037, batch accuracy = 0.319580078125, val acc = 0.1953125\n",
      "(769) loss = 1.8973, batch accuracy = 0.320068359375, val acc = 0.1953125\n",
      "(770) loss = 1.8921, batch accuracy = 0.3212890625, val acc = 0.2005208432674408\n",
      "(771) loss = 1.8974, batch accuracy = 0.320556640625, val acc = 0.1770833432674408\n",
      "(772) loss = 1.8933, batch accuracy = 0.32568359375, val acc = 0.1875\n",
      "(773) loss = 1.8944, batch accuracy = 0.3245849609375, val acc = 0.1901041716337204\n",
      "(774) loss = 1.9085, batch accuracy = 0.31982421875, val acc = 0.1875\n",
      "(775) loss = 1.9088, batch accuracy = 0.3203125, val acc = 0.1796875\n",
      "(776) loss = 1.8967, batch accuracy = 0.321044921875, val acc = 0.1822916716337204\n",
      "(777) loss = 1.9036, batch accuracy = 0.319091796875, val acc = 0.1796875\n",
      "(778) loss = 1.8907, batch accuracy = 0.3211669921875, val acc = 0.1848958432674408\n",
      "(779) loss = 1.9006, batch accuracy = 0.3173828125, val acc = 0.1822916716337204\n",
      "(780) loss = 1.8913, batch accuracy = 0.3245849609375, val acc = 0.171875\n",
      "(781) loss = 1.8877, batch accuracy = 0.3226318359375, val acc = 0.1875\n",
      "(782) loss = 1.8844, batch accuracy = 0.32470703125, val acc = 0.1848958432674408\n",
      "(783) loss = 1.8896, batch accuracy = 0.322998046875, val acc = 0.1927083432674408\n",
      "(784) loss = 1.8882, batch accuracy = 0.3204345703125, val acc = 0.1927083432674408\n",
      "(785) loss = 1.8881, batch accuracy = 0.32080078125, val acc = 0.1848958432674408\n",
      "(786) loss = 1.8858, batch accuracy = 0.322265625, val acc = 0.1822916716337204\n",
      "(787) loss = 1.8913, batch accuracy = 0.3251953125, val acc = 0.1901041716337204\n",
      "(788) loss = 1.8847, batch accuracy = 0.3221435546875, val acc = 0.1770833432674408\n",
      "(789) loss = 1.8875, batch accuracy = 0.3231201171875, val acc = 0.1875\n",
      "(790) loss = 1.8821, batch accuracy = 0.32373046875, val acc = 0.1848958432674408\n",
      "(791) loss = 1.8878, batch accuracy = 0.3199462890625, val acc = 0.1822916716337204\n",
      "(792) loss = 1.8782, batch accuracy = 0.3260498046875, val acc = 0.1822916716337204\n",
      "(793) loss = 1.8754, batch accuracy = 0.32666015625, val acc = 0.1822916716337204\n",
      "(794) loss = 1.8839, batch accuracy = 0.321533203125, val acc = 0.1848958432674408\n",
      "(795) loss = 1.8816, batch accuracy = 0.32568359375, val acc = 0.1953125\n",
      "(796) loss = 1.8840, batch accuracy = 0.3243408203125, val acc = 0.1927083432674408\n",
      "(797) loss = 1.8831, batch accuracy = 0.3272705078125, val acc = 0.1848958432674408\n",
      "(798) loss = 1.8815, batch accuracy = 0.3240966796875, val acc = 0.1770833432674408\n",
      "(799) loss = 1.8865, batch accuracy = 0.322509765625, val acc = 0.1822916716337204\n",
      "(800) loss = 1.8781, batch accuracy = 0.3270263671875, val acc = 0.1875\n",
      "(801) loss = 1.8832, batch accuracy = 0.3280029296875, val acc = 0.1822916716337204\n",
      "(802) loss = 1.8873, batch accuracy = 0.3271484375, val acc = 0.1875\n",
      "(803) loss = 1.8859, batch accuracy = 0.32373046875, val acc = 0.171875\n",
      "(804) loss = 1.8950, batch accuracy = 0.3209228515625, val acc = 0.1744791716337204\n",
      "(805) loss = 1.8844, batch accuracy = 0.32568359375, val acc = 0.1848958432674408\n",
      "(806) loss = 1.8965, batch accuracy = 0.3206787109375, val acc = 0.1901041716337204\n",
      "(807) loss = 1.8831, batch accuracy = 0.3211669921875, val acc = 0.1927083432674408\n",
      "(808) loss = 1.8918, batch accuracy = 0.31884765625, val acc = 0.1953125\n",
      "(809) loss = 1.8867, batch accuracy = 0.3255615234375, val acc = 0.1875\n",
      "(810) loss = 1.8937, batch accuracy = 0.3182373046875, val acc = 0.1770833432674408\n",
      "(811) loss = 1.9103, batch accuracy = 0.3233642578125, val acc = 0.1848958432674408\n",
      "(812) loss = 1.8976, batch accuracy = 0.3243408203125, val acc = 0.1848958432674408\n",
      "(813) loss = 1.8914, batch accuracy = 0.3262939453125, val acc = 0.1822916716337204\n",
      "(814) loss = 1.8947, batch accuracy = 0.3243408203125, val acc = 0.1744791716337204\n",
      "(815) loss = 1.8959, batch accuracy = 0.3240966796875, val acc = 0.1822916716337204\n",
      "(816) loss = 1.9026, batch accuracy = 0.3192138671875, val acc = 0.1875\n",
      "(817) loss = 1.9079, batch accuracy = 0.3231201171875, val acc = 0.1796875\n",
      "(818) loss = 1.8833, batch accuracy = 0.324462890625, val acc = 0.1770833432674408\n",
      "(819) loss = 1.8876, batch accuracy = 0.323974609375, val acc = 0.1796875\n",
      "(820) loss = 1.8733, batch accuracy = 0.326904296875, val acc = 0.1770833432674408\n",
      "(821) loss = 1.8840, batch accuracy = 0.3231201171875, val acc = 0.1901041716337204\n",
      "(822) loss = 1.8840, batch accuracy = 0.3251953125, val acc = 0.1822916716337204\n",
      "(823) loss = 1.8860, batch accuracy = 0.3248291015625, val acc = 0.1901041716337204\n",
      "(824) loss = 1.8840, batch accuracy = 0.3271484375, val acc = 0.1848958432674408\n",
      "(825) loss = 1.8887, batch accuracy = 0.32568359375, val acc = 0.1901041716337204\n",
      "(826) loss = 1.8861, batch accuracy = 0.324951171875, val acc = 0.1770833432674408\n",
      "(827) loss = 1.8824, batch accuracy = 0.3258056640625, val acc = 0.1953125\n",
      "(828) loss = 1.8869, batch accuracy = 0.321044921875, val acc = 0.1796875\n",
      "(829) loss = 1.8854, batch accuracy = 0.3253173828125, val acc = 0.1953125\n",
      "(830) loss = 1.8937, batch accuracy = 0.3245849609375, val acc = 0.1666666716337204\n",
      "(831) loss = 1.8891, batch accuracy = 0.3212890625, val acc = 0.1848958432674408\n",
      "(832) loss = 1.8841, batch accuracy = 0.3218994140625, val acc = 0.1848958432674408\n",
      "(833) loss = 1.9011, batch accuracy = 0.3206787109375, val acc = 0.1927083432674408\n",
      "(834) loss = 1.9007, batch accuracy = 0.32177734375, val acc = 0.1979166716337204\n",
      "(835) loss = 1.8904, batch accuracy = 0.3194580078125, val acc = 0.1953125\n",
      "(836) loss = 1.8914, batch accuracy = 0.3240966796875, val acc = 0.1822916716337204\n",
      "(837) loss = 1.8946, batch accuracy = 0.3238525390625, val acc = 0.1822916716337204\n",
      "(838) loss = 1.8915, batch accuracy = 0.3248291015625, val acc = 0.171875\n",
      "(839) loss = 1.8918, batch accuracy = 0.3214111328125, val acc = 0.1927083432674408\n",
      "(840) loss = 1.8883, batch accuracy = 0.3240966796875, val acc = 0.1770833432674408\n",
      "(841) loss = 1.8942, batch accuracy = 0.3240966796875, val acc = 0.1822916716337204\n",
      "(842) loss = 1.8972, batch accuracy = 0.3245849609375, val acc = 0.1848958432674408\n",
      "(843) loss = 1.8889, batch accuracy = 0.3228759765625, val acc = 0.1796875\n",
      "(844) loss = 1.8981, batch accuracy = 0.3197021484375, val acc = 0.1927083432674408\n",
      "(845) loss = 1.8863, batch accuracy = 0.320068359375, val acc = 0.1770833432674408\n",
      "(846) loss = 1.8892, batch accuracy = 0.3216552734375, val acc = 0.1848958432674408\n",
      "(847) loss = 1.8977, batch accuracy = 0.3223876953125, val acc = 0.1770833432674408\n",
      "(848) loss = 1.9005, batch accuracy = 0.32080078125, val acc = 0.1822916716337204\n",
      "(849) loss = 1.8896, batch accuracy = 0.3240966796875, val acc = 0.1822916716337204\n",
      "(850) loss = 1.8990, batch accuracy = 0.3204345703125, val acc = 0.1901041716337204\n",
      "(851) loss = 1.8899, batch accuracy = 0.321533203125, val acc = 0.1875\n",
      "(852) loss = 1.8991, batch accuracy = 0.32080078125, val acc = 0.1848958432674408\n",
      "(853) loss = 1.8957, batch accuracy = 0.32177734375, val acc = 0.1848958432674408\n",
      "(854) loss = 1.9023, batch accuracy = 0.3216552734375, val acc = 0.1848958432674408\n",
      "(855) loss = 1.8994, batch accuracy = 0.322265625, val acc = 0.1796875\n",
      "(856) loss = 1.8978, batch accuracy = 0.3204345703125, val acc = 0.1848958432674408\n",
      "(857) loss = 1.8999, batch accuracy = 0.3232421875, val acc = 0.1875\n",
      "(858) loss = 1.8931, batch accuracy = 0.3182373046875, val acc = 0.1927083432674408\n",
      "(859) loss = 1.8807, batch accuracy = 0.3238525390625, val acc = 0.1848958432674408\n",
      "(860) loss = 1.8888, batch accuracy = 0.324951171875, val acc = 0.1901041716337204\n",
      "(861) loss = 1.8862, batch accuracy = 0.3265380859375, val acc = 0.1875\n",
      "(862) loss = 1.8801, batch accuracy = 0.3267822265625, val acc = 0.1822916716337204\n",
      "(863) loss = 1.8849, batch accuracy = 0.32421875, val acc = 0.1901041716337204\n",
      "(864) loss = 1.9056, batch accuracy = 0.322265625, val acc = 0.1927083432674408\n",
      "(865) loss = 1.8879, batch accuracy = 0.32470703125, val acc = 0.1848958432674408\n",
      "(866) loss = 1.8921, batch accuracy = 0.325439453125, val acc = 0.1822916716337204\n",
      "(867) loss = 1.8884, batch accuracy = 0.325439453125, val acc = 0.1822916716337204\n",
      "(868) loss = 1.8851, batch accuracy = 0.3231201171875, val acc = 0.1744791716337204\n",
      "(869) loss = 1.8817, batch accuracy = 0.3270263671875, val acc = 0.1770833432674408\n",
      "(870) loss = 1.8832, batch accuracy = 0.3258056640625, val acc = 0.1770833432674408\n",
      "(871) loss = 1.8900, batch accuracy = 0.3251953125, val acc = 0.1927083432674408\n",
      "(872) loss = 1.8827, batch accuracy = 0.323974609375, val acc = 0.1770833432674408\n",
      "(873) loss = 1.8931, batch accuracy = 0.3232421875, val acc = 0.1796875\n",
      "(874) loss = 1.8970, batch accuracy = 0.323974609375, val acc = 0.1875\n",
      "(875) loss = 1.8895, batch accuracy = 0.32763671875, val acc = 0.1744791716337204\n",
      "(876) loss = 1.8924, batch accuracy = 0.32421875, val acc = 0.1848958432674408\n",
      "(877) loss = 1.8924, batch accuracy = 0.32861328125, val acc = 0.1796875\n",
      "(878) loss = 1.8975, batch accuracy = 0.3250732421875, val acc = 0.1796875\n",
      "(879) loss = 1.8883, batch accuracy = 0.3248291015625, val acc = 0.1848958432674408\n",
      "(880) loss = 1.8849, batch accuracy = 0.323974609375, val acc = 0.1927083432674408\n",
      "(881) loss = 1.8906, batch accuracy = 0.324462890625, val acc = 0.1927083432674408\n",
      "(882) loss = 1.8905, batch accuracy = 0.325439453125, val acc = 0.1822916716337204\n",
      "(883) loss = 1.8856, batch accuracy = 0.3250732421875, val acc = 0.2005208432674408\n",
      "(884) loss = 1.8913, batch accuracy = 0.3265380859375, val acc = 0.1692708432674408\n",
      "(885) loss = 1.8815, batch accuracy = 0.3265380859375, val acc = 0.1875\n",
      "(886) loss = 1.8832, batch accuracy = 0.3258056640625, val acc = 0.1848958432674408\n",
      "(887) loss = 1.8834, batch accuracy = 0.3251953125, val acc = 0.1822916716337204\n",
      "(888) loss = 1.8904, batch accuracy = 0.3231201171875, val acc = 0.1875\n",
      "(889) loss = 1.8911, batch accuracy = 0.32666015625, val acc = 0.1796875\n",
      "(890) loss = 1.8871, batch accuracy = 0.323974609375, val acc = 0.1744791716337204\n",
      "(891) loss = 1.8869, batch accuracy = 0.32470703125, val acc = 0.1927083432674408\n",
      "(892) loss = 1.8889, batch accuracy = 0.3206787109375, val acc = 0.1901041716337204\n",
      "(893) loss = 1.8780, batch accuracy = 0.3294677734375, val acc = 0.1848958432674408\n",
      "(894) loss = 1.8807, batch accuracy = 0.3265380859375, val acc = 0.1848958432674408\n",
      "(895) loss = 1.8768, batch accuracy = 0.32958984375, val acc = 0.1822916716337204\n",
      "(896) loss = 1.8837, batch accuracy = 0.32568359375, val acc = 0.1953125\n",
      "(897) loss = 1.8810, batch accuracy = 0.328125, val acc = 0.1901041716337204\n",
      "(898) loss = 1.8730, batch accuracy = 0.32763671875, val acc = 0.1796875\n",
      "(899) loss = 1.8754, batch accuracy = 0.3294677734375, val acc = 0.1848958432674408\n",
      "(900) loss = 1.8705, batch accuracy = 0.3280029296875, val acc = 0.1822916716337204\n",
      "(901) loss = 1.8805, batch accuracy = 0.325927734375, val acc = 0.1901041716337204\n",
      "(902) loss = 1.8862, batch accuracy = 0.3270263671875, val acc = 0.1822916716337204\n",
      "(903) loss = 1.8820, batch accuracy = 0.325927734375, val acc = 0.1796875\n",
      "(904) loss = 1.8767, batch accuracy = 0.3292236328125, val acc = 0.1901041716337204\n",
      "(905) loss = 1.8852, batch accuracy = 0.3255615234375, val acc = 0.1901041716337204\n",
      "(906) loss = 1.8816, batch accuracy = 0.3238525390625, val acc = 0.1875\n",
      "(907) loss = 1.8880, batch accuracy = 0.3267822265625, val acc = 0.1848958432674408\n",
      "(908) loss = 1.8843, batch accuracy = 0.3258056640625, val acc = 0.1822916716337204\n",
      "(909) loss = 1.8924, batch accuracy = 0.32421875, val acc = 0.1848958432674408\n",
      "(910) loss = 1.8749, batch accuracy = 0.3250732421875, val acc = 0.1901041716337204\n",
      "(911) loss = 1.8809, batch accuracy = 0.325927734375, val acc = 0.1927083432674408\n",
      "(912) loss = 1.8854, batch accuracy = 0.3270263671875, val acc = 0.1822916716337204\n",
      "(913) loss = 1.8822, batch accuracy = 0.3277587890625, val acc = 0.1953125\n",
      "(914) loss = 1.8898, batch accuracy = 0.322998046875, val acc = 0.1927083432674408\n",
      "(915) loss = 1.8815, batch accuracy = 0.3251953125, val acc = 0.1875\n",
      "(916) loss = 1.8857, batch accuracy = 0.32421875, val acc = 0.1927083432674408\n",
      "(917) loss = 1.8741, batch accuracy = 0.3284912109375, val acc = 0.1901041716337204\n",
      "(918) loss = 1.8774, batch accuracy = 0.3277587890625, val acc = 0.1848958432674408\n",
      "(919) loss = 1.8829, batch accuracy = 0.326416015625, val acc = 0.1796875\n",
      "(920) loss = 1.8733, batch accuracy = 0.3282470703125, val acc = 0.1796875\n",
      "(921) loss = 1.8815, batch accuracy = 0.3250732421875, val acc = 0.1953125\n",
      "(922) loss = 1.8735, batch accuracy = 0.3277587890625, val acc = 0.1953125\n",
      "(923) loss = 1.8852, batch accuracy = 0.3280029296875, val acc = 0.1927083432674408\n",
      "(924) loss = 1.8756, batch accuracy = 0.32861328125, val acc = 0.1848958432674408\n",
      "(925) loss = 1.8764, batch accuracy = 0.327880859375, val acc = 0.1927083432674408\n",
      "(926) loss = 1.8713, batch accuracy = 0.328857421875, val acc = 0.1979166716337204\n",
      "(927) loss = 1.8921, batch accuracy = 0.3275146484375, val acc = 0.1875\n",
      "(928) loss = 1.8923, batch accuracy = 0.3221435546875, val acc = 0.1979166716337204\n",
      "(929) loss = 1.8874, batch accuracy = 0.3260498046875, val acc = 0.1901041716337204\n",
      "(930) loss = 1.8838, batch accuracy = 0.326416015625, val acc = 0.1901041716337204\n",
      "(931) loss = 1.8774, batch accuracy = 0.3302001953125, val acc = 0.1796875\n",
      "(932) loss = 1.8711, batch accuracy = 0.3277587890625, val acc = 0.1744791716337204\n",
      "(933) loss = 1.8748, batch accuracy = 0.3299560546875, val acc = 0.1822916716337204\n",
      "(934) loss = 1.8649, batch accuracy = 0.331787109375, val acc = 0.1901041716337204\n",
      "(935) loss = 1.8746, batch accuracy = 0.32763671875, val acc = 0.1770833432674408\n",
      "(936) loss = 1.8707, batch accuracy = 0.3280029296875, val acc = 0.1901041716337204\n",
      "(937) loss = 1.8727, batch accuracy = 0.3289794921875, val acc = 0.1770833432674408\n",
      "(938) loss = 1.8743, batch accuracy = 0.327392578125, val acc = 0.1848958432674408\n",
      "(939) loss = 1.8754, batch accuracy = 0.3304443359375, val acc = 0.1927083432674408\n",
      "(940) loss = 1.8772, batch accuracy = 0.329345703125, val acc = 0.203125\n",
      "(941) loss = 1.8760, batch accuracy = 0.32763671875, val acc = 0.1901041716337204\n",
      "(942) loss = 1.8802, batch accuracy = 0.326904296875, val acc = 0.1927083432674408\n",
      "(943) loss = 1.8798, batch accuracy = 0.3251953125, val acc = 0.2057291716337204\n",
      "(944) loss = 1.8884, batch accuracy = 0.3275146484375, val acc = 0.1979166716337204\n",
      "(945) loss = 1.8935, batch accuracy = 0.3272705078125, val acc = 0.1953125\n",
      "(946) loss = 1.8824, batch accuracy = 0.32666015625, val acc = 0.2005208432674408\n",
      "(947) loss = 1.8807, batch accuracy = 0.328369140625, val acc = 0.1848958432674408\n",
      "(948) loss = 1.8823, batch accuracy = 0.3267822265625, val acc = 0.1875\n",
      "(949) loss = 1.8796, batch accuracy = 0.32763671875, val acc = 0.2005208432674408\n",
      "(950) loss = 1.8806, batch accuracy = 0.3271484375, val acc = 0.1796875\n",
      "(951) loss = 1.8926, batch accuracy = 0.32421875, val acc = 0.1875\n",
      "(952) loss = 1.9009, batch accuracy = 0.322021484375, val acc = 0.1744791716337204\n",
      "(953) loss = 1.8944, batch accuracy = 0.3218994140625, val acc = 0.1822916716337204\n",
      "(954) loss = 1.8863, batch accuracy = 0.3262939453125, val acc = 0.1927083432674408\n",
      "(955) loss = 1.8937, batch accuracy = 0.324951171875, val acc = 0.171875\n",
      "(956) loss = 1.8873, batch accuracy = 0.3236083984375, val acc = 0.1848958432674408\n",
      "(957) loss = 1.8778, batch accuracy = 0.3284912109375, val acc = 0.1901041716337204\n",
      "(958) loss = 1.8789, batch accuracy = 0.3267822265625, val acc = 0.1770833432674408\n",
      "(959) loss = 1.8826, batch accuracy = 0.330810546875, val acc = 0.1796875\n",
      "(960) loss = 1.8787, batch accuracy = 0.3271484375, val acc = 0.171875\n",
      "(961) loss = 1.8857, batch accuracy = 0.3251953125, val acc = 0.2005208432674408\n",
      "(962) loss = 1.8780, batch accuracy = 0.3262939453125, val acc = 0.1875\n",
      "(963) loss = 1.8726, batch accuracy = 0.3250732421875, val acc = 0.1953125\n",
      "(964) loss = 1.8728, batch accuracy = 0.3306884765625, val acc = 0.1796875\n",
      "(965) loss = 1.8788, batch accuracy = 0.3262939453125, val acc = 0.1848958432674408\n",
      "(966) loss = 1.8710, batch accuracy = 0.3291015625, val acc = 0.1848958432674408\n",
      "(967) loss = 1.8830, batch accuracy = 0.3282470703125, val acc = 0.1979166716337204\n",
      "(968) loss = 1.8677, batch accuracy = 0.327880859375, val acc = 0.1848958432674408\n",
      "(969) loss = 1.8737, batch accuracy = 0.3319091796875, val acc = 0.1875\n",
      "(970) loss = 1.8637, batch accuracy = 0.3304443359375, val acc = 0.1901041716337204\n",
      "(971) loss = 1.8719, batch accuracy = 0.33056640625, val acc = 0.1744791716337204\n",
      "(972) loss = 1.8716, batch accuracy = 0.330078125, val acc = 0.1822916716337204\n",
      "(973) loss = 1.8740, batch accuracy = 0.3297119140625, val acc = 0.1875\n",
      "(974) loss = 1.8774, batch accuracy = 0.328369140625, val acc = 0.1875\n",
      "(975) loss = 1.8788, batch accuracy = 0.3289794921875, val acc = 0.1796875\n",
      "(976) loss = 1.8862, batch accuracy = 0.330078125, val acc = 0.1848958432674408\n",
      "(977) loss = 1.8895, batch accuracy = 0.3232421875, val acc = 0.1927083432674408\n",
      "(978) loss = 1.8801, batch accuracy = 0.3223876953125, val acc = 0.1979166716337204\n",
      "(979) loss = 1.8874, batch accuracy = 0.326904296875, val acc = 0.1901041716337204\n",
      "(980) loss = 1.8919, batch accuracy = 0.3248291015625, val acc = 0.1875\n",
      "(981) loss = 1.8829, batch accuracy = 0.3292236328125, val acc = 0.1848958432674408\n",
      "(982) loss = 1.8832, batch accuracy = 0.3236083984375, val acc = 0.1848958432674408\n",
      "(983) loss = 1.8729, batch accuracy = 0.3267822265625, val acc = 0.2005208432674408\n",
      "(984) loss = 1.8751, batch accuracy = 0.327392578125, val acc = 0.1822916716337204\n",
      "(985) loss = 1.8913, batch accuracy = 0.3277587890625, val acc = 0.1901041716337204\n",
      "(986) loss = 1.8785, batch accuracy = 0.3280029296875, val acc = 0.1822916716337204\n",
      "(987) loss = 1.8828, batch accuracy = 0.326171875, val acc = 0.1848958432674408\n",
      "(988) loss = 1.8780, batch accuracy = 0.3270263671875, val acc = 0.1875\n",
      "(989) loss = 1.8813, batch accuracy = 0.3267822265625, val acc = 0.1901041716337204\n",
      "(990) loss = 1.8853, batch accuracy = 0.3277587890625, val acc = 0.1901041716337204\n",
      "(991) loss = 1.8817, batch accuracy = 0.32421875, val acc = 0.1796875\n",
      "(992) loss = 1.8741, batch accuracy = 0.3272705078125, val acc = 0.1770833432674408\n",
      "(993) loss = 1.8792, batch accuracy = 0.3243408203125, val acc = 0.1744791716337204\n",
      "(994) loss = 1.8724, batch accuracy = 0.32666015625, val acc = 0.1848958432674408\n",
      "(995) loss = 1.8789, batch accuracy = 0.3297119140625, val acc = 0.1770833432674408\n",
      "(996) loss = 1.8768, batch accuracy = 0.3255615234375, val acc = 0.1796875\n",
      "(997) loss = 1.8725, batch accuracy = 0.331298828125, val acc = 0.171875\n",
      "(998) loss = 1.8763, batch accuracy = 0.32763671875, val acc = 0.1744791716337204\n",
      "(999) loss = 1.8774, batch accuracy = 0.327880859375, val acc = 0.1875\n",
      "(1000) loss = 1.8777, batch accuracy = 0.3280029296875, val acc = 0.1901041716337204\n",
      "(1001) loss = 1.8773, batch accuracy = 0.328125, val acc = 0.1979166716337204\n",
      "(1002) loss = 1.8741, batch accuracy = 0.331787109375, val acc = 0.1796875\n",
      "(1003) loss = 1.8800, batch accuracy = 0.3280029296875, val acc = 0.1875\n",
      "(1004) loss = 1.8732, batch accuracy = 0.33203125, val acc = 0.1901041716337204\n",
      "(1005) loss = 1.8801, batch accuracy = 0.326904296875, val acc = 0.1875\n",
      "(1006) loss = 1.8715, batch accuracy = 0.329833984375, val acc = 0.1927083432674408\n",
      "(1007) loss = 1.8761, batch accuracy = 0.326904296875, val acc = 0.1848958432674408\n",
      "(1008) loss = 1.8820, batch accuracy = 0.326416015625, val acc = 0.1901041716337204\n",
      "(1009) loss = 1.8947, batch accuracy = 0.3243408203125, val acc = 0.1901041716337204\n",
      "(1010) loss = 1.8951, batch accuracy = 0.323974609375, val acc = 0.1796875\n",
      "(1011) loss = 1.8898, batch accuracy = 0.326171875, val acc = 0.1796875\n",
      "(1012) loss = 1.8831, batch accuracy = 0.324951171875, val acc = 0.2083333432674408\n",
      "(1013) loss = 1.8812, batch accuracy = 0.3270263671875, val acc = 0.1822916716337204\n",
      "(1014) loss = 1.8753, batch accuracy = 0.3262939453125, val acc = 0.1901041716337204\n",
      "(1015) loss = 1.8727, batch accuracy = 0.3287353515625, val acc = 0.1796875\n",
      "(1016) loss = 1.8756, batch accuracy = 0.3323974609375, val acc = 0.2109375\n",
      "(1017) loss = 1.8756, batch accuracy = 0.330078125, val acc = 0.1927083432674408\n",
      "(1018) loss = 1.8732, batch accuracy = 0.32861328125, val acc = 0.1927083432674408\n",
      "(1019) loss = 1.8726, batch accuracy = 0.3282470703125, val acc = 0.1901041716337204\n",
      "(1020) loss = 1.8679, batch accuracy = 0.3299560546875, val acc = 0.1875\n",
      "(1021) loss = 1.8726, batch accuracy = 0.328857421875, val acc = 0.1953125\n",
      "(1022) loss = 1.8682, batch accuracy = 0.3292236328125, val acc = 0.1875\n",
      "(1023) loss = 1.8704, batch accuracy = 0.3291015625, val acc = 0.1927083432674408\n",
      "(1024) loss = 1.8714, batch accuracy = 0.3291015625, val acc = 0.1901041716337204\n",
      "(1025) loss = 1.8731, batch accuracy = 0.3304443359375, val acc = 0.2005208432674408\n",
      "(1026) loss = 1.8723, batch accuracy = 0.3267822265625, val acc = 0.1875\n",
      "(1027) loss = 1.8675, batch accuracy = 0.3304443359375, val acc = 0.1848958432674408\n",
      "(1028) loss = 1.8725, batch accuracy = 0.325439453125, val acc = 0.1848958432674408\n",
      "(1029) loss = 1.8729, batch accuracy = 0.3291015625, val acc = 0.1848958432674408\n",
      "(1030) loss = 1.8811, batch accuracy = 0.323974609375, val acc = 0.1901041716337204\n",
      "(1031) loss = 1.8728, batch accuracy = 0.328857421875, val acc = 0.1927083432674408\n",
      "(1032) loss = 1.8737, batch accuracy = 0.3275146484375, val acc = 0.1822916716337204\n",
      "(1033) loss = 1.8774, batch accuracy = 0.3287353515625, val acc = 0.1848958432674408\n",
      "(1034) loss = 1.8737, batch accuracy = 0.329345703125, val acc = 0.1927083432674408\n",
      "(1035) loss = 1.8728, batch accuracy = 0.326416015625, val acc = 0.1875\n",
      "(1036) loss = 1.8767, batch accuracy = 0.3277587890625, val acc = 0.2005208432674408\n",
      "(1037) loss = 1.8780, batch accuracy = 0.324951171875, val acc = 0.1953125\n",
      "(1038) loss = 1.8933, batch accuracy = 0.3238525390625, val acc = 0.1979166716337204\n",
      "(1039) loss = 1.8851, batch accuracy = 0.3228759765625, val acc = 0.1901041716337204\n",
      "(1040) loss = 1.8761, batch accuracy = 0.32666015625, val acc = 0.2005208432674408\n",
      "(1041) loss = 1.8703, batch accuracy = 0.3262939453125, val acc = 0.1927083432674408\n",
      "(1042) loss = 1.8722, batch accuracy = 0.3292236328125, val acc = 0.203125\n",
      "(1043) loss = 1.8755, batch accuracy = 0.3236083984375, val acc = 0.1875\n",
      "(1044) loss = 1.8707, batch accuracy = 0.3287353515625, val acc = 0.1979166716337204\n",
      "(1045) loss = 1.8696, batch accuracy = 0.3287353515625, val acc = 0.2005208432674408\n",
      "(1046) loss = 1.8727, batch accuracy = 0.3258056640625, val acc = 0.1848958432674408\n",
      "(1047) loss = 1.8691, batch accuracy = 0.3333740234375, val acc = 0.1953125\n",
      "(1048) loss = 1.8786, batch accuracy = 0.329833984375, val acc = 0.1875\n",
      "(1049) loss = 1.8692, batch accuracy = 0.332763671875, val acc = 0.1770833432674408\n",
      "(1050) loss = 1.8743, batch accuracy = 0.328125, val acc = 0.1796875\n",
      "(1051) loss = 1.8750, batch accuracy = 0.32958984375, val acc = 0.1901041716337204\n",
      "(1052) loss = 1.8745, batch accuracy = 0.3289794921875, val acc = 0.1927083432674408\n",
      "(1053) loss = 1.8766, batch accuracy = 0.3309326171875, val acc = 0.1875\n",
      "(1054) loss = 1.8786, batch accuracy = 0.326904296875, val acc = 0.1953125\n",
      "(1055) loss = 1.8754, batch accuracy = 0.32666015625, val acc = 0.203125\n",
      "(1056) loss = 1.8716, batch accuracy = 0.325927734375, val acc = 0.1875\n",
      "(1057) loss = 1.8756, batch accuracy = 0.3277587890625, val acc = 0.1901041716337204\n",
      "(1058) loss = 1.8696, batch accuracy = 0.3306884765625, val acc = 0.1901041716337204\n",
      "(1059) loss = 1.8707, batch accuracy = 0.3275146484375, val acc = 0.1979166716337204\n",
      "(1060) loss = 1.8708, batch accuracy = 0.3321533203125, val acc = 0.1796875\n",
      "(1061) loss = 1.8762, batch accuracy = 0.3292236328125, val acc = 0.1875\n",
      "(1062) loss = 1.8691, batch accuracy = 0.3304443359375, val acc = 0.1796875\n",
      "(1063) loss = 1.8663, batch accuracy = 0.32861328125, val acc = 0.1796875\n",
      "(1064) loss = 1.8808, batch accuracy = 0.326904296875, val acc = 0.1822916716337204\n",
      "(1065) loss = 1.8774, batch accuracy = 0.328857421875, val acc = 0.1848958432674408\n",
      "(1066) loss = 1.8780, batch accuracy = 0.3282470703125, val acc = 0.2005208432674408\n",
      "(1067) loss = 1.8769, batch accuracy = 0.325439453125, val acc = 0.1770833432674408\n",
      "(1068) loss = 1.8913, batch accuracy = 0.3272705078125, val acc = 0.1979166716337204\n",
      "(1069) loss = 1.8778, batch accuracy = 0.32666015625, val acc = 0.1927083432674408\n",
      "(1070) loss = 1.8870, batch accuracy = 0.3231201171875, val acc = 0.1953125\n",
      "(1071) loss = 1.8777, batch accuracy = 0.327392578125, val acc = 0.1927083432674408\n",
      "(1072) loss = 1.8769, batch accuracy = 0.3262939453125, val acc = 0.1953125\n",
      "(1073) loss = 1.8805, batch accuracy = 0.3272705078125, val acc = 0.2005208432674408\n",
      "(1074) loss = 1.8900, batch accuracy = 0.323974609375, val acc = 0.1953125\n",
      "(1075) loss = 1.8775, batch accuracy = 0.3272705078125, val acc = 0.1901041716337204\n",
      "(1076) loss = 1.8775, batch accuracy = 0.326416015625, val acc = 0.1770833432674408\n",
      "(1077) loss = 1.8770, batch accuracy = 0.323974609375, val acc = 0.2005208432674408\n",
      "(1078) loss = 1.8836, batch accuracy = 0.3277587890625, val acc = 0.1901041716337204\n",
      "(1079) loss = 1.8716, batch accuracy = 0.3292236328125, val acc = 0.1901041716337204\n",
      "(1080) loss = 1.8865, batch accuracy = 0.325927734375, val acc = 0.2057291716337204\n",
      "(1081) loss = 1.8808, batch accuracy = 0.32763671875, val acc = 0.1927083432674408\n",
      "(1082) loss = 1.8792, batch accuracy = 0.328369140625, val acc = 0.2135416716337204\n",
      "(1083) loss = 1.8736, batch accuracy = 0.3267822265625, val acc = 0.2005208432674408\n",
      "(1084) loss = 1.8725, batch accuracy = 0.3289794921875, val acc = 0.1953125\n",
      "(1085) loss = 1.8743, batch accuracy = 0.3275146484375, val acc = 0.1927083432674408\n",
      "(1086) loss = 1.8639, batch accuracy = 0.33056640625, val acc = 0.1953125\n",
      "(1087) loss = 1.8713, batch accuracy = 0.3271484375, val acc = 0.2005208432674408\n",
      "(1088) loss = 1.8662, batch accuracy = 0.3284912109375, val acc = 0.2005208432674408\n",
      "(1089) loss = 1.8713, batch accuracy = 0.3304443359375, val acc = 0.203125\n",
      "(1090) loss = 1.8673, batch accuracy = 0.330322265625, val acc = 0.1875\n",
      "(1091) loss = 1.8764, batch accuracy = 0.32421875, val acc = 0.1927083432674408\n",
      "(1092) loss = 1.8740, batch accuracy = 0.3240966796875, val acc = 0.1953125\n",
      "(1093) loss = 1.8749, batch accuracy = 0.3284912109375, val acc = 0.203125\n",
      "(1094) loss = 1.8738, batch accuracy = 0.3297119140625, val acc = 0.1901041716337204\n",
      "(1095) loss = 1.8886, batch accuracy = 0.3240966796875, val acc = 0.1927083432674408\n",
      "(1096) loss = 1.8856, batch accuracy = 0.3245849609375, val acc = 0.1953125\n",
      "(1097) loss = 1.8771, batch accuracy = 0.3258056640625, val acc = 0.1979166716337204\n",
      "(1098) loss = 1.8847, batch accuracy = 0.327392578125, val acc = 0.2109375\n",
      "(1099) loss = 1.8816, batch accuracy = 0.3291015625, val acc = 0.2005208432674408\n",
      "(1100) loss = 1.8777, batch accuracy = 0.328857421875, val acc = 0.2005208432674408\n",
      "(1101) loss = 1.8652, batch accuracy = 0.32958984375, val acc = 0.1901041716337204\n",
      "(1102) loss = 1.8767, batch accuracy = 0.3297119140625, val acc = 0.2057291716337204\n",
      "(1103) loss = 1.8884, batch accuracy = 0.3251953125, val acc = 0.2005208432674408\n",
      "(1104) loss = 1.8773, batch accuracy = 0.3287353515625, val acc = 0.1848958432674408\n",
      "(1105) loss = 1.8763, batch accuracy = 0.32763671875, val acc = 0.2005208432674408\n",
      "(1106) loss = 1.8797, batch accuracy = 0.3291015625, val acc = 0.2005208432674408\n",
      "(1107) loss = 1.8849, batch accuracy = 0.325439453125, val acc = 0.1927083432674408\n",
      "(1108) loss = 1.8802, batch accuracy = 0.325439453125, val acc = 0.1848958432674408\n",
      "(1109) loss = 1.8848, batch accuracy = 0.330322265625, val acc = 0.1953125\n",
      "(1110) loss = 1.8814, batch accuracy = 0.330810546875, val acc = 0.1875\n",
      "(1111) loss = 1.8838, batch accuracy = 0.328369140625, val acc = 0.1979166716337204\n",
      "(1112) loss = 1.8790, batch accuracy = 0.3270263671875, val acc = 0.1927083432674408\n",
      "(1113) loss = 1.8828, batch accuracy = 0.3267822265625, val acc = 0.1848958432674408\n",
      "(1114) loss = 1.8873, batch accuracy = 0.3243408203125, val acc = 0.1796875\n",
      "(1115) loss = 1.8865, batch accuracy = 0.3236083984375, val acc = 0.1770833432674408\n",
      "(1116) loss = 1.8900, batch accuracy = 0.3251953125, val acc = 0.1979166716337204\n",
      "(1117) loss = 1.8854, batch accuracy = 0.3250732421875, val acc = 0.1796875\n",
      "(1118) loss = 1.8840, batch accuracy = 0.3267822265625, val acc = 0.1979166716337204\n",
      "(1119) loss = 1.8838, batch accuracy = 0.3270263671875, val acc = 0.1875\n",
      "(1120) loss = 1.8871, batch accuracy = 0.3260498046875, val acc = 0.1901041716337204\n",
      "(1121) loss = 1.8921, batch accuracy = 0.32763671875, val acc = 0.1927083432674408\n",
      "(1122) loss = 1.8810, batch accuracy = 0.32666015625, val acc = 0.1822916716337204\n",
      "(1123) loss = 1.8778, batch accuracy = 0.327392578125, val acc = 0.1979166716337204\n",
      "(1124) loss = 1.8758, batch accuracy = 0.3284912109375, val acc = 0.1901041716337204\n",
      "(1125) loss = 1.8779, batch accuracy = 0.3271484375, val acc = 0.1875\n",
      "(1126) loss = 1.8852, batch accuracy = 0.329345703125, val acc = 0.2005208432674408\n",
      "(1127) loss = 1.8824, batch accuracy = 0.3272705078125, val acc = 0.1901041716337204\n",
      "(1128) loss = 1.8903, batch accuracy = 0.3258056640625, val acc = 0.1953125\n",
      "(1129) loss = 1.8772, batch accuracy = 0.3319091796875, val acc = 0.1927083432674408\n",
      "(1130) loss = 1.8718, batch accuracy = 0.3304443359375, val acc = 0.203125\n",
      "(1131) loss = 1.8735, batch accuracy = 0.32958984375, val acc = 0.2005208432674408\n",
      "(1132) loss = 1.8665, batch accuracy = 0.3309326171875, val acc = 0.1901041716337204\n",
      "(1133) loss = 1.8829, batch accuracy = 0.3310546875, val acc = 0.1875\n",
      "(1134) loss = 1.8666, batch accuracy = 0.332275390625, val acc = 0.1927083432674408\n",
      "(1135) loss = 1.8804, batch accuracy = 0.3287353515625, val acc = 0.1901041716337204\n",
      "(1136) loss = 1.8766, batch accuracy = 0.3277587890625, val acc = 0.1901041716337204\n",
      "(1137) loss = 1.8791, batch accuracy = 0.3309326171875, val acc = 0.1875\n",
      "(1138) loss = 1.8923, batch accuracy = 0.325439453125, val acc = 0.1901041716337204\n",
      "(1139) loss = 1.8653, batch accuracy = 0.33154296875, val acc = 0.1953125\n",
      "(1140) loss = 1.8774, batch accuracy = 0.3282470703125, val acc = 0.1822916716337204\n",
      "(1141) loss = 1.8648, batch accuracy = 0.3355712890625, val acc = 0.1848958432674408\n",
      "(1142) loss = 1.8807, batch accuracy = 0.327880859375, val acc = 0.1822916716337204\n",
      "(1143) loss = 1.8714, batch accuracy = 0.3294677734375, val acc = 0.1848958432674408\n",
      "(1144) loss = 1.8728, batch accuracy = 0.33203125, val acc = 0.1901041716337204\n",
      "(1145) loss = 1.8649, batch accuracy = 0.3336181640625, val acc = 0.1822916716337204\n",
      "(1146) loss = 1.8703, batch accuracy = 0.3316650390625, val acc = 0.1875\n",
      "(1147) loss = 1.8691, batch accuracy = 0.330078125, val acc = 0.1822916716337204\n",
      "(1148) loss = 1.8666, batch accuracy = 0.333740234375, val acc = 0.1848958432674408\n",
      "(1149) loss = 1.8668, batch accuracy = 0.3314208984375, val acc = 0.1953125\n",
      "(1150) loss = 1.8664, batch accuracy = 0.33642578125, val acc = 0.1875\n",
      "(1151) loss = 1.8629, batch accuracy = 0.33154296875, val acc = 0.1875\n",
      "(1152) loss = 1.8605, batch accuracy = 0.3326416015625, val acc = 0.1848958432674408\n",
      "(1153) loss = 1.8599, batch accuracy = 0.33544921875, val acc = 0.1901041716337204\n",
      "(1154) loss = 1.8572, batch accuracy = 0.338623046875, val acc = 0.1927083432674408\n",
      "(1155) loss = 1.8588, batch accuracy = 0.3328857421875, val acc = 0.1875\n",
      "(1156) loss = 1.8611, batch accuracy = 0.33251953125, val acc = 0.1848958432674408\n",
      "(1157) loss = 1.8579, batch accuracy = 0.336669921875, val acc = 0.1927083432674408\n",
      "(1158) loss = 1.8537, batch accuracy = 0.3359375, val acc = 0.2057291716337204\n",
      "(1159) loss = 1.8617, batch accuracy = 0.3349609375, val acc = 0.1848958432674408\n",
      "(1160) loss = 1.8565, batch accuracy = 0.3321533203125, val acc = 0.1796875\n",
      "(1161) loss = 1.8736, batch accuracy = 0.3353271484375, val acc = 0.1979166716337204\n",
      "(1162) loss = 1.8561, batch accuracy = 0.335205078125, val acc = 0.1901041716337204\n",
      "(1163) loss = 1.8662, batch accuracy = 0.33203125, val acc = 0.1901041716337204\n",
      "(1164) loss = 1.8788, batch accuracy = 0.329833984375, val acc = 0.1901041716337204\n",
      "(1165) loss = 1.8674, batch accuracy = 0.3323974609375, val acc = 0.2005208432674408\n",
      "(1166) loss = 1.8656, batch accuracy = 0.332275390625, val acc = 0.1848958432674408\n",
      "(1167) loss = 1.8763, batch accuracy = 0.33154296875, val acc = 0.1822916716337204\n",
      "(1168) loss = 1.8708, batch accuracy = 0.3272705078125, val acc = 0.1979166716337204\n",
      "(1169) loss = 1.8669, batch accuracy = 0.3331298828125, val acc = 0.1901041716337204\n",
      "(1170) loss = 1.8652, batch accuracy = 0.3297119140625, val acc = 0.1901041716337204\n",
      "(1171) loss = 1.8611, batch accuracy = 0.3316650390625, val acc = 0.1901041716337204\n",
      "(1172) loss = 1.8581, batch accuracy = 0.3333740234375, val acc = 0.1796875\n",
      "(1173) loss = 1.8597, batch accuracy = 0.3343505859375, val acc = 0.1875\n",
      "(1174) loss = 1.8615, batch accuracy = 0.3360595703125, val acc = 0.1927083432674408\n",
      "(1175) loss = 1.8606, batch accuracy = 0.3343505859375, val acc = 0.1796875\n",
      "(1176) loss = 1.8660, batch accuracy = 0.3319091796875, val acc = 0.1901041716337204\n",
      "(1177) loss = 1.8666, batch accuracy = 0.328857421875, val acc = 0.1875\n",
      "(1178) loss = 1.8722, batch accuracy = 0.3326416015625, val acc = 0.1822916716337204\n",
      "(1179) loss = 1.8735, batch accuracy = 0.3292236328125, val acc = 0.1848958432674408\n",
      "(1180) loss = 1.8744, batch accuracy = 0.3302001953125, val acc = 0.1901041716337204\n",
      "(1181) loss = 1.8653, batch accuracy = 0.333740234375, val acc = 0.1796875\n",
      "(1182) loss = 1.8676, batch accuracy = 0.329345703125, val acc = 0.1953125\n",
      "(1183) loss = 1.8818, batch accuracy = 0.32666015625, val acc = 0.1901041716337204\n",
      "(1184) loss = 1.8732, batch accuracy = 0.3297119140625, val acc = 0.1927083432674408\n",
      "(1185) loss = 1.8737, batch accuracy = 0.3292236328125, val acc = 0.1875\n",
      "(1186) loss = 1.8808, batch accuracy = 0.329345703125, val acc = 0.1953125\n",
      "(1187) loss = 1.8838, batch accuracy = 0.324951171875, val acc = 0.1848958432674408\n",
      "(1188) loss = 1.8779, batch accuracy = 0.32666015625, val acc = 0.1901041716337204\n",
      "(1189) loss = 1.8653, batch accuracy = 0.332763671875, val acc = 0.1796875\n",
      "(1190) loss = 1.8675, batch accuracy = 0.3330078125, val acc = 0.1927083432674408\n",
      "(1191) loss = 1.8952, batch accuracy = 0.32861328125, val acc = 0.2083333432674408\n",
      "(1192) loss = 1.8929, batch accuracy = 0.3245849609375, val acc = 0.1875\n",
      "(1193) loss = 1.8786, batch accuracy = 0.327880859375, val acc = 0.1822916716337204\n",
      "(1194) loss = 1.8684, batch accuracy = 0.32958984375, val acc = 0.1848958432674408\n",
      "(1195) loss = 1.8672, batch accuracy = 0.32861328125, val acc = 0.1927083432674408\n",
      "(1196) loss = 1.8608, batch accuracy = 0.330810546875, val acc = 0.1953125\n",
      "(1197) loss = 1.8621, batch accuracy = 0.3328857421875, val acc = 0.1848958432674408\n",
      "(1198) loss = 1.8635, batch accuracy = 0.335205078125, val acc = 0.1953125\n",
      "(1199) loss = 1.8535, batch accuracy = 0.33251953125, val acc = 0.1875\n",
      "(1200) loss = 1.8622, batch accuracy = 0.3336181640625, val acc = 0.1953125\n",
      "(1201) loss = 1.8554, batch accuracy = 0.33447265625, val acc = 0.1796875\n",
      "(1202) loss = 1.8668, batch accuracy = 0.332275390625, val acc = 0.1901041716337204\n",
      "(1203) loss = 1.8660, batch accuracy = 0.333984375, val acc = 0.1848958432674408\n",
      "(1204) loss = 1.8751, batch accuracy = 0.3294677734375, val acc = 0.1822916716337204\n",
      "(1205) loss = 1.8808, batch accuracy = 0.3245849609375, val acc = 0.1848958432674408\n",
      "(1206) loss = 1.8833, batch accuracy = 0.32861328125, val acc = 0.1927083432674408\n",
      "(1207) loss = 1.8772, batch accuracy = 0.3292236328125, val acc = 0.1822916716337204\n",
      "(1208) loss = 1.8767, batch accuracy = 0.330322265625, val acc = 0.1848958432674408\n",
      "(1209) loss = 1.8593, batch accuracy = 0.3321533203125, val acc = 0.1822916716337204\n",
      "(1210) loss = 1.8654, batch accuracy = 0.33154296875, val acc = 0.1875\n",
      "(1211) loss = 1.8500, batch accuracy = 0.33447265625, val acc = 0.1979166716337204\n",
      "(1212) loss = 1.8602, batch accuracy = 0.332275390625, val acc = 0.2005208432674408\n",
      "(1213) loss = 1.8650, batch accuracy = 0.3314208984375, val acc = 0.203125\n",
      "(1214) loss = 1.8714, batch accuracy = 0.3304443359375, val acc = 0.1953125\n",
      "(1215) loss = 1.8776, batch accuracy = 0.3294677734375, val acc = 0.1875\n",
      "(1216) loss = 1.8701, batch accuracy = 0.3289794921875, val acc = 0.1822916716337204\n",
      "(1217) loss = 1.8612, batch accuracy = 0.3341064453125, val acc = 0.1927083432674408\n",
      "(1218) loss = 1.8732, batch accuracy = 0.3240966796875, val acc = 0.1875\n",
      "(1219) loss = 1.8847, batch accuracy = 0.327880859375, val acc = 0.1927083432674408\n",
      "(1220) loss = 1.9033, batch accuracy = 0.3218994140625, val acc = 0.1796875\n",
      "(1221) loss = 1.8935, batch accuracy = 0.327392578125, val acc = 0.1901041716337204\n",
      "(1222) loss = 1.8954, batch accuracy = 0.3238525390625, val acc = 0.1848958432674408\n",
      "(1223) loss = 1.8786, batch accuracy = 0.32275390625, val acc = 0.1744791716337204\n",
      "(1224) loss = 1.8828, batch accuracy = 0.328369140625, val acc = 0.1848958432674408\n",
      "(1225) loss = 1.8890, batch accuracy = 0.3231201171875, val acc = 0.1848958432674408\n",
      "(1226) loss = 1.8741, batch accuracy = 0.3262939453125, val acc = 0.1901041716337204\n",
      "(1227) loss = 1.8695, batch accuracy = 0.32763671875, val acc = 0.1901041716337204\n",
      "(1228) loss = 1.8680, batch accuracy = 0.3297119140625, val acc = 0.1979166716337204\n",
      "(1229) loss = 1.8735, batch accuracy = 0.32861328125, val acc = 0.1875\n",
      "(1230) loss = 1.8737, batch accuracy = 0.33056640625, val acc = 0.1927083432674408\n",
      "(1231) loss = 1.8737, batch accuracy = 0.3280029296875, val acc = 0.1875\n",
      "(1232) loss = 1.8729, batch accuracy = 0.3291015625, val acc = 0.1875\n",
      "(1233) loss = 1.8798, batch accuracy = 0.3284912109375, val acc = 0.1770833432674408\n",
      "(1234) loss = 1.8646, batch accuracy = 0.33056640625, val acc = 0.1953125\n",
      "(1235) loss = 1.8678, batch accuracy = 0.3321533203125, val acc = 0.1901041716337204\n",
      "(1236) loss = 1.8623, batch accuracy = 0.333984375, val acc = 0.1927083432674408\n",
      "(1237) loss = 1.8663, batch accuracy = 0.33203125, val acc = 0.1848958432674408\n",
      "(1238) loss = 1.8628, batch accuracy = 0.334716796875, val acc = 0.1927083432674408\n",
      "(1239) loss = 1.8594, batch accuracy = 0.3314208984375, val acc = 0.1927083432674408\n",
      "(1240) loss = 1.8601, batch accuracy = 0.33447265625, val acc = 0.1822916716337204\n",
      "(1241) loss = 1.8560, batch accuracy = 0.3336181640625, val acc = 0.1875\n",
      "(1242) loss = 1.8558, batch accuracy = 0.3363037109375, val acc = 0.1953125\n",
      "(1243) loss = 1.8661, batch accuracy = 0.334228515625, val acc = 0.1848958432674408\n",
      "(1244) loss = 1.8567, batch accuracy = 0.3348388671875, val acc = 0.1848958432674408\n",
      "(1245) loss = 1.8638, batch accuracy = 0.3316650390625, val acc = 0.1822916716337204\n",
      "(1246) loss = 1.8585, batch accuracy = 0.333740234375, val acc = 0.1901041716337204\n",
      "(1247) loss = 1.8590, batch accuracy = 0.333251953125, val acc = 0.1901041716337204\n",
      "(1248) loss = 1.8656, batch accuracy = 0.3331298828125, val acc = 0.1796875\n",
      "(1249) loss = 1.8654, batch accuracy = 0.3348388671875, val acc = 0.1901041716337204\n",
      "(1250) loss = 1.8794, batch accuracy = 0.327880859375, val acc = 0.1901041716337204\n",
      "(1251) loss = 1.8672, batch accuracy = 0.329833984375, val acc = 0.1875\n",
      "(1252) loss = 1.8732, batch accuracy = 0.329833984375, val acc = 0.1796875\n",
      "(1253) loss = 1.8686, batch accuracy = 0.333251953125, val acc = 0.1822916716337204\n",
      "(1254) loss = 1.8624, batch accuracy = 0.3343505859375, val acc = 0.1927083432674408\n",
      "(1255) loss = 1.8680, batch accuracy = 0.331298828125, val acc = 0.1822916716337204\n",
      "(1256) loss = 1.8604, batch accuracy = 0.3333740234375, val acc = 0.1901041716337204\n",
      "(1257) loss = 1.8697, batch accuracy = 0.3323974609375, val acc = 0.1848958432674408\n",
      "(1258) loss = 1.8577, batch accuracy = 0.3311767578125, val acc = 0.1953125\n",
      "(1259) loss = 1.8606, batch accuracy = 0.3328857421875, val acc = 0.1901041716337204\n",
      "(1260) loss = 1.8634, batch accuracy = 0.331787109375, val acc = 0.1927083432674408\n",
      "(1261) loss = 1.8601, batch accuracy = 0.3333740234375, val acc = 0.1927083432674408\n",
      "(1262) loss = 1.8610, batch accuracy = 0.3314208984375, val acc = 0.203125\n",
      "(1263) loss = 1.8596, batch accuracy = 0.334228515625, val acc = 0.1875\n",
      "(1264) loss = 1.8574, batch accuracy = 0.3341064453125, val acc = 0.1979166716337204\n",
      "(1265) loss = 1.8576, batch accuracy = 0.3319091796875, val acc = 0.1901041716337204\n",
      "(1266) loss = 1.8670, batch accuracy = 0.331787109375, val acc = 0.1875\n",
      "(1267) loss = 1.8630, batch accuracy = 0.33154296875, val acc = 0.1875\n",
      "(1268) loss = 1.8644, batch accuracy = 0.3328857421875, val acc = 0.1953125\n",
      "(1269) loss = 1.8745, batch accuracy = 0.32958984375, val acc = 0.1927083432674408\n",
      "(1270) loss = 1.8696, batch accuracy = 0.3330078125, val acc = 0.2005208432674408\n",
      "(1271) loss = 1.8661, batch accuracy = 0.3304443359375, val acc = 0.2005208432674408\n",
      "(1272) loss = 1.8701, batch accuracy = 0.3310546875, val acc = 0.1901041716337204\n",
      "(1273) loss = 1.8781, batch accuracy = 0.330078125, val acc = 0.1901041716337204\n",
      "(1274) loss = 1.8742, batch accuracy = 0.328125, val acc = 0.171875\n",
      "(1275) loss = 1.8949, batch accuracy = 0.32763671875, val acc = 0.1875\n",
      "(1276) loss = 1.8735, batch accuracy = 0.32763671875, val acc = 0.1848958432674408\n",
      "(1277) loss = 1.8618, batch accuracy = 0.3310546875, val acc = 0.1953125\n",
      "(1278) loss = 1.8583, batch accuracy = 0.330810546875, val acc = 0.1848958432674408\n",
      "(1279) loss = 1.8676, batch accuracy = 0.3345947265625, val acc = 0.1796875\n",
      "(1280) loss = 1.8626, batch accuracy = 0.3333740234375, val acc = 0.1953125\n",
      "(1281) loss = 1.8599, batch accuracy = 0.3331298828125, val acc = 0.1875\n",
      "(1282) loss = 1.8650, batch accuracy = 0.3326416015625, val acc = 0.1953125\n",
      "(1283) loss = 1.8579, batch accuracy = 0.334228515625, val acc = 0.1875\n",
      "(1284) loss = 1.8769, batch accuracy = 0.334716796875, val acc = 0.1953125\n",
      "(1285) loss = 1.8625, batch accuracy = 0.33447265625, val acc = 0.1927083432674408\n",
      "(1286) loss = 1.8757, batch accuracy = 0.332275390625, val acc = 0.2005208432674408\n",
      "(1287) loss = 1.8609, batch accuracy = 0.330810546875, val acc = 0.1770833432674408\n",
      "(1288) loss = 1.8684, batch accuracy = 0.33642578125, val acc = 0.203125\n",
      "(1289) loss = 1.8606, batch accuracy = 0.3314208984375, val acc = 0.1848958432674408\n",
      "(1290) loss = 1.8698, batch accuracy = 0.3310546875, val acc = 0.1927083432674408\n",
      "(1291) loss = 1.8579, batch accuracy = 0.332275390625, val acc = 0.1848958432674408\n",
      "(1292) loss = 1.8601, batch accuracy = 0.3321533203125, val acc = 0.1848958432674408\n",
      "(1293) loss = 1.8581, batch accuracy = 0.3349609375, val acc = 0.1796875\n",
      "(1294) loss = 1.8744, batch accuracy = 0.3309326171875, val acc = 0.1822916716337204\n",
      "(1295) loss = 1.8693, batch accuracy = 0.3309326171875, val acc = 0.1770833432674408\n",
      "(1296) loss = 1.8642, batch accuracy = 0.330810546875, val acc = 0.1927083432674408\n",
      "(1297) loss = 1.8711, batch accuracy = 0.3294677734375, val acc = 0.1875\n",
      "(1298) loss = 1.8634, batch accuracy = 0.331298828125, val acc = 0.1848958432674408\n",
      "(1299) loss = 1.8721, batch accuracy = 0.3321533203125, val acc = 0.1901041716337204\n",
      "(1300) loss = 1.8654, batch accuracy = 0.3316650390625, val acc = 0.203125\n",
      "(1301) loss = 1.8733, batch accuracy = 0.3291015625, val acc = 0.1822916716337204\n",
      "(1302) loss = 1.8889, batch accuracy = 0.3262939453125, val acc = 0.1875\n",
      "(1303) loss = 1.8987, batch accuracy = 0.3214111328125, val acc = 0.1979166716337204\n",
      "(1304) loss = 1.8998, batch accuracy = 0.326171875, val acc = 0.1822916716337204\n",
      "(1305) loss = 1.8917, batch accuracy = 0.324462890625, val acc = 0.1822916716337204\n",
      "(1306) loss = 1.8803, batch accuracy = 0.329345703125, val acc = 0.1979166716337204\n",
      "(1307) loss = 1.8879, batch accuracy = 0.326171875, val acc = 0.1953125\n",
      "(1308) loss = 1.8908, batch accuracy = 0.3260498046875, val acc = 0.1875\n",
      "(1309) loss = 1.8755, batch accuracy = 0.3275146484375, val acc = 0.1796875\n",
      "(1310) loss = 1.8746, batch accuracy = 0.331787109375, val acc = 0.1770833432674408\n",
      "(1311) loss = 1.8734, batch accuracy = 0.3291015625, val acc = 0.1901041716337204\n",
      "(1312) loss = 1.8645, batch accuracy = 0.3349609375, val acc = 0.1875\n",
      "(1313) loss = 1.8637, batch accuracy = 0.332275390625, val acc = 0.1901041716337204\n",
      "(1314) loss = 1.8611, batch accuracy = 0.3294677734375, val acc = 0.1901041716337204\n",
      "(1315) loss = 1.8680, batch accuracy = 0.3345947265625, val acc = 0.203125\n",
      "(1316) loss = 1.8620, batch accuracy = 0.3284912109375, val acc = 0.2005208432674408\n",
      "(1317) loss = 1.8677, batch accuracy = 0.33740234375, val acc = 0.1927083432674408\n",
      "(1318) loss = 1.8679, batch accuracy = 0.3341064453125, val acc = 0.1848958432674408\n",
      "(1319) loss = 1.8783, batch accuracy = 0.334228515625, val acc = 0.1979166716337204\n",
      "(1320) loss = 1.8661, batch accuracy = 0.33349609375, val acc = 0.1901041716337204\n",
      "(1321) loss = 1.8671, batch accuracy = 0.332763671875, val acc = 0.2005208432674408\n",
      "(1322) loss = 1.8698, batch accuracy = 0.3336181640625, val acc = 0.1875\n",
      "(1323) loss = 1.8664, batch accuracy = 0.33251953125, val acc = 0.1901041716337204\n",
      "(1324) loss = 1.8653, batch accuracy = 0.3311767578125, val acc = 0.1822916716337204\n",
      "(1325) loss = 1.8610, batch accuracy = 0.333984375, val acc = 0.1822916716337204\n",
      "(1326) loss = 1.8651, batch accuracy = 0.33203125, val acc = 0.1927083432674408\n",
      "(1327) loss = 1.8662, batch accuracy = 0.335205078125, val acc = 0.1979166716337204\n",
      "(1328) loss = 1.8791, batch accuracy = 0.326416015625, val acc = 0.2057291716337204\n",
      "(1329) loss = 1.8748, batch accuracy = 0.3294677734375, val acc = 0.1901041716337204\n",
      "(1330) loss = 1.8701, batch accuracy = 0.3310546875, val acc = 0.2005208432674408\n",
      "(1331) loss = 1.8620, batch accuracy = 0.3328857421875, val acc = 0.1848958432674408\n",
      "(1332) loss = 1.8600, batch accuracy = 0.33349609375, val acc = 0.203125\n",
      "(1333) loss = 1.8631, batch accuracy = 0.3319091796875, val acc = 0.1953125\n",
      "(1334) loss = 1.8698, batch accuracy = 0.3333740234375, val acc = 0.1796875\n",
      "(1335) loss = 1.8672, batch accuracy = 0.3330078125, val acc = 0.1848958432674408\n",
      "(1336) loss = 1.8722, batch accuracy = 0.33447265625, val acc = 0.1796875\n",
      "(1337) loss = 1.8715, batch accuracy = 0.3316650390625, val acc = 0.1901041716337204\n",
      "(1338) loss = 1.8775, batch accuracy = 0.334228515625, val acc = 0.1901041716337204\n",
      "(1339) loss = 1.8656, batch accuracy = 0.3291015625, val acc = 0.1901041716337204\n",
      "(1340) loss = 1.8587, batch accuracy = 0.3365478515625, val acc = 0.1901041716337204\n",
      "(1341) loss = 1.8521, batch accuracy = 0.3365478515625, val acc = 0.2005208432674408\n",
      "(1342) loss = 1.8558, batch accuracy = 0.332763671875, val acc = 0.1796875\n",
      "(1343) loss = 1.8530, batch accuracy = 0.3341064453125, val acc = 0.1848958432674408\n",
      "(1344) loss = 1.8511, batch accuracy = 0.3367919921875, val acc = 0.1927083432674408\n",
      "(1345) loss = 1.8585, batch accuracy = 0.3353271484375, val acc = 0.1927083432674408\n",
      "(1346) loss = 1.8620, batch accuracy = 0.3343505859375, val acc = 0.203125\n",
      "(1347) loss = 1.8629, batch accuracy = 0.338134765625, val acc = 0.1901041716337204\n",
      "(1348) loss = 1.8616, batch accuracy = 0.336669921875, val acc = 0.1901041716337204\n",
      "(1349) loss = 1.8621, batch accuracy = 0.3314208984375, val acc = 0.1927083432674408\n",
      "(1350) loss = 1.8613, batch accuracy = 0.3338623046875, val acc = 0.1927083432674408\n",
      "(1351) loss = 1.8637, batch accuracy = 0.3316650390625, val acc = 0.1927083432674408\n",
      "(1352) loss = 1.8604, batch accuracy = 0.3336181640625, val acc = 0.1875\n",
      "(1353) loss = 1.8773, batch accuracy = 0.328369140625, val acc = 0.1901041716337204\n",
      "(1354) loss = 1.8674, batch accuracy = 0.333984375, val acc = 0.1927083432674408\n",
      "(1355) loss = 1.8620, batch accuracy = 0.3316650390625, val acc = 0.1796875\n",
      "(1356) loss = 1.8597, batch accuracy = 0.3348388671875, val acc = 0.1848958432674408\n",
      "(1357) loss = 1.8621, batch accuracy = 0.3355712890625, val acc = 0.1979166716337204\n",
      "(1358) loss = 1.8627, batch accuracy = 0.3343505859375, val acc = 0.1875\n",
      "(1359) loss = 1.8675, batch accuracy = 0.333740234375, val acc = 0.1927083432674408\n",
      "(1360) loss = 1.8895, batch accuracy = 0.3326416015625, val acc = 0.1848958432674408\n",
      "(1361) loss = 1.8901, batch accuracy = 0.3248291015625, val acc = 0.1796875\n",
      "(1362) loss = 1.8954, batch accuracy = 0.326416015625, val acc = 0.1901041716337204\n",
      "(1363) loss = 1.9049, batch accuracy = 0.3275146484375, val acc = 0.1927083432674408\n",
      "(1364) loss = 1.8978, batch accuracy = 0.3240966796875, val acc = 0.1744791716337204\n",
      "(1365) loss = 1.8902, batch accuracy = 0.3289794921875, val acc = 0.1979166716337204\n",
      "(1366) loss = 1.8736, batch accuracy = 0.331787109375, val acc = 0.1901041716337204\n",
      "(1367) loss = 1.8794, batch accuracy = 0.3294677734375, val acc = 0.1927083432674408\n",
      "(1368) loss = 1.8758, batch accuracy = 0.3341064453125, val acc = 0.1901041716337204\n",
      "(1369) loss = 1.8631, batch accuracy = 0.331298828125, val acc = 0.1848958432674408\n",
      "(1370) loss = 1.8587, batch accuracy = 0.3353271484375, val acc = 0.2057291716337204\n",
      "(1371) loss = 1.8580, batch accuracy = 0.3326416015625, val acc = 0.1927083432674408\n",
      "(1372) loss = 1.8605, batch accuracy = 0.3369140625, val acc = 0.2057291716337204\n",
      "(1373) loss = 1.8618, batch accuracy = 0.332275390625, val acc = 0.1927083432674408\n",
      "(1374) loss = 1.8580, batch accuracy = 0.335693359375, val acc = 0.1953125\n",
      "(1375) loss = 1.8548, batch accuracy = 0.3377685546875, val acc = 0.1953125\n",
      "(1376) loss = 1.8588, batch accuracy = 0.3343505859375, val acc = 0.1979166716337204\n",
      "(1377) loss = 1.8619, batch accuracy = 0.3349609375, val acc = 0.2057291716337204\n",
      "(1378) loss = 1.8546, batch accuracy = 0.3345947265625, val acc = 0.1875\n",
      "(1379) loss = 1.8582, batch accuracy = 0.335205078125, val acc = 0.1901041716337204\n",
      "(1380) loss = 1.8626, batch accuracy = 0.335693359375, val acc = 0.1901041716337204\n",
      "(1381) loss = 1.8613, batch accuracy = 0.3355712890625, val acc = 0.1822916716337204\n",
      "(1382) loss = 1.8627, batch accuracy = 0.3343505859375, val acc = 0.1796875\n",
      "(1383) loss = 1.8634, batch accuracy = 0.3331298828125, val acc = 0.1927083432674408\n",
      "(1384) loss = 1.8719, batch accuracy = 0.3289794921875, val acc = 0.1848958432674408\n",
      "(1385) loss = 1.8696, batch accuracy = 0.3360595703125, val acc = 0.203125\n",
      "(1386) loss = 1.8566, batch accuracy = 0.3319091796875, val acc = 0.1901041716337204\n",
      "(1387) loss = 1.8714, batch accuracy = 0.3345947265625, val acc = 0.1927083432674408\n",
      "(1388) loss = 1.8557, batch accuracy = 0.337646484375, val acc = 0.1979166716337204\n",
      "(1389) loss = 1.8648, batch accuracy = 0.335693359375, val acc = 0.1953125\n",
      "(1390) loss = 1.8722, batch accuracy = 0.3350830078125, val acc = 0.1901041716337204\n",
      "(1391) loss = 1.8594, batch accuracy = 0.335693359375, val acc = 0.1927083432674408\n",
      "(1392) loss = 1.8718, batch accuracy = 0.3326416015625, val acc = 0.1953125\n",
      "(1393) loss = 1.8596, batch accuracy = 0.335693359375, val acc = 0.1822916716337204\n",
      "(1394) loss = 1.8689, batch accuracy = 0.330322265625, val acc = 0.1744791716337204\n",
      "(1395) loss = 1.8862, batch accuracy = 0.329833984375, val acc = 0.1822916716337204\n",
      "(1396) loss = 1.8837, batch accuracy = 0.330078125, val acc = 0.2005208432674408\n",
      "(1397) loss = 1.8844, batch accuracy = 0.3299560546875, val acc = 0.1953125\n",
      "(1398) loss = 1.8773, batch accuracy = 0.3294677734375, val acc = 0.1796875\n",
      "(1399) loss = 1.8802, batch accuracy = 0.3297119140625, val acc = 0.1770833432674408\n",
      "(1400) loss = 1.8743, batch accuracy = 0.3319091796875, val acc = 0.1744791716337204\n",
      "(1401) loss = 1.8803, batch accuracy = 0.331787109375, val acc = 0.1796875\n",
      "(1402) loss = 1.8659, batch accuracy = 0.3328857421875, val acc = 0.1744791716337204\n",
      "(1403) loss = 1.8711, batch accuracy = 0.3304443359375, val acc = 0.1953125\n",
      "(1404) loss = 1.8688, batch accuracy = 0.3328857421875, val acc = 0.1901041716337204\n",
      "(1405) loss = 1.8762, batch accuracy = 0.33203125, val acc = 0.1979166716337204\n",
      "(1406) loss = 1.8759, batch accuracy = 0.3292236328125, val acc = 0.1822916716337204\n",
      "(1407) loss = 1.8804, batch accuracy = 0.3302001953125, val acc = 0.1822916716337204\n",
      "(1408) loss = 1.8784, batch accuracy = 0.3299560546875, val acc = 0.1848958432674408\n",
      "(1409) loss = 1.8742, batch accuracy = 0.32861328125, val acc = 0.2005208432674408\n",
      "(1410) loss = 1.8785, batch accuracy = 0.333251953125, val acc = 0.1953125\n",
      "(1411) loss = 1.8833, batch accuracy = 0.330810546875, val acc = 0.1927083432674408\n",
      "(1412) loss = 1.8659, batch accuracy = 0.32861328125, val acc = 0.1848958432674408\n",
      "(1413) loss = 1.8734, batch accuracy = 0.331298828125, val acc = 0.1848958432674408\n",
      "(1414) loss = 1.8649, batch accuracy = 0.3287353515625, val acc = 0.1979166716337204\n",
      "(1415) loss = 1.8715, batch accuracy = 0.332275390625, val acc = 0.1901041716337204\n",
      "(1416) loss = 1.8700, batch accuracy = 0.331787109375, val acc = 0.1875\n",
      "(1417) loss = 1.8668, batch accuracy = 0.3314208984375, val acc = 0.1875\n",
      "(1418) loss = 1.8617, batch accuracy = 0.3316650390625, val acc = 0.1927083432674408\n",
      "(1419) loss = 1.8591, batch accuracy = 0.331787109375, val acc = 0.1822916716337204\n",
      "(1420) loss = 1.8623, batch accuracy = 0.3319091796875, val acc = 0.1901041716337204\n",
      "(1421) loss = 1.8661, batch accuracy = 0.3363037109375, val acc = 0.1875\n",
      "(1422) loss = 1.8674, batch accuracy = 0.33056640625, val acc = 0.1901041716337204\n",
      "(1423) loss = 1.8709, batch accuracy = 0.333984375, val acc = 0.1796875\n",
      "(1424) loss = 1.8699, batch accuracy = 0.33544921875, val acc = 0.1744791716337204\n",
      "(1425) loss = 1.8669, batch accuracy = 0.33544921875, val acc = 0.1875\n",
      "(1426) loss = 1.8642, batch accuracy = 0.3314208984375, val acc = 0.1744791716337204\n",
      "(1427) loss = 1.8757, batch accuracy = 0.3319091796875, val acc = 0.1822916716337204\n",
      "(1428) loss = 1.8618, batch accuracy = 0.3316650390625, val acc = 0.1770833432674408\n",
      "(1429) loss = 1.8748, batch accuracy = 0.3331298828125, val acc = 0.1796875\n",
      "(1430) loss = 1.8677, batch accuracy = 0.3333740234375, val acc = 0.1875\n",
      "(1431) loss = 1.8614, batch accuracy = 0.3348388671875, val acc = 0.1822916716337204\n",
      "(1432) loss = 1.8653, batch accuracy = 0.33349609375, val acc = 0.1822916716337204\n",
      "(1433) loss = 1.8596, batch accuracy = 0.3333740234375, val acc = 0.1692708432674408\n",
      "(1434) loss = 1.8721, batch accuracy = 0.3375244140625, val acc = 0.171875\n",
      "(1435) loss = 1.8628, batch accuracy = 0.33447265625, val acc = 0.1744791716337204\n",
      "(1436) loss = 1.8550, batch accuracy = 0.3338623046875, val acc = 0.1770833432674408\n",
      "(1437) loss = 1.8667, batch accuracy = 0.3330078125, val acc = 0.1875\n",
      "(1438) loss = 1.8676, batch accuracy = 0.329833984375, val acc = 0.1796875\n",
      "(1439) loss = 1.8679, batch accuracy = 0.3338623046875, val acc = 0.1744791716337204\n",
      "(1440) loss = 1.8691, batch accuracy = 0.32861328125, val acc = 0.1953125\n",
      "(1441) loss = 1.8654, batch accuracy = 0.3302001953125, val acc = 0.1770833432674408\n",
      "(1442) loss = 1.8626, batch accuracy = 0.3369140625, val acc = 0.1901041716337204\n",
      "(1443) loss = 1.8565, batch accuracy = 0.3353271484375, val acc = 0.1875\n",
      "(1444) loss = 1.8570, batch accuracy = 0.3321533203125, val acc = 0.1822916716337204\n",
      "(1445) loss = 1.8639, batch accuracy = 0.335205078125, val acc = 0.1744791716337204\n",
      "(1446) loss = 1.8592, batch accuracy = 0.336181640625, val acc = 0.1822916716337204\n",
      "(1447) loss = 1.8601, batch accuracy = 0.3316650390625, val acc = 0.1822916716337204\n",
      "(1448) loss = 1.8623, batch accuracy = 0.3359375, val acc = 0.1744791716337204\n",
      "(1449) loss = 1.8655, batch accuracy = 0.3349609375, val acc = 0.1953125\n",
      "(1450) loss = 1.8723, batch accuracy = 0.3345947265625, val acc = 0.1796875\n",
      "(1451) loss = 1.8737, batch accuracy = 0.3367919921875, val acc = 0.1796875\n",
      "(1452) loss = 1.8615, batch accuracy = 0.3363037109375, val acc = 0.1744791716337204\n",
      "(1453) loss = 1.8639, batch accuracy = 0.3370361328125, val acc = 0.1822916716337204\n",
      "(1454) loss = 1.8628, batch accuracy = 0.336669921875, val acc = 0.1770833432674408\n",
      "(1455) loss = 1.8694, batch accuracy = 0.3341064453125, val acc = 0.1770833432674408\n",
      "(1456) loss = 1.8605, batch accuracy = 0.334228515625, val acc = 0.1744791716337204\n",
      "(1457) loss = 1.8649, batch accuracy = 0.3343505859375, val acc = 0.1796875\n",
      "(1458) loss = 1.8575, batch accuracy = 0.33642578125, val acc = 0.1927083432674408\n",
      "(1459) loss = 1.8585, batch accuracy = 0.33642578125, val acc = 0.1770833432674408\n",
      "(1460) loss = 1.8632, batch accuracy = 0.3358154296875, val acc = 0.1744791716337204\n",
      "(1461) loss = 1.8607, batch accuracy = 0.3355712890625, val acc = 0.1822916716337204\n",
      "(1462) loss = 1.8627, batch accuracy = 0.333740234375, val acc = 0.1770833432674408\n",
      "(1463) loss = 1.8554, batch accuracy = 0.3370361328125, val acc = 0.1927083432674408\n",
      "(1464) loss = 1.8617, batch accuracy = 0.3399658203125, val acc = 0.1927083432674408\n",
      "(1465) loss = 1.8557, batch accuracy = 0.333984375, val acc = 0.1614583432674408\n",
      "(1466) loss = 1.8655, batch accuracy = 0.3330078125, val acc = 0.1822916716337204\n",
      "(1467) loss = 1.8699, batch accuracy = 0.334228515625, val acc = 0.1822916716337204\n",
      "(1468) loss = 1.8698, batch accuracy = 0.3353271484375, val acc = 0.1822916716337204\n",
      "(1469) loss = 1.8682, batch accuracy = 0.3350830078125, val acc = 0.1770833432674408\n",
      "(1470) loss = 1.8611, batch accuracy = 0.334716796875, val acc = 0.1796875\n",
      "(1471) loss = 1.8579, batch accuracy = 0.3350830078125, val acc = 0.1901041716337204\n",
      "(1472) loss = 1.8591, batch accuracy = 0.33740234375, val acc = 0.171875\n",
      "(1473) loss = 1.8663, batch accuracy = 0.335693359375, val acc = 0.1692708432674408\n",
      "(1474) loss = 1.8738, batch accuracy = 0.33251953125, val acc = 0.1744791716337204\n",
      "(1475) loss = 1.8659, batch accuracy = 0.33544921875, val acc = 0.171875\n",
      "(1476) loss = 1.8791, batch accuracy = 0.333740234375, val acc = 0.1796875\n",
      "(1477) loss = 1.8717, batch accuracy = 0.335205078125, val acc = 0.1901041716337204\n",
      "(1478) loss = 1.8967, batch accuracy = 0.3333740234375, val acc = 0.171875\n",
      "(1479) loss = 1.8634, batch accuracy = 0.332763671875, val acc = 0.1744791716337204\n",
      "(1480) loss = 1.8739, batch accuracy = 0.3330078125, val acc = 0.1953125\n",
      "(1481) loss = 1.8576, batch accuracy = 0.3369140625, val acc = 0.1822916716337204\n",
      "(1482) loss = 1.8566, batch accuracy = 0.3369140625, val acc = 0.1692708432674408\n",
      "(1483) loss = 1.8526, batch accuracy = 0.340576171875, val acc = 0.1796875\n",
      "(1484) loss = 1.8569, batch accuracy = 0.3411865234375, val acc = 0.1822916716337204\n",
      "(1485) loss = 1.8619, batch accuracy = 0.3350830078125, val acc = 0.1744791716337204\n",
      "(1486) loss = 1.8555, batch accuracy = 0.3375244140625, val acc = 0.1822916716337204\n",
      "(1487) loss = 1.8588, batch accuracy = 0.339599609375, val acc = 0.1692708432674408\n",
      "(1488) loss = 1.8554, batch accuracy = 0.3375244140625, val acc = 0.1822916716337204\n",
      "(1489) loss = 1.8596, batch accuracy = 0.3330078125, val acc = 0.1692708432674408\n",
      "(1490) loss = 1.8519, batch accuracy = 0.3377685546875, val acc = 0.1692708432674408\n",
      "(1491) loss = 1.8581, batch accuracy = 0.3369140625, val acc = 0.1770833432674408\n",
      "(1492) loss = 1.8529, batch accuracy = 0.3394775390625, val acc = 0.1744791716337204\n",
      "(1493) loss = 1.8534, batch accuracy = 0.3392333984375, val acc = 0.1822916716337204\n",
      "(1494) loss = 1.8539, batch accuracy = 0.33740234375, val acc = 0.1692708432674408\n",
      "(1495) loss = 1.8543, batch accuracy = 0.338623046875, val acc = 0.1770833432674408\n",
      "(1496) loss = 1.8601, batch accuracy = 0.3360595703125, val acc = 0.1744791716337204\n",
      "(1497) loss = 1.8597, batch accuracy = 0.340087890625, val acc = 0.1666666716337204\n",
      "(1498) loss = 1.8592, batch accuracy = 0.339599609375, val acc = 0.1875\n",
      "(1499) loss = 1.8598, batch accuracy = 0.3323974609375, val acc = 0.1822916716337204\n",
      "(1500) loss = 1.8609, batch accuracy = 0.3353271484375, val acc = 0.1848958432674408\n",
      "(1501) loss = 1.8572, batch accuracy = 0.33642578125, val acc = 0.1822916716337204\n",
      "(1502) loss = 1.8602, batch accuracy = 0.3353271484375, val acc = 0.1796875\n",
      "(1503) loss = 1.8656, batch accuracy = 0.3343505859375, val acc = 0.1770833432674408\n",
      "(1504) loss = 1.8697, batch accuracy = 0.334716796875, val acc = 0.1848958432674408\n",
      "(1505) loss = 1.8793, batch accuracy = 0.33154296875, val acc = 0.171875\n",
      "(1506) loss = 1.8800, batch accuracy = 0.331787109375, val acc = 0.1614583432674408\n",
      "(1507) loss = 1.8697, batch accuracy = 0.33251953125, val acc = 0.1666666716337204\n",
      "(1508) loss = 1.8598, batch accuracy = 0.3363037109375, val acc = 0.1744791716337204\n",
      "(1509) loss = 1.8693, batch accuracy = 0.3349609375, val acc = 0.1770833432674408\n",
      "(1510) loss = 1.8684, batch accuracy = 0.3343505859375, val acc = 0.1744791716337204\n",
      "(1511) loss = 1.8636, batch accuracy = 0.3370361328125, val acc = 0.1822916716337204\n",
      "(1512) loss = 1.8623, batch accuracy = 0.3365478515625, val acc = 0.1770833432674408\n",
      "(1513) loss = 1.8536, batch accuracy = 0.3377685546875, val acc = 0.1796875\n",
      "(1514) loss = 1.8624, batch accuracy = 0.336181640625, val acc = 0.1979166716337204\n",
      "(1515) loss = 1.8560, batch accuracy = 0.33642578125, val acc = 0.1822916716337204\n",
      "(1516) loss = 1.8573, batch accuracy = 0.336669921875, val acc = 0.1875\n",
      "(1517) loss = 1.8654, batch accuracy = 0.33642578125, val acc = 0.1848958432674408\n",
      "(1518) loss = 1.8716, batch accuracy = 0.3349609375, val acc = 0.1822916716337204\n",
      "(1519) loss = 1.8656, batch accuracy = 0.3363037109375, val acc = 0.1875\n",
      "(1520) loss = 1.8766, batch accuracy = 0.3330078125, val acc = 0.1822916716337204\n",
      "(1521) loss = 1.8621, batch accuracy = 0.333984375, val acc = 0.1770833432674408\n",
      "(1522) loss = 1.8856, batch accuracy = 0.3299560546875, val acc = 0.1822916716337204\n",
      "(1523) loss = 1.8832, batch accuracy = 0.3328857421875, val acc = 0.1822916716337204\n",
      "(1524) loss = 1.8724, batch accuracy = 0.3341064453125, val acc = 0.1875\n",
      "(1525) loss = 1.8596, batch accuracy = 0.3358154296875, val acc = 0.1875\n",
      "(1526) loss = 1.8604, batch accuracy = 0.333251953125, val acc = 0.1875\n",
      "(1527) loss = 1.8692, batch accuracy = 0.3345947265625, val acc = 0.1848958432674408\n",
      "(1528) loss = 1.8718, batch accuracy = 0.3345947265625, val acc = 0.1744791716337204\n",
      "(1529) loss = 1.8683, batch accuracy = 0.334228515625, val acc = 0.1770833432674408\n",
      "(1530) loss = 1.8647, batch accuracy = 0.3330078125, val acc = 0.1744791716337204\n",
      "(1531) loss = 1.8665, batch accuracy = 0.331787109375, val acc = 0.1875\n",
      "(1532) loss = 1.8526, batch accuracy = 0.3326416015625, val acc = 0.1796875\n",
      "(1533) loss = 1.8544, batch accuracy = 0.334716796875, val acc = 0.1796875\n",
      "(1534) loss = 1.8654, batch accuracy = 0.3336181640625, val acc = 0.171875\n",
      "(1535) loss = 1.8605, batch accuracy = 0.3367919921875, val acc = 0.1848958432674408\n",
      "(1536) loss = 1.8585, batch accuracy = 0.333251953125, val acc = 0.1770833432674408\n",
      "(1537) loss = 1.8554, batch accuracy = 0.3372802734375, val acc = 0.1927083432674408\n",
      "(1538) loss = 1.8531, batch accuracy = 0.3353271484375, val acc = 0.1901041716337204\n",
      "(1539) loss = 1.8627, batch accuracy = 0.3370361328125, val acc = 0.1848958432674408\n",
      "(1540) loss = 1.8491, batch accuracy = 0.3409423828125, val acc = 0.1901041716337204\n",
      "(1541) loss = 1.8486, batch accuracy = 0.3389892578125, val acc = 0.1927083432674408\n",
      "(1542) loss = 1.8469, batch accuracy = 0.3387451171875, val acc = 0.1875\n",
      "(1543) loss = 1.8512, batch accuracy = 0.341064453125, val acc = 0.1875\n",
      "(1544) loss = 1.8609, batch accuracy = 0.3365478515625, val acc = 0.1822916716337204\n",
      "(1545) loss = 1.8637, batch accuracy = 0.335205078125, val acc = 0.1875\n",
      "(1546) loss = 1.8613, batch accuracy = 0.334228515625, val acc = 0.1848958432674408\n",
      "(1547) loss = 1.8622, batch accuracy = 0.3326416015625, val acc = 0.1822916716337204\n",
      "(1548) loss = 1.8611, batch accuracy = 0.3363037109375, val acc = 0.1822916716337204\n",
      "(1549) loss = 1.8570, batch accuracy = 0.337158203125, val acc = 0.1901041716337204\n",
      "(1550) loss = 1.8647, batch accuracy = 0.336669921875, val acc = 0.1796875\n",
      "(1551) loss = 1.8602, batch accuracy = 0.3372802734375, val acc = 0.1796875\n",
      "(1552) loss = 1.8663, batch accuracy = 0.3369140625, val acc = 0.1875\n",
      "(1553) loss = 1.8504, batch accuracy = 0.336669921875, val acc = 0.1953125\n",
      "(1554) loss = 1.8597, batch accuracy = 0.3387451171875, val acc = 0.1744791716337204\n",
      "(1555) loss = 1.8510, batch accuracy = 0.3427734375, val acc = 0.1901041716337204\n",
      "(1556) loss = 1.8539, batch accuracy = 0.3389892578125, val acc = 0.1953125\n",
      "(1557) loss = 1.8662, batch accuracy = 0.3375244140625, val acc = 0.1822916716337204\n",
      "(1558) loss = 1.8581, batch accuracy = 0.337646484375, val acc = 0.1875\n",
      "(1559) loss = 1.8541, batch accuracy = 0.3392333984375, val acc = 0.1901041716337204\n",
      "(1560) loss = 1.8563, batch accuracy = 0.338134765625, val acc = 0.1770833432674408\n",
      "(1561) loss = 1.8547, batch accuracy = 0.337158203125, val acc = 0.1848958432674408\n",
      "(1562) loss = 1.8481, batch accuracy = 0.341064453125, val acc = 0.1770833432674408\n",
      "(1563) loss = 1.8458, batch accuracy = 0.343505859375, val acc = 0.1848958432674408\n",
      "(1564) loss = 1.8416, batch accuracy = 0.3411865234375, val acc = 0.1953125\n",
      "(1565) loss = 1.8457, batch accuracy = 0.3433837890625, val acc = 0.1770833432674408\n",
      "(1566) loss = 1.8426, batch accuracy = 0.3427734375, val acc = 0.1796875\n",
      "(1567) loss = 1.8431, batch accuracy = 0.343505859375, val acc = 0.1822916716337204\n",
      "(1568) loss = 1.8423, batch accuracy = 0.341552734375, val acc = 0.1770833432674408\n",
      "(1569) loss = 1.8559, batch accuracy = 0.3382568359375, val acc = 0.1875\n",
      "(1570) loss = 1.8512, batch accuracy = 0.3411865234375, val acc = 0.1875\n",
      "(1571) loss = 1.8690, batch accuracy = 0.3355712890625, val acc = 0.1953125\n",
      "(1572) loss = 1.8604, batch accuracy = 0.3331298828125, val acc = 0.1875\n",
      "(1573) loss = 1.8676, batch accuracy = 0.332763671875, val acc = 0.1901041716337204\n",
      "(1574) loss = 1.8621, batch accuracy = 0.3348388671875, val acc = 0.1796875\n",
      "(1575) loss = 1.8662, batch accuracy = 0.3289794921875, val acc = 0.1953125\n",
      "(1576) loss = 1.8699, batch accuracy = 0.3372802734375, val acc = 0.1953125\n",
      "(1577) loss = 1.8669, batch accuracy = 0.3365478515625, val acc = 0.203125\n",
      "(1578) loss = 1.8555, batch accuracy = 0.336181640625, val acc = 0.1848958432674408\n",
      "(1579) loss = 1.8606, batch accuracy = 0.334228515625, val acc = 0.1927083432674408\n",
      "(1580) loss = 1.8506, batch accuracy = 0.3350830078125, val acc = 0.1875\n",
      "(1581) loss = 1.8520, batch accuracy = 0.3377685546875, val acc = 0.1875\n",
      "(1582) loss = 1.8535, batch accuracy = 0.335693359375, val acc = 0.1822916716337204\n",
      "(1583) loss = 1.8538, batch accuracy = 0.338623046875, val acc = 0.1927083432674408\n",
      "(1584) loss = 1.8514, batch accuracy = 0.3375244140625, val acc = 0.1901041716337204\n",
      "(1585) loss = 1.8471, batch accuracy = 0.3380126953125, val acc = 0.1927083432674408\n",
      "(1586) loss = 1.8552, batch accuracy = 0.338134765625, val acc = 0.1848958432674408\n",
      "(1587) loss = 1.8601, batch accuracy = 0.3382568359375, val acc = 0.1901041716337204\n",
      "(1588) loss = 1.8593, batch accuracy = 0.3402099609375, val acc = 0.1901041716337204\n",
      "(1589) loss = 1.8537, batch accuracy = 0.3411865234375, val acc = 0.1848958432674408\n",
      "(1590) loss = 1.8708, batch accuracy = 0.3363037109375, val acc = 0.1953125\n",
      "(1591) loss = 1.8532, batch accuracy = 0.336669921875, val acc = 0.1848958432674408\n",
      "(1592) loss = 1.8546, batch accuracy = 0.3355712890625, val acc = 0.1875\n",
      "(1593) loss = 1.8508, batch accuracy = 0.337158203125, val acc = 0.1875\n",
      "(1594) loss = 1.8515, batch accuracy = 0.338134765625, val acc = 0.1875\n",
      "(1595) loss = 1.8467, batch accuracy = 0.3409423828125, val acc = 0.1875\n",
      "(1596) loss = 1.8543, batch accuracy = 0.3389892578125, val acc = 0.1901041716337204\n",
      "(1597) loss = 1.8602, batch accuracy = 0.338623046875, val acc = 0.1901041716337204\n",
      "(1598) loss = 1.8535, batch accuracy = 0.3355712890625, val acc = 0.1901041716337204\n",
      "(1599) loss = 1.8612, batch accuracy = 0.3369140625, val acc = 0.1875\n",
      "(1600) loss = 1.8507, batch accuracy = 0.3419189453125, val acc = 0.1927083432674408\n",
      "(1601) loss = 1.8496, batch accuracy = 0.3406982421875, val acc = 0.1875\n",
      "(1602) loss = 1.8555, batch accuracy = 0.3380126953125, val acc = 0.1770833432674408\n",
      "(1603) loss = 1.8630, batch accuracy = 0.337890625, val acc = 0.1953125\n",
      "(1604) loss = 1.8637, batch accuracy = 0.333984375, val acc = 0.1822916716337204\n",
      "(1605) loss = 1.8653, batch accuracy = 0.3349609375, val acc = 0.1770833432674408\n",
      "(1606) loss = 1.8552, batch accuracy = 0.3372802734375, val acc = 0.1875\n",
      "(1607) loss = 1.8562, batch accuracy = 0.3387451171875, val acc = 0.1901041716337204\n",
      "(1608) loss = 1.8562, batch accuracy = 0.335205078125, val acc = 0.1770833432674408\n",
      "(1609) loss = 1.8504, batch accuracy = 0.3367919921875, val acc = 0.1744791716337204\n",
      "(1610) loss = 1.8509, batch accuracy = 0.337890625, val acc = 0.1901041716337204\n",
      "(1611) loss = 1.8525, batch accuracy = 0.3387451171875, val acc = 0.1875\n",
      "(1612) loss = 1.8547, batch accuracy = 0.337158203125, val acc = 0.1875\n",
      "(1613) loss = 1.8539, batch accuracy = 0.3377685546875, val acc = 0.1770833432674408\n",
      "(1614) loss = 1.8576, batch accuracy = 0.33642578125, val acc = 0.1901041716337204\n",
      "(1615) loss = 1.8486, batch accuracy = 0.3358154296875, val acc = 0.1848958432674408\n",
      "(1616) loss = 1.8532, batch accuracy = 0.3363037109375, val acc = 0.1822916716337204\n",
      "(1617) loss = 1.8510, batch accuracy = 0.335693359375, val acc = 0.1848958432674408\n",
      "(1618) loss = 1.8542, batch accuracy = 0.3389892578125, val acc = 0.1927083432674408\n",
      "(1619) loss = 1.8461, batch accuracy = 0.3372802734375, val acc = 0.1796875\n",
      "(1620) loss = 1.8551, batch accuracy = 0.3394775390625, val acc = 0.1822916716337204\n",
      "(1621) loss = 1.8499, batch accuracy = 0.3365478515625, val acc = 0.1927083432674408\n",
      "(1622) loss = 1.8575, batch accuracy = 0.3382568359375, val acc = 0.1822916716337204\n",
      "(1623) loss = 1.8685, batch accuracy = 0.335205078125, val acc = 0.1822916716337204\n",
      "(1624) loss = 1.8588, batch accuracy = 0.3409423828125, val acc = 0.1796875\n",
      "(1625) loss = 1.8648, batch accuracy = 0.3353271484375, val acc = 0.1744791716337204\n",
      "(1626) loss = 1.8632, batch accuracy = 0.33642578125, val acc = 0.1927083432674408\n",
      "(1627) loss = 1.8573, batch accuracy = 0.3372802734375, val acc = 0.1822916716337204\n",
      "(1628) loss = 1.8534, batch accuracy = 0.3402099609375, val acc = 0.1875\n",
      "(1629) loss = 1.8559, batch accuracy = 0.33740234375, val acc = 0.1770833432674408\n",
      "(1630) loss = 1.8680, batch accuracy = 0.3348388671875, val acc = 0.1901041716337204\n",
      "(1631) loss = 1.8598, batch accuracy = 0.3370361328125, val acc = 0.1901041716337204\n",
      "(1632) loss = 1.8607, batch accuracy = 0.340576171875, val acc = 0.1927083432674408\n",
      "(1633) loss = 1.8713, batch accuracy = 0.333984375, val acc = 0.1796875\n",
      "(1634) loss = 1.8817, batch accuracy = 0.336669921875, val acc = 0.1822916716337204\n",
      "(1635) loss = 1.8702, batch accuracy = 0.3355712890625, val acc = 0.1953125\n",
      "(1636) loss = 1.8643, batch accuracy = 0.3360595703125, val acc = 0.1953125\n",
      "(1637) loss = 1.8608, batch accuracy = 0.3406982421875, val acc = 0.1927083432674408\n",
      "(1638) loss = 1.8546, batch accuracy = 0.3408203125, val acc = 0.171875\n",
      "(1639) loss = 1.8595, batch accuracy = 0.3397216796875, val acc = 0.1927083432674408\n",
      "(1640) loss = 1.8606, batch accuracy = 0.3341064453125, val acc = 0.1770833432674408\n",
      "(1641) loss = 1.8554, batch accuracy = 0.337890625, val acc = 0.1953125\n",
      "(1642) loss = 1.8541, batch accuracy = 0.3382568359375, val acc = 0.1770833432674408\n",
      "(1643) loss = 1.8593, batch accuracy = 0.33447265625, val acc = 0.1822916716337204\n",
      "(1644) loss = 1.8732, batch accuracy = 0.337646484375, val acc = 0.1796875\n",
      "(1645) loss = 1.8588, batch accuracy = 0.3370361328125, val acc = 0.1901041716337204\n",
      "(1646) loss = 1.8632, batch accuracy = 0.338134765625, val acc = 0.1901041716337204\n",
      "(1647) loss = 1.8510, batch accuracy = 0.3409423828125, val acc = 0.1796875\n",
      "(1648) loss = 1.8518, batch accuracy = 0.3399658203125, val acc = 0.1848958432674408\n",
      "(1649) loss = 1.8489, batch accuracy = 0.3394775390625, val acc = 0.1692708432674408\n",
      "(1650) loss = 1.8535, batch accuracy = 0.3409423828125, val acc = 0.1744791716337204\n",
      "(1651) loss = 1.8477, batch accuracy = 0.33740234375, val acc = 0.1875\n",
      "(1652) loss = 1.8586, batch accuracy = 0.336669921875, val acc = 0.1875\n",
      "(1653) loss = 1.8516, batch accuracy = 0.33984375, val acc = 0.1927083432674408\n",
      "(1654) loss = 1.8561, batch accuracy = 0.3402099609375, val acc = 0.1822916716337204\n",
      "(1655) loss = 1.8524, batch accuracy = 0.3394775390625, val acc = 0.1848958432674408\n",
      "(1656) loss = 1.8536, batch accuracy = 0.3370361328125, val acc = 0.1796875\n",
      "(1657) loss = 1.8536, batch accuracy = 0.3402099609375, val acc = 0.1875\n",
      "(1658) loss = 1.8584, batch accuracy = 0.3353271484375, val acc = 0.1796875\n",
      "(1659) loss = 1.8596, batch accuracy = 0.3402099609375, val acc = 0.1822916716337204\n",
      "(1660) loss = 1.8663, batch accuracy = 0.3399658203125, val acc = 0.1822916716337204\n",
      "(1661) loss = 1.8615, batch accuracy = 0.33349609375, val acc = 0.1848958432674408\n",
      "(1662) loss = 1.8600, batch accuracy = 0.3360595703125, val acc = 0.1875\n",
      "(1663) loss = 1.8559, batch accuracy = 0.3360595703125, val acc = 0.171875\n",
      "(1664) loss = 1.8642, batch accuracy = 0.3341064453125, val acc = 0.1822916716337204\n",
      "(1665) loss = 1.8622, batch accuracy = 0.3331298828125, val acc = 0.1875\n",
      "(1666) loss = 1.8648, batch accuracy = 0.3336181640625, val acc = 0.1796875\n",
      "(1667) loss = 1.8824, batch accuracy = 0.331298828125, val acc = 0.1875\n",
      "(1668) loss = 1.8650, batch accuracy = 0.333251953125, val acc = 0.1796875\n",
      "(1669) loss = 1.8575, batch accuracy = 0.3343505859375, val acc = 0.1770833432674408\n",
      "(1670) loss = 1.8517, batch accuracy = 0.3349609375, val acc = 0.1796875\n",
      "(1671) loss = 1.8532, batch accuracy = 0.337890625, val acc = 0.1848958432674408\n",
      "(1672) loss = 1.8587, batch accuracy = 0.3355712890625, val acc = 0.1848958432674408\n",
      "(1673) loss = 1.8549, batch accuracy = 0.339599609375, val acc = 0.1848958432674408\n",
      "(1674) loss = 1.8594, batch accuracy = 0.3388671875, val acc = 0.1822916716337204\n",
      "(1675) loss = 1.8536, batch accuracy = 0.33544921875, val acc = 0.1770833432674408\n",
      "(1676) loss = 1.8547, batch accuracy = 0.3338623046875, val acc = 0.1875\n",
      "(1677) loss = 1.8506, batch accuracy = 0.3370361328125, val acc = 0.1796875\n",
      "(1678) loss = 1.8570, batch accuracy = 0.3367919921875, val acc = 0.1770833432674408\n",
      "(1679) loss = 1.8535, batch accuracy = 0.33740234375, val acc = 0.1796875\n",
      "(1680) loss = 1.8512, batch accuracy = 0.3402099609375, val acc = 0.1875\n",
      "(1681) loss = 1.8539, batch accuracy = 0.33642578125, val acc = 0.1822916716337204\n",
      "(1682) loss = 1.8556, batch accuracy = 0.3375244140625, val acc = 0.1927083432674408\n",
      "(1683) loss = 1.8462, batch accuracy = 0.3365478515625, val acc = 0.1744791716337204\n",
      "(1684) loss = 1.8468, batch accuracy = 0.338134765625, val acc = 0.1953125\n",
      "(1685) loss = 1.8501, batch accuracy = 0.338134765625, val acc = 0.1822916716337204\n",
      "(1686) loss = 1.8601, batch accuracy = 0.33349609375, val acc = 0.1692708432674408\n",
      "(1687) loss = 1.8472, batch accuracy = 0.339111328125, val acc = 0.1901041716337204\n",
      "(1688) loss = 1.8475, batch accuracy = 0.3409423828125, val acc = 0.1875\n",
      "(1689) loss = 1.8508, batch accuracy = 0.3388671875, val acc = 0.1822916716337204\n",
      "(1690) loss = 1.8554, batch accuracy = 0.3377685546875, val acc = 0.1848958432674408\n",
      "(1691) loss = 1.8547, batch accuracy = 0.337646484375, val acc = 0.1822916716337204\n",
      "(1692) loss = 1.8511, batch accuracy = 0.3341064453125, val acc = 0.1901041716337204\n",
      "(1693) loss = 1.8592, batch accuracy = 0.33837890625, val acc = 0.1848958432674408\n",
      "(1694) loss = 1.8529, batch accuracy = 0.337158203125, val acc = 0.1848958432674408\n",
      "(1695) loss = 1.8568, batch accuracy = 0.3375244140625, val acc = 0.1848958432674408\n",
      "(1696) loss = 1.8534, batch accuracy = 0.3369140625, val acc = 0.1744791716337204\n",
      "(1697) loss = 1.8550, batch accuracy = 0.3380126953125, val acc = 0.1848958432674408\n",
      "(1698) loss = 1.8587, batch accuracy = 0.337890625, val acc = 0.1875\n",
      "(1699) loss = 1.8532, batch accuracy = 0.3377685546875, val acc = 0.1901041716337204\n",
      "(1700) loss = 1.8617, batch accuracy = 0.336669921875, val acc = 0.1901041716337204\n",
      "(1701) loss = 1.8495, batch accuracy = 0.338134765625, val acc = 0.1927083432674408\n",
      "(1702) loss = 1.8522, batch accuracy = 0.339111328125, val acc = 0.1927083432674408\n",
      "(1703) loss = 1.8477, batch accuracy = 0.3360595703125, val acc = 0.1770833432674408\n",
      "(1704) loss = 1.8535, batch accuracy = 0.3394775390625, val acc = 0.1927083432674408\n",
      "(1705) loss = 1.8527, batch accuracy = 0.3382568359375, val acc = 0.1848958432674408\n",
      "(1706) loss = 1.8517, batch accuracy = 0.33984375, val acc = 0.1822916716337204\n",
      "(1707) loss = 1.8556, batch accuracy = 0.337646484375, val acc = 0.1796875\n",
      "(1708) loss = 1.8495, batch accuracy = 0.338623046875, val acc = 0.1822916716337204\n",
      "(1709) loss = 1.8510, batch accuracy = 0.3382568359375, val acc = 0.1770833432674408\n",
      "(1710) loss = 1.8627, batch accuracy = 0.3348388671875, val acc = 0.1822916716337204\n",
      "(1711) loss = 1.8621, batch accuracy = 0.3326416015625, val acc = 0.1848958432674408\n",
      "(1712) loss = 1.8672, batch accuracy = 0.333984375, val acc = 0.1796875\n",
      "(1713) loss = 1.8693, batch accuracy = 0.3330078125, val acc = 0.1822916716337204\n",
      "(1714) loss = 1.8588, batch accuracy = 0.3341064453125, val acc = 0.1848958432674408\n",
      "(1715) loss = 1.8581, batch accuracy = 0.3375244140625, val acc = 0.1796875\n",
      "(1716) loss = 1.8591, batch accuracy = 0.3367919921875, val acc = 0.1848958432674408\n",
      "(1717) loss = 1.8544, batch accuracy = 0.3359375, val acc = 0.1848958432674408\n",
      "(1718) loss = 1.8529, batch accuracy = 0.3375244140625, val acc = 0.1770833432674408\n",
      "(1719) loss = 1.8538, batch accuracy = 0.3369140625, val acc = 0.1927083432674408\n",
      "(1720) loss = 1.8457, batch accuracy = 0.33935546875, val acc = 0.1770833432674408\n",
      "(1721) loss = 1.8458, batch accuracy = 0.3408203125, val acc = 0.1822916716337204\n",
      "(1722) loss = 1.8513, batch accuracy = 0.337158203125, val acc = 0.1848958432674408\n",
      "(1723) loss = 1.8486, batch accuracy = 0.3380126953125, val acc = 0.1848958432674408\n",
      "(1724) loss = 1.8501, batch accuracy = 0.33935546875, val acc = 0.1822916716337204\n",
      "(1725) loss = 1.8488, batch accuracy = 0.3394775390625, val acc = 0.1770833432674408\n",
      "(1726) loss = 1.8550, batch accuracy = 0.3353271484375, val acc = 0.1770833432674408\n",
      "(1727) loss = 1.8511, batch accuracy = 0.3359375, val acc = 0.1822916716337204\n",
      "(1728) loss = 1.8511, batch accuracy = 0.3392333984375, val acc = 0.1770833432674408\n",
      "(1729) loss = 1.8483, batch accuracy = 0.338623046875, val acc = 0.1796875\n",
      "(1730) loss = 1.8457, batch accuracy = 0.3350830078125, val acc = 0.171875\n",
      "(1731) loss = 1.8478, batch accuracy = 0.340576171875, val acc = 0.1692708432674408\n",
      "(1732) loss = 1.8531, batch accuracy = 0.337646484375, val acc = 0.171875\n",
      "(1733) loss = 1.8522, batch accuracy = 0.336181640625, val acc = 0.1770833432674408\n",
      "(1734) loss = 1.8542, batch accuracy = 0.3363037109375, val acc = 0.1770833432674408\n",
      "(1735) loss = 1.8574, batch accuracy = 0.336669921875, val acc = 0.1875\n",
      "(1736) loss = 1.8506, batch accuracy = 0.3404541015625, val acc = 0.1822916716337204\n",
      "(1737) loss = 1.8458, batch accuracy = 0.340576171875, val acc = 0.1822916716337204\n",
      "(1738) loss = 1.8541, batch accuracy = 0.3399658203125, val acc = 0.1822916716337204\n",
      "(1739) loss = 1.8508, batch accuracy = 0.339599609375, val acc = 0.171875\n",
      "(1740) loss = 1.8609, batch accuracy = 0.33935546875, val acc = 0.1901041716337204\n",
      "(1741) loss = 1.8560, batch accuracy = 0.340576171875, val acc = 0.1875\n",
      "(1742) loss = 1.8478, batch accuracy = 0.3443603515625, val acc = 0.1875\n",
      "(1743) loss = 1.8574, batch accuracy = 0.3350830078125, val acc = 0.1822916716337204\n",
      "(1744) loss = 1.8564, batch accuracy = 0.3394775390625, val acc = 0.1770833432674408\n",
      "(1745) loss = 1.8606, batch accuracy = 0.3388671875, val acc = 0.1796875\n",
      "(1746) loss = 1.8698, batch accuracy = 0.3375244140625, val acc = 0.1796875\n",
      "(1747) loss = 1.8661, batch accuracy = 0.3355712890625, val acc = 0.1770833432674408\n",
      "(1748) loss = 1.8701, batch accuracy = 0.3321533203125, val acc = 0.1822916716337204\n",
      "(1749) loss = 1.8691, batch accuracy = 0.3382568359375, val acc = 0.171875\n",
      "(1750) loss = 1.8570, batch accuracy = 0.3385009765625, val acc = 0.1979166716337204\n",
      "(1751) loss = 1.8565, batch accuracy = 0.3306884765625, val acc = 0.1822916716337204\n",
      "(1752) loss = 1.8560, batch accuracy = 0.3382568359375, val acc = 0.1822916716337204\n",
      "(1753) loss = 1.8497, batch accuracy = 0.3387451171875, val acc = 0.1614583432674408\n",
      "(1754) loss = 1.8466, batch accuracy = 0.3387451171875, val acc = 0.1848958432674408\n",
      "(1755) loss = 1.8487, batch accuracy = 0.339111328125, val acc = 0.171875\n",
      "(1756) loss = 1.8461, batch accuracy = 0.34130859375, val acc = 0.1822916716337204\n",
      "(1757) loss = 1.8467, batch accuracy = 0.34033203125, val acc = 0.1822916716337204\n",
      "(1758) loss = 1.8424, batch accuracy = 0.3392333984375, val acc = 0.1822916716337204\n",
      "(1759) loss = 1.8431, batch accuracy = 0.3402099609375, val acc = 0.1822916716337204\n",
      "(1760) loss = 1.8410, batch accuracy = 0.342041015625, val acc = 0.171875\n",
      "(1761) loss = 1.8458, batch accuracy = 0.34033203125, val acc = 0.1640625\n",
      "(1762) loss = 1.8442, batch accuracy = 0.3411865234375, val acc = 0.1822916716337204\n",
      "(1763) loss = 1.8371, batch accuracy = 0.34130859375, val acc = 0.171875\n",
      "(1764) loss = 1.8392, batch accuracy = 0.3409423828125, val acc = 0.1822916716337204\n",
      "(1765) loss = 1.8442, batch accuracy = 0.3419189453125, val acc = 0.1796875\n",
      "(1766) loss = 1.8396, batch accuracy = 0.339111328125, val acc = 0.1822916716337204\n",
      "(1767) loss = 1.8540, batch accuracy = 0.3404541015625, val acc = 0.1770833432674408\n",
      "(1768) loss = 1.8440, batch accuracy = 0.3404541015625, val acc = 0.1744791716337204\n",
      "(1769) loss = 1.8457, batch accuracy = 0.339599609375, val acc = 0.1822916716337204\n",
      "(1770) loss = 1.8474, batch accuracy = 0.3404541015625, val acc = 0.1770833432674408\n",
      "(1771) loss = 1.8532, batch accuracy = 0.3370361328125, val acc = 0.1666666716337204\n",
      "(1772) loss = 1.8458, batch accuracy = 0.342041015625, val acc = 0.1796875\n",
      "(1773) loss = 1.8613, batch accuracy = 0.3365478515625, val acc = 0.1848958432674408\n",
      "(1774) loss = 1.8555, batch accuracy = 0.337646484375, val acc = 0.1901041716337204\n",
      "(1775) loss = 1.8510, batch accuracy = 0.3392333984375, val acc = 0.1848958432674408\n",
      "(1776) loss = 1.8585, batch accuracy = 0.339599609375, val acc = 0.1770833432674408\n",
      "(1777) loss = 1.8574, batch accuracy = 0.3377685546875, val acc = 0.1796875\n",
      "(1778) loss = 1.8504, batch accuracy = 0.33935546875, val acc = 0.1744791716337204\n",
      "(1779) loss = 1.8576, batch accuracy = 0.3360595703125, val acc = 0.1822916716337204\n",
      "(1780) loss = 1.8838, batch accuracy = 0.33447265625, val acc = 0.1875\n",
      "(1781) loss = 1.8608, batch accuracy = 0.335693359375, val acc = 0.1796875\n",
      "(1782) loss = 1.8720, batch accuracy = 0.3343505859375, val acc = 0.1822916716337204\n",
      "(1783) loss = 1.8693, batch accuracy = 0.3345947265625, val acc = 0.1770833432674408\n",
      "(1784) loss = 1.8580, batch accuracy = 0.336181640625, val acc = 0.1875\n",
      "(1785) loss = 1.8550, batch accuracy = 0.3367919921875, val acc = 0.1901041716337204\n",
      "(1786) loss = 1.8447, batch accuracy = 0.3411865234375, val acc = 0.1875\n",
      "(1787) loss = 1.8514, batch accuracy = 0.3377685546875, val acc = 0.1927083432674408\n",
      "(1788) loss = 1.8435, batch accuracy = 0.33984375, val acc = 0.1875\n",
      "(1789) loss = 1.8475, batch accuracy = 0.3426513671875, val acc = 0.203125\n",
      "(1790) loss = 1.8486, batch accuracy = 0.3385009765625, val acc = 0.1927083432674408\n",
      "(1791) loss = 1.8496, batch accuracy = 0.3409423828125, val acc = 0.1979166716337204\n",
      "(1792) loss = 1.8508, batch accuracy = 0.33984375, val acc = 0.1770833432674408\n",
      "(1793) loss = 1.8577, batch accuracy = 0.336669921875, val acc = 0.1848958432674408\n",
      "(1794) loss = 1.8476, batch accuracy = 0.3372802734375, val acc = 0.1979166716337204\n",
      "(1795) loss = 1.8548, batch accuracy = 0.3404541015625, val acc = 0.1848958432674408\n",
      "(1796) loss = 1.8465, batch accuracy = 0.3408203125, val acc = 0.1770833432674408\n",
      "(1797) loss = 1.8534, batch accuracy = 0.3404541015625, val acc = 0.1848958432674408\n",
      "(1798) loss = 1.8408, batch accuracy = 0.340087890625, val acc = 0.1796875\n",
      "(1799) loss = 1.8559, batch accuracy = 0.3394775390625, val acc = 0.1901041716337204\n",
      "(1800) loss = 1.8467, batch accuracy = 0.340576171875, val acc = 0.1875\n",
      "(1801) loss = 1.8591, batch accuracy = 0.3380126953125, val acc = 0.1901041716337204\n",
      "(1802) loss = 1.8482, batch accuracy = 0.3377685546875, val acc = 0.1822916716337204\n",
      "(1803) loss = 1.8526, batch accuracy = 0.334716796875, val acc = 0.1875\n",
      "(1804) loss = 1.8634, batch accuracy = 0.337646484375, val acc = 0.1848958432674408\n",
      "(1805) loss = 1.8581, batch accuracy = 0.3338623046875, val acc = 0.1822916716337204\n",
      "(1806) loss = 1.8561, batch accuracy = 0.333251953125, val acc = 0.1744791716337204\n",
      "(1807) loss = 1.8534, batch accuracy = 0.3372802734375, val acc = 0.1927083432674408\n",
      "(1808) loss = 1.8479, batch accuracy = 0.337890625, val acc = 0.1770833432674408\n",
      "(1809) loss = 1.8410, batch accuracy = 0.343994140625, val acc = 0.1927083432674408\n",
      "(1810) loss = 1.8379, batch accuracy = 0.3421630859375, val acc = 0.1796875\n",
      "(1811) loss = 1.8428, batch accuracy = 0.3438720703125, val acc = 0.1875\n",
      "(1812) loss = 1.8384, batch accuracy = 0.341796875, val acc = 0.1744791716337204\n",
      "(1813) loss = 1.8391, batch accuracy = 0.3409423828125, val acc = 0.1822916716337204\n",
      "(1814) loss = 1.8433, batch accuracy = 0.3446044921875, val acc = 0.1822916716337204\n",
      "(1815) loss = 1.8518, batch accuracy = 0.33935546875, val acc = 0.1770833432674408\n",
      "(1816) loss = 1.8457, batch accuracy = 0.339599609375, val acc = 0.171875\n",
      "(1817) loss = 1.8436, batch accuracy = 0.339111328125, val acc = 0.1875\n",
      "(1818) loss = 1.8551, batch accuracy = 0.3382568359375, val acc = 0.1796875\n",
      "(1819) loss = 1.8470, batch accuracy = 0.340576171875, val acc = 0.1744791716337204\n",
      "(1820) loss = 1.8506, batch accuracy = 0.33984375, val acc = 0.1848958432674408\n",
      "(1821) loss = 1.8736, batch accuracy = 0.332763671875, val acc = 0.1796875\n",
      "(1822) loss = 1.8607, batch accuracy = 0.33740234375, val acc = 0.1692708432674408\n",
      "(1823) loss = 1.8563, batch accuracy = 0.33154296875, val acc = 0.1901041716337204\n",
      "(1824) loss = 1.8625, batch accuracy = 0.33544921875, val acc = 0.1744791716337204\n",
      "(1825) loss = 1.8673, batch accuracy = 0.336181640625, val acc = 0.1848958432674408\n",
      "(1826) loss = 1.8642, batch accuracy = 0.338134765625, val acc = 0.1848958432674408\n",
      "(1827) loss = 1.8593, batch accuracy = 0.33544921875, val acc = 0.1901041716337204\n",
      "(1828) loss = 1.8560, batch accuracy = 0.33935546875, val acc = 0.1822916716337204\n",
      "(1829) loss = 1.8488, batch accuracy = 0.34130859375, val acc = 0.1796875\n",
      "(1830) loss = 1.8588, batch accuracy = 0.3348388671875, val acc = 0.1770833432674408\n",
      "(1831) loss = 1.8625, batch accuracy = 0.33642578125, val acc = 0.1953125\n",
      "(1832) loss = 1.8596, batch accuracy = 0.335693359375, val acc = 0.1848958432674408\n",
      "(1833) loss = 1.8552, batch accuracy = 0.3359375, val acc = 0.1744791716337204\n",
      "(1834) loss = 1.8545, batch accuracy = 0.3370361328125, val acc = 0.1875\n",
      "(1835) loss = 1.8489, batch accuracy = 0.340087890625, val acc = 0.1848958432674408\n",
      "(1836) loss = 1.8449, batch accuracy = 0.3402099609375, val acc = 0.1953125\n",
      "(1837) loss = 1.8423, batch accuracy = 0.341552734375, val acc = 0.1875\n",
      "(1838) loss = 1.8524, batch accuracy = 0.3402099609375, val acc = 0.1796875\n",
      "(1839) loss = 1.8612, batch accuracy = 0.337646484375, val acc = 0.1901041716337204\n",
      "(1840) loss = 1.8656, batch accuracy = 0.33349609375, val acc = 0.1875\n",
      "(1841) loss = 1.8622, batch accuracy = 0.335205078125, val acc = 0.1770833432674408\n",
      "(1842) loss = 1.8586, batch accuracy = 0.3365478515625, val acc = 0.1875\n",
      "(1843) loss = 1.8650, batch accuracy = 0.3331298828125, val acc = 0.1927083432674408\n",
      "(1844) loss = 1.8529, batch accuracy = 0.3353271484375, val acc = 0.1953125\n",
      "(1845) loss = 1.8638, batch accuracy = 0.3382568359375, val acc = 0.1692708432674408\n",
      "(1846) loss = 1.8568, batch accuracy = 0.3355712890625, val acc = 0.1822916716337204\n",
      "(1847) loss = 1.8510, batch accuracy = 0.3385009765625, val acc = 0.1796875\n",
      "(1848) loss = 1.8514, batch accuracy = 0.3355712890625, val acc = 0.1770833432674408\n",
      "(1849) loss = 1.8447, batch accuracy = 0.3389892578125, val acc = 0.1744791716337204\n",
      "(1850) loss = 1.8470, batch accuracy = 0.3416748046875, val acc = 0.1822916716337204\n",
      "(1851) loss = 1.8432, batch accuracy = 0.3411865234375, val acc = 0.1848958432674408\n",
      "(1852) loss = 1.8627, batch accuracy = 0.3408203125, val acc = 0.1848958432674408\n",
      "(1853) loss = 1.8542, batch accuracy = 0.3382568359375, val acc = 0.1848958432674408\n",
      "(1854) loss = 1.8516, batch accuracy = 0.342041015625, val acc = 0.1953125\n",
      "(1855) loss = 1.8437, batch accuracy = 0.34326171875, val acc = 0.1848958432674408\n",
      "(1856) loss = 1.8488, batch accuracy = 0.3424072265625, val acc = 0.1796875\n",
      "(1857) loss = 1.8396, batch accuracy = 0.3431396484375, val acc = 0.1770833432674408\n",
      "(1858) loss = 1.8451, batch accuracy = 0.34423828125, val acc = 0.1796875\n",
      "(1859) loss = 1.8488, batch accuracy = 0.341064453125, val acc = 0.171875\n",
      "(1860) loss = 1.8455, batch accuracy = 0.3421630859375, val acc = 0.1848958432674408\n",
      "(1861) loss = 1.8460, batch accuracy = 0.3436279296875, val acc = 0.1875\n",
      "(1862) loss = 1.8395, batch accuracy = 0.34228515625, val acc = 0.1822916716337204\n",
      "(1863) loss = 1.8386, batch accuracy = 0.340576171875, val acc = 0.1692708432674408\n",
      "(1864) loss = 1.8382, batch accuracy = 0.3411865234375, val acc = 0.1901041716337204\n",
      "(1865) loss = 1.8456, batch accuracy = 0.341064453125, val acc = 0.1796875\n",
      "(1866) loss = 1.8647, batch accuracy = 0.342529296875, val acc = 0.1875\n",
      "(1867) loss = 1.8643, batch accuracy = 0.3385009765625, val acc = 0.1848958432674408\n",
      "(1868) loss = 1.8799, batch accuracy = 0.3348388671875, val acc = 0.1796875\n",
      "(1869) loss = 1.9015, batch accuracy = 0.3306884765625, val acc = 0.1640625\n",
      "(1870) loss = 1.8884, batch accuracy = 0.3349609375, val acc = 0.1901041716337204\n",
      "(1871) loss = 1.8778, batch accuracy = 0.3328857421875, val acc = 0.1953125\n",
      "(1872) loss = 1.8766, batch accuracy = 0.33203125, val acc = 0.1744791716337204\n",
      "(1873) loss = 1.8621, batch accuracy = 0.3328857421875, val acc = 0.1875\n",
      "(1874) loss = 1.8620, batch accuracy = 0.337890625, val acc = 0.1770833432674408\n",
      "(1875) loss = 1.8549, batch accuracy = 0.33837890625, val acc = 0.1979166716337204\n",
      "(1876) loss = 1.8520, batch accuracy = 0.338134765625, val acc = 0.1796875\n",
      "(1877) loss = 1.8557, batch accuracy = 0.3388671875, val acc = 0.1848958432674408\n",
      "(1878) loss = 1.8491, batch accuracy = 0.338623046875, val acc = 0.1901041716337204\n",
      "(1879) loss = 1.8499, batch accuracy = 0.3399658203125, val acc = 0.171875\n",
      "(1880) loss = 1.8484, batch accuracy = 0.3411865234375, val acc = 0.1848958432674408\n",
      "(1881) loss = 1.8415, batch accuracy = 0.343017578125, val acc = 0.1770833432674408\n",
      "(1882) loss = 1.8420, batch accuracy = 0.341552734375, val acc = 0.1770833432674408\n",
      "(1883) loss = 1.8398, batch accuracy = 0.342041015625, val acc = 0.1927083432674408\n",
      "(1884) loss = 1.8385, batch accuracy = 0.342041015625, val acc = 0.1744791716337204\n",
      "(1885) loss = 1.8428, batch accuracy = 0.343994140625, val acc = 0.1901041716337204\n",
      "(1886) loss = 1.8491, batch accuracy = 0.3367919921875, val acc = 0.1848958432674408\n",
      "(1887) loss = 1.8535, batch accuracy = 0.3409423828125, val acc = 0.1796875\n",
      "(1888) loss = 1.8555, batch accuracy = 0.33837890625, val acc = 0.1848958432674408\n",
      "(1889) loss = 1.8549, batch accuracy = 0.337646484375, val acc = 0.1796875\n",
      "(1890) loss = 1.8437, batch accuracy = 0.33984375, val acc = 0.1822916716337204\n",
      "(1891) loss = 1.8491, batch accuracy = 0.3402099609375, val acc = 0.1822916716337204\n",
      "(1892) loss = 1.8496, batch accuracy = 0.3367919921875, val acc = 0.1901041716337204\n",
      "(1893) loss = 1.8406, batch accuracy = 0.340087890625, val acc = 0.1901041716337204\n",
      "(1894) loss = 1.8509, batch accuracy = 0.3408203125, val acc = 0.1927083432674408\n",
      "(1895) loss = 1.8534, batch accuracy = 0.337890625, val acc = 0.1979166716337204\n",
      "(1896) loss = 1.8386, batch accuracy = 0.3392333984375, val acc = 0.1875\n",
      "(1897) loss = 1.8434, batch accuracy = 0.342041015625, val acc = 0.1901041716337204\n",
      "(1898) loss = 1.8526, batch accuracy = 0.3380126953125, val acc = 0.1770833432674408\n",
      "(1899) loss = 1.8553, batch accuracy = 0.3394775390625, val acc = 0.1822916716337204\n",
      "(1900) loss = 1.8435, batch accuracy = 0.3385009765625, val acc = 0.1822916716337204\n",
      "(1901) loss = 1.8468, batch accuracy = 0.3427734375, val acc = 0.1901041716337204\n",
      "(1902) loss = 1.8405, batch accuracy = 0.3399658203125, val acc = 0.1901041716337204\n",
      "(1903) loss = 1.8497, batch accuracy = 0.34033203125, val acc = 0.1848958432674408\n",
      "(1904) loss = 1.8423, batch accuracy = 0.3441162109375, val acc = 0.1875\n",
      "(1905) loss = 1.8448, batch accuracy = 0.3385009765625, val acc = 0.1848958432674408\n",
      "(1906) loss = 1.8387, batch accuracy = 0.3441162109375, val acc = 0.1822916716337204\n",
      "(1907) loss = 1.8525, batch accuracy = 0.3404541015625, val acc = 0.1796875\n",
      "(1908) loss = 1.8430, batch accuracy = 0.3392333984375, val acc = 0.1848958432674408\n",
      "(1909) loss = 1.8431, batch accuracy = 0.341064453125, val acc = 0.1822916716337204\n",
      "(1910) loss = 1.8448, batch accuracy = 0.3416748046875, val acc = 0.1875\n",
      "(1911) loss = 1.8481, batch accuracy = 0.3389892578125, val acc = 0.171875\n",
      "(1912) loss = 1.8475, batch accuracy = 0.339111328125, val acc = 0.1901041716337204\n",
      "(1913) loss = 1.8489, batch accuracy = 0.33837890625, val acc = 0.171875\n",
      "(1914) loss = 1.8561, batch accuracy = 0.3382568359375, val acc = 0.1953125\n",
      "(1915) loss = 1.8642, batch accuracy = 0.3392333984375, val acc = 0.1796875\n",
      "(1916) loss = 1.8574, batch accuracy = 0.3372802734375, val acc = 0.1822916716337204\n",
      "(1917) loss = 1.8603, batch accuracy = 0.3397216796875, val acc = 0.1848958432674408\n",
      "(1918) loss = 1.8575, batch accuracy = 0.338623046875, val acc = 0.1796875\n",
      "(1919) loss = 1.8518, batch accuracy = 0.338134765625, val acc = 0.1796875\n",
      "(1920) loss = 1.8497, batch accuracy = 0.33984375, val acc = 0.1822916716337204\n",
      "(1921) loss = 1.8603, batch accuracy = 0.34130859375, val acc = 0.1875\n",
      "(1922) loss = 1.8589, batch accuracy = 0.33740234375, val acc = 0.1875\n",
      "(1923) loss = 1.8715, batch accuracy = 0.3394775390625, val acc = 0.171875\n",
      "(1924) loss = 1.8564, batch accuracy = 0.3359375, val acc = 0.1822916716337204\n",
      "(1925) loss = 1.8545, batch accuracy = 0.33740234375, val acc = 0.1770833432674408\n",
      "(1926) loss = 1.8442, batch accuracy = 0.339111328125, val acc = 0.1901041716337204\n",
      "(1927) loss = 1.8507, batch accuracy = 0.336181640625, val acc = 0.1848958432674408\n",
      "(1928) loss = 1.8502, batch accuracy = 0.3382568359375, val acc = 0.1744791716337204\n",
      "(1929) loss = 1.8447, batch accuracy = 0.34130859375, val acc = 0.1770833432674408\n",
      "(1930) loss = 1.8404, batch accuracy = 0.34130859375, val acc = 0.1744791716337204\n",
      "(1931) loss = 1.8426, batch accuracy = 0.3397216796875, val acc = 0.1770833432674408\n",
      "(1932) loss = 1.8386, batch accuracy = 0.3392333984375, val acc = 0.1770833432674408\n",
      "(1933) loss = 1.8451, batch accuracy = 0.3392333984375, val acc = 0.1875\n",
      "(1934) loss = 1.8381, batch accuracy = 0.3426513671875, val acc = 0.1875\n",
      "(1935) loss = 1.8403, batch accuracy = 0.3389892578125, val acc = 0.1770833432674408\n",
      "(1936) loss = 1.8437, batch accuracy = 0.3397216796875, val acc = 0.1875\n",
      "(1937) loss = 1.8466, batch accuracy = 0.3392333984375, val acc = 0.1848958432674408\n",
      "(1938) loss = 1.8417, batch accuracy = 0.3399658203125, val acc = 0.1796875\n",
      "(1939) loss = 1.8433, batch accuracy = 0.341064453125, val acc = 0.1875\n",
      "(1940) loss = 1.8581, batch accuracy = 0.3406982421875, val acc = 0.1796875\n",
      "(1941) loss = 1.8471, batch accuracy = 0.343017578125, val acc = 0.1927083432674408\n",
      "(1942) loss = 1.8448, batch accuracy = 0.3404541015625, val acc = 0.1875\n",
      "(1943) loss = 1.8463, batch accuracy = 0.33984375, val acc = 0.1901041716337204\n",
      "(1944) loss = 1.8507, batch accuracy = 0.3388671875, val acc = 0.1848958432674408\n",
      "(1945) loss = 1.8452, batch accuracy = 0.338134765625, val acc = 0.1770833432674408\n",
      "(1946) loss = 1.8505, batch accuracy = 0.33984375, val acc = 0.1796875\n",
      "(1947) loss = 1.8431, batch accuracy = 0.3428955078125, val acc = 0.1822916716337204\n",
      "(1948) loss = 1.8419, batch accuracy = 0.341552734375, val acc = 0.1901041716337204\n",
      "(1949) loss = 1.8428, batch accuracy = 0.3397216796875, val acc = 0.1796875\n",
      "(1950) loss = 1.8485, batch accuracy = 0.3441162109375, val acc = 0.2005208432674408\n",
      "(1951) loss = 1.8513, batch accuracy = 0.3367919921875, val acc = 0.1796875\n",
      "(1952) loss = 1.8509, batch accuracy = 0.340087890625, val acc = 0.1770833432674408\n",
      "(1953) loss = 1.8504, batch accuracy = 0.3385009765625, val acc = 0.1796875\n",
      "(1954) loss = 1.8523, batch accuracy = 0.3350830078125, val acc = 0.1953125\n",
      "(1955) loss = 1.8506, batch accuracy = 0.3399658203125, val acc = 0.1927083432674408\n",
      "(1956) loss = 1.8584, batch accuracy = 0.33740234375, val acc = 0.1796875\n",
      "(1957) loss = 1.8485, batch accuracy = 0.336181640625, val acc = 0.1848958432674408\n",
      "(1958) loss = 1.8542, batch accuracy = 0.33837890625, val acc = 0.1822916716337204\n",
      "(1959) loss = 1.8461, batch accuracy = 0.3402099609375, val acc = 0.1822916716337204\n",
      "(1960) loss = 1.8418, batch accuracy = 0.3404541015625, val acc = 0.1692708432674408\n",
      "(1961) loss = 1.8440, batch accuracy = 0.34130859375, val acc = 0.1796875\n",
      "(1962) loss = 1.8519, batch accuracy = 0.3380126953125, val acc = 0.1822916716337204\n",
      "(1963) loss = 1.8489, batch accuracy = 0.340087890625, val acc = 0.1953125\n",
      "(1964) loss = 1.8517, batch accuracy = 0.33935546875, val acc = 0.1770833432674408\n",
      "(1965) loss = 1.8523, batch accuracy = 0.337646484375, val acc = 0.1796875\n",
      "(1966) loss = 1.8439, batch accuracy = 0.3370361328125, val acc = 0.1822916716337204\n",
      "(1967) loss = 1.8475, batch accuracy = 0.3414306640625, val acc = 0.1901041716337204\n",
      "(1968) loss = 1.8526, batch accuracy = 0.3365478515625, val acc = 0.1927083432674408\n",
      "(1969) loss = 1.8614, batch accuracy = 0.3382568359375, val acc = 0.1901041716337204\n",
      "(1970) loss = 1.8485, batch accuracy = 0.337890625, val acc = 0.1822916716337204\n",
      "(1971) loss = 1.8591, batch accuracy = 0.3385009765625, val acc = 0.1901041716337204\n",
      "(1972) loss = 1.8518, batch accuracy = 0.33642578125, val acc = 0.1796875\n",
      "(1973) loss = 1.8488, batch accuracy = 0.3388671875, val acc = 0.1927083432674408\n",
      "(1974) loss = 1.8512, batch accuracy = 0.34033203125, val acc = 0.1666666716337204\n",
      "(1975) loss = 1.8459, batch accuracy = 0.338623046875, val acc = 0.1875\n",
      "(1976) loss = 1.8461, batch accuracy = 0.340087890625, val acc = 0.171875\n",
      "(1977) loss = 1.8439, batch accuracy = 0.337890625, val acc = 0.1901041716337204\n",
      "(1978) loss = 1.8501, batch accuracy = 0.3392333984375, val acc = 0.1822916716337204\n",
      "(1979) loss = 1.8503, batch accuracy = 0.3394775390625, val acc = 0.1848958432674408\n",
      "(1980) loss = 1.8487, batch accuracy = 0.3388671875, val acc = 0.1848958432674408\n",
      "(1981) loss = 1.8696, batch accuracy = 0.3336181640625, val acc = 0.1770833432674408\n",
      "(1982) loss = 1.8543, batch accuracy = 0.3372802734375, val acc = 0.1796875\n",
      "(1983) loss = 1.8542, batch accuracy = 0.3372802734375, val acc = 0.1822916716337204\n",
      "(1984) loss = 1.8669, batch accuracy = 0.332275390625, val acc = 0.1796875\n",
      "(1985) loss = 1.8555, batch accuracy = 0.337646484375, val acc = 0.1822916716337204\n",
      "(1986) loss = 1.8615, batch accuracy = 0.3343505859375, val acc = 0.1614583432674408\n",
      "(1987) loss = 1.8621, batch accuracy = 0.3358154296875, val acc = 0.1796875\n",
      "(1988) loss = 1.8494, batch accuracy = 0.338623046875, val acc = 0.1770833432674408\n",
      "(1989) loss = 1.8477, batch accuracy = 0.339599609375, val acc = 0.1875\n",
      "(1990) loss = 1.8430, batch accuracy = 0.3392333984375, val acc = 0.1744791716337204\n",
      "(1991) loss = 1.8385, batch accuracy = 0.3421630859375, val acc = 0.1848958432674408\n",
      "(1992) loss = 1.8367, batch accuracy = 0.3404541015625, val acc = 0.1822916716337204\n",
      "(1993) loss = 1.8345, batch accuracy = 0.34326171875, val acc = 0.1822916716337204\n",
      "(1994) loss = 1.8366, batch accuracy = 0.3419189453125, val acc = 0.1692708432674408\n",
      "(1995) loss = 1.8374, batch accuracy = 0.34326171875, val acc = 0.1901041716337204\n",
      "(1996) loss = 1.8375, batch accuracy = 0.337646484375, val acc = 0.1796875\n",
      "(1997) loss = 1.8469, batch accuracy = 0.3419189453125, val acc = 0.1875\n",
      "(1998) loss = 1.8440, batch accuracy = 0.3394775390625, val acc = 0.1770833432674408\n",
      "(1999) loss = 1.8458, batch accuracy = 0.3406982421875, val acc = 0.1848958432674408\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainingHistory(losses=[2.7001776695251465, 2.3353967666625977, 2.2960526943206787, 2.272921562194824, 2.2572948932647705, 2.251805543899536, 2.246047258377075, 2.237260580062866, 2.2257370948791504, 2.2156782150268555, 2.209268808364868, 2.192901372909546, 2.1773579120635986, 2.168205499649048, 2.1605498790740967, 2.1536567211151123, 2.1554951667785645, 2.151613473892212, 2.139726161956787, 2.1345551013946533, 2.121931314468384, 2.1253185272216797, 2.1267752647399902, 2.1185853481292725, 2.1111702919006348, 2.101480007171631, 2.107832431793213, 2.096137762069702, 2.109063148498535, 2.0914316177368164, 2.094968557357788, 2.0818920135498047, 2.10025691986084, 2.0824124813079834, 2.0975332260131836, 2.0779056549072266, 2.083785057067871, 2.071578025817871, 2.0792131423950195, 2.063232660293579, 2.0698304176330566, 2.060471296310425, 2.0706980228424072, 2.0535058975219727, 2.0662918090820312, 2.0483031272888184, 2.059706211090088, 2.046405076980591, 2.058708429336548, 2.054506301879883, 2.0687215328216553, 2.060197114944458, 2.061361312866211, 2.0445618629455566, 2.046776056289673, 2.048466682434082, 2.051894187927246, 2.043684720993042, 2.047147274017334, 2.045806646347046, 2.042598247528076, 2.0345451831817627, 2.0342936515808105, 2.040008068084717, 2.0528767108917236, 2.0360546112060547, 2.0403995513916016, 2.044245719909668, 2.0388498306274414, 2.0310592651367188, 2.0412495136260986, 2.032116174697876, 2.0319337844848633, 2.028747320175171, 2.03206467628479, 2.0195603370666504, 2.0315937995910645, 2.032243490219116, 2.025846242904663, 2.023043394088745, 2.0242486000061035, 2.0201807022094727, 2.0236656665802, 2.0195963382720947, 2.0332183837890625, 2.01721453666687, 2.0239763259887695, 2.021901845932007, 2.0238969326019287, 2.0265491008758545, 2.027299165725708, 2.023784875869751, 2.0347375869750977, 2.025376081466675, 2.0253114700317383, 2.0235233306884766, 2.0110557079315186, 2.0095505714416504, 2.008702278137207, 2.0032427310943604, 2.0006284713745117, 1.9990758895874023, 1.9938981533050537, 1.9934258460998535, 2.0063369274139404, 2.0036725997924805, 2.0038249492645264, 2.0091421604156494, 2.0115270614624023, 2.008394241333008, 2.0143330097198486, 2.0181081295013428, 2.027101993560791, 2.0334579944610596, 2.0195515155792236, 2.010648727416992, 2.0080618858337402, 1.9970202445983887, 1.9963767528533936, 2.002368450164795, 2.008037805557251, 1.9976972341537476, 2.00471568107605, 2.004133701324463, 2.0062718391418457, 1.9952936172485352, 2.0103983879089355, 2.001241683959961, 2.0001728534698486, 2.0005383491516113, 1.992344617843628, 1.9975069761276245, 1.9934338331222534, 1.9979753494262695, 1.992609977722168, 1.9952677488327026, 1.9937800168991089, 1.986974835395813, 1.9881526231765747, 1.9948025941848755, 1.980393886566162, 1.9974223375320435, 1.9839988946914673, 1.987092137336731, 1.985543966293335, 1.985318899154663, 1.9806654453277588, 1.9793646335601807, 1.9821646213531494, 1.9920176267623901, 1.9810878038406372, 1.9884945154190063, 1.9814229011535645, 1.9876049757003784, 1.9860918521881104, 1.9875421524047852, 1.983620047569275, 1.9802336692810059, 1.973952054977417, 1.9812222719192505, 1.9681731462478638, 1.983215570449829, 1.9715520143508911, 1.9817851781845093, 1.9775022268295288, 1.975653886795044, 1.9762521982192993, 1.9761205911636353, 1.9774012565612793, 1.9736664295196533, 1.9762159585952759, 1.9874387979507446, 1.9788092374801636, 1.9899462461471558, 1.9926713705062866, 1.9925944805145264, 1.978646993637085, 1.989729642868042, 1.9800934791564941, 2.002153158187866, 2.000505208969116, 1.9938384294509888, 1.9858088493347168, 1.980912446975708, 1.9820345640182495, 1.9742543697357178, 1.9730814695358276, 1.974611759185791, 1.9793729782104492, 1.9780668020248413, 1.9731584787368774, 1.964819312095642, 1.9661991596221924, 1.9730747938156128, 1.974771499633789, 1.9686564207077026, 1.9709185361862183, 1.9688098430633545, 1.9698649644851685, 1.968641757965088, 1.9728500843048096, 1.963735818862915, 1.9643958806991577, 1.973552942276001, 1.9730608463287354, 1.9682170152664185, 1.9664554595947266, 1.9708060026168823, 1.9701865911483765, 1.975510597229004, 1.9839552640914917, 1.9787862300872803, 1.9725751876831055, 1.9724738597869873, 1.9641156196594238, 1.9657331705093384, 1.961317777633667, 1.964648962020874, 1.9590145349502563, 1.9559063911437988, 1.9664554595947266, 1.9700263738632202, 1.972446322441101, 1.9568687677383423, 1.9573078155517578, 1.9657033681869507, 1.9630383253097534, 1.967146396636963, 1.96019446849823, 1.963712215423584, 1.9589439630508423, 1.957336187362671, 1.951338768005371, 1.953886866569519, 1.9601705074310303, 1.9572906494140625, 1.9575287103652954, 1.9547463655471802, 1.966113805770874, 1.9499605894088745, 1.9509308338165283, 1.9543663263320923, 1.9663772583007812, 1.9619109630584717, 1.9592103958129883, 1.9565231800079346, 1.9539380073547363, 1.9503583908081055, 1.9588087797164917, 1.9534565210342407, 1.9507216215133667, 1.9430099725723267, 1.948671579360962, 1.9454673528671265, 1.941755771636963, 1.9484906196594238, 1.9488749504089355, 1.9553682804107666, 1.9521887302398682, 1.9508984088897705, 1.9500341415405273, 1.955665946006775, 1.945077657699585, 1.9497613906860352, 1.9445781707763672, 1.9455205202102661, 1.9405441284179688, 1.9441255331039429, 1.94891357421875, 1.9419150352478027, 1.964482069015503, 1.9687094688415527, 1.9390637874603271, 1.9514224529266357, 1.9498761892318726, 1.9478604793548584, 1.9604520797729492, 1.9578074216842651, 1.9559485912322998, 1.9566229581832886, 1.946558952331543, 1.9432952404022217, 1.9382671117782593, 1.9398572444915771, 1.9425113201141357, 1.9401251077651978, 1.9464846849441528, 1.936429738998413, 1.9489781856536865, 1.9538445472717285, 1.9490373134613037, 1.953909158706665, 1.9534631967544556, 1.9408161640167236, 1.9514601230621338, 1.936896562576294, 1.9359749555587769, 1.932879090309143, 1.9407657384872437, 1.9325865507125854, 1.9390077590942383, 1.9360074996948242, 1.939612865447998, 1.935710072517395, 1.9444022178649902, 1.9381446838378906, 1.9404716491699219, 1.9421777725219727, 1.9371577501296997, 1.9346518516540527, 1.9358240365982056, 1.9344172477722168, 1.9388679265975952, 1.9387097358703613, 1.9506046772003174, 1.9426298141479492, 1.94332754611969, 1.9419726133346558, 1.9400392770767212, 1.9411014318466187, 1.9481357336044312, 1.9348725080490112, 1.9451760053634644, 1.9526079893112183, 1.9536677598953247, 1.9593497514724731, 1.9539806842803955, 1.9475144147872925, 1.9558159112930298, 1.9496333599090576, 1.9456381797790527, 1.9360648393630981, 1.9341458082199097, 1.9384539127349854, 1.9367047548294067, 1.9339261054992676, 1.9386974573135376, 1.9423601627349854, 1.9467270374298096, 1.9397187232971191, 1.9482436180114746, 1.9492847919464111, 1.9392642974853516, 1.9460405111312866, 1.9399486780166626, 1.9462921619415283, 1.9525412321090698, 1.95575749874115, 1.9436155557632446, 1.9397486448287964, 1.9512171745300293, 1.9361262321472168, 1.9464681148529053, 1.9399447441101074, 1.9458976984024048, 1.9405986070632935, 1.9489153623580933, 1.951575517654419, 1.9434224367141724, 1.9568051099777222, 1.940836787223816, 1.9362256526947021, 1.934309959411621, 1.9424364566802979, 1.9489569664001465, 1.9507098197937012, 1.950447678565979, 1.953240156173706, 1.948349118232727, 1.9598627090454102, 1.958890438079834, 1.9383689165115356, 1.9322744607925415, 1.9353238344192505, 1.9327915906906128, 1.9368647336959839, 1.9212310314178467, 1.9317116737365723, 1.926846981048584, 1.9325742721557617, 1.9307975769042969, 1.9377940893173218, 1.9395672082901, 1.9321916103363037, 1.9271270036697388, 1.929718017578125, 1.9246282577514648, 1.9349303245544434, 1.9320775270462036, 1.9293031692504883, 1.9340159893035889, 1.923432469367981, 1.928497552871704, 1.9371960163116455, 1.9348772764205933, 1.9284999370574951, 1.9282326698303223, 1.9322569370269775, 1.9281166791915894, 1.9387997388839722, 1.9253448247909546, 1.9287647008895874, 1.9273371696472168, 1.9285469055175781, 1.922643780708313, 1.926809549331665, 1.916091799736023, 1.9222853183746338, 1.924628496170044, 1.924666166305542, 1.9143998622894287, 1.9208437204360962, 1.9125545024871826, 1.931628704071045, 1.920127511024475, 1.9167025089263916, 1.9091204404830933, 1.9168789386749268, 1.9146528244018555, 1.9092974662780762, 1.9336615800857544, 1.9165098667144775, 1.921188235282898, 1.9176470041275024, 1.9202749729156494, 1.919480562210083, 1.9254183769226074, 1.9182909727096558, 1.9152604341506958, 1.918298602104187, 1.9161826372146606, 1.9241582155227661, 1.9251421689987183, 1.9278854131698608, 1.926617980003357, 1.9444515705108643, 1.9450916051864624, 1.9410637617111206, 1.938273310661316, 1.9254379272460938, 1.9354509115219116, 1.9266350269317627, 1.928729772567749, 1.9176644086837769, 1.9090702533721924, 1.9093724489212036, 1.9096556901931763, 1.9099751710891724, 1.9187266826629639, 1.9117008447647095, 1.8996843099594116, 1.905611515045166, 1.9124760627746582, 1.9169021844863892, 1.9227416515350342, 1.9234638214111328, 1.9132099151611328, 1.9113198518753052, 1.9154695272445679, 1.9185857772827148, 1.9098854064941406, 1.9069424867630005, 1.921263337135315, 1.9171535968780518, 1.9137048721313477, 1.9175742864608765, 1.9214472770690918, 1.918527603149414, 1.9174246788024902, 1.910434603691101, 1.9109479188919067, 1.9087865352630615, 1.9192299842834473, 1.9201593399047852, 1.9206963777542114, 1.9168086051940918, 1.9366792440414429, 1.936336874961853, 1.9203319549560547, 1.920021891593933, 1.9239600896835327, 1.9292168617248535, 1.935920000076294, 1.9198758602142334, 1.9198901653289795, 1.9190622568130493, 1.9209587574005127, 1.9149242639541626, 1.917159914970398, 1.9221569299697876, 1.919405221939087, 1.9166412353515625, 1.91939377784729, 1.915692687034607, 1.9141913652420044, 1.916163444519043, 1.9151734113693237, 1.9116413593292236, 1.9189480543136597, 1.9173916578292847, 1.9206786155700684, 1.9203853607177734, 1.9372433423995972, 1.9304828643798828, 1.9367530345916748, 1.9346339702606201, 1.9386789798736572, 1.9228707551956177, 1.9348042011260986, 1.9219199419021606, 1.9107635021209717, 1.914769172668457, 1.932837724685669, 1.9320263862609863, 1.9251388311386108, 1.9344425201416016, 1.9280256032943726, 1.9180917739868164, 1.923775315284729, 1.911307692527771, 1.908942461013794, 1.900098204612732, 1.9137986898422241, 1.907364845275879, 1.9011969566345215, 1.907304048538208, 1.9081987142562866, 1.9037153720855713, 1.9149765968322754, 1.9083642959594727, 1.914717674255371, 1.9047318696975708, 1.9207528829574585, 1.916930913925171, 1.9224698543548584, 1.928437352180481, 1.9190332889556885, 1.9179471731185913, 1.917317509651184, 1.9244906902313232, 1.9185519218444824, 1.9256068468093872, 1.9131852388381958, 1.9185473918914795, 1.9168496131896973, 1.9095300436019897, 1.9155545234680176, 1.9179331064224243, 1.9116116762161255, 1.9123008251190186, 1.9140459299087524, 1.9154105186462402, 1.9069995880126953, 1.914899230003357, 1.905208706855774, 1.9008872509002686, 1.8999453783035278, 1.9105305671691895, 1.9158544540405273, 1.9232820272445679, 1.9167118072509766, 1.9114512205123901, 1.906880497932434, 1.9145715236663818, 1.9121768474578857, 1.9107915163040161, 1.9148273468017578, 1.9075281620025635, 1.9113023281097412, 1.9067808389663696, 1.93181574344635, 1.9271694421768188, 1.9176065921783447, 1.9094438552856445, 1.9122507572174072, 1.908366322517395, 1.9175478219985962, 1.9163768291473389, 1.9111998081207275, 1.910103678703308, 1.9112746715545654, 1.9023640155792236, 1.8979719877243042, 1.8936501741409302, 1.9017632007598877, 1.8923295736312866, 1.8991352319717407, 1.9038070440292358, 1.8951083421707153, 1.9042973518371582, 1.897477626800537, 1.9015536308288574, 1.8985809087753296, 1.910102128982544, 1.8967012166976929, 1.9129804372787476, 1.9132111072540283, 1.922200083732605, 1.9014396667480469, 1.911423683166504, 1.901922583580017, 1.903278112411499, 1.9092578887939453, 1.9102526903152466, 1.9061260223388672, 1.907381534576416, 1.9172024726867676, 1.9129855632781982, 1.918139100074768, 1.9097603559494019, 1.9146407842636108, 1.915794014930725, 1.9067280292510986, 1.9158421754837036, 1.9027793407440186, 1.906312108039856, 1.9055472612380981, 1.9053547382354736, 1.9075177907943726, 1.9055441617965698, 1.8968849182128906, 1.8979955911636353, 1.8983443975448608, 1.8976716995239258, 1.8935694694519043, 1.9102048873901367, 1.9034782648086548, 1.909388780593872, 1.9067445993423462, 1.9174610376358032, 1.9149960279464722, 1.9103329181671143, 1.897376537322998, 1.9161803722381592, 1.9044616222381592, 1.904834508895874, 1.8991599082946777, 1.8957180976867676, 1.9023349285125732, 1.9001591205596924, 1.8990259170532227, 1.9031797647476196, 1.8961834907531738, 1.9023891687393188, 1.8918261528015137, 1.9080003499984741, 1.9009095430374146, 1.897309422492981, 1.8949214220046997, 1.900132179260254, 1.8971328735351562, 1.9079772233963013, 1.9060840606689453, 1.9062845706939697, 1.9045448303222656, 1.905221939086914, 1.9087283611297607, 1.909127950668335, 1.9073957204818726, 1.9082614183425903, 1.9034243822097778, 1.9064555168151855, 1.901415467262268, 1.898423194885254, 1.899814248085022, 1.9045512676239014, 1.9079108238220215, 1.9015302658081055, 1.9064381122589111, 1.907274603843689, 1.9111970663070679, 1.9177778959274292, 1.9123176336288452, 1.9142026901245117, 1.9141815900802612, 1.913831114768982, 1.9110158681869507, 1.9132845401763916, 1.8988127708435059, 1.8970746994018555, 1.8944543600082397, 1.8926103115081787, 1.9033199548721313, 1.8900848627090454, 1.8989156484603882, 1.8950743675231934, 1.8930515050888062, 1.8931779861450195, 1.8880412578582764, 1.905276894569397, 1.8892232179641724, 1.8883215188980103, 1.8920700550079346, 1.889966607093811, 1.8974783420562744, 1.8998817205429077, 1.8934810161590576, 1.8908016681671143, 1.8979771137237549, 1.8910030126571655, 1.8958933353424072, 1.895742654800415, 1.9026538133621216, 1.8926447629928589, 1.8931609392166138, 1.891701579093933, 1.893347978591919, 1.9013340473175049, 1.9108552932739258, 1.920314073562622, 1.931484580039978, 1.918465495109558, 1.9053200483322144, 1.9014984369277954, 1.8954639434814453, 1.8898893594741821, 1.8883970975875854, 1.8850845098495483, 1.8883265256881714, 1.8898468017578125, 1.8909534215927124, 1.8935868740081787, 1.8858201503753662, 1.8870000839233398, 1.898147702217102, 1.8999311923980713, 1.8936388492584229, 1.8946365118026733, 1.8887385129928589, 1.8992809057235718, 1.8897312879562378, 1.888664722442627, 1.8942203521728516, 1.8961032629013062, 1.8971275091171265, 1.8887306451797485, 1.8986365795135498, 1.9041556119918823, 1.902337670326233, 1.891452670097351, 1.8928933143615723, 1.8935916423797607, 1.8939098119735718, 1.897606372833252, 1.8979405164718628, 1.9105231761932373, 1.9080744981765747, 1.912915587425232, 1.9144941568374634, 1.9150588512420654, 1.9158146381378174, 1.9119577407836914, 1.9089789390563965, 1.9092915058135986, 1.9190534353256226, 1.9013445377349854, 1.8886603116989136, 1.9022616147994995, 1.8896424770355225, 1.8959691524505615, 1.8992244005203247, 1.8976709842681885, 1.9061452150344849, 1.90024995803833, 1.9037569761276245, 1.9009032249450684, 1.8894972801208496, 1.9014424085617065, 1.9036917686462402, 1.8973137140274048, 1.8921213150024414, 1.8974336385726929, 1.8933050632476807, 1.8943806886672974, 1.9084830284118652, 1.9087885618209839, 1.8966997861862183, 1.9036195278167725, 1.8906835317611694, 1.9005855321884155, 1.891251802444458, 1.8877171277999878, 1.8844159841537476, 1.8896199464797974, 1.8881944417953491, 1.8881126642227173, 1.8857909440994263, 1.8913297653198242, 1.8846911191940308, 1.8874759674072266, 1.882143259048462, 1.887779951095581, 1.8782179355621338, 1.8753831386566162, 1.8839025497436523, 1.8815771341323853, 1.884016513824463, 1.8831164836883545, 1.8814594745635986, 1.8864836692810059, 1.8781452178955078, 1.883165717124939, 1.8873467445373535, 1.88593590259552, 1.8950409889221191, 1.8844009637832642, 1.8965094089508057, 1.8830777406692505, 1.8918044567108154, 1.8866522312164307, 1.8936882019042969, 1.9102500677108765, 1.8976467847824097, 1.8914388418197632, 1.8946950435638428, 1.8958855867385864, 1.9025765657424927, 1.907909631729126, 1.8832557201385498, 1.8876299858093262, 1.8732978105545044, 1.8840328454971313, 1.8840464353561401, 1.8860048055648804, 1.8839842081069946, 1.8886590003967285, 1.8861039876937866, 1.8824429512023926, 1.8869280815124512, 1.885366439819336, 1.8936610221862793, 1.889123558998108, 1.8840798139572144, 1.9010776281356812, 1.900676965713501, 1.8903508186340332, 1.891398310661316, 1.8946101665496826, 1.891531229019165, 1.8918142318725586, 1.8883212804794312, 1.894189476966858, 1.8971757888793945, 1.8888535499572754, 1.8980858325958252, 1.8862619400024414, 1.8891942501068115, 1.8976730108261108, 1.9005130529403687, 1.889561414718628, 1.898996353149414, 1.8899418115615845, 1.899131417274475, 1.8956972360610962, 1.9023138284683228, 1.89942467212677, 1.897840976715088, 1.8998826742172241, 1.8930524587631226, 1.8806533813476562, 1.8887617588043213, 1.8861593008041382, 1.880109190940857, 1.8848592042922974, 1.90559720993042, 1.8879072666168213, 1.8921024799346924, 1.888358235359192, 1.8851428031921387, 1.8816734552383423, 1.8832319974899292, 1.8899933099746704, 1.8826881647109985, 1.8930816650390625, 1.89700448513031, 1.8894593715667725, 1.8924459218978882, 1.8924336433410645, 1.897479772567749, 1.8883328437805176, 1.8848519325256348, 1.8906301259994507, 1.8904727697372437, 1.885640263557434, 1.8913251161575317, 1.8815388679504395, 1.8831650018692017, 1.8833814859390259, 1.8904359340667725, 1.8910709619522095, 1.8871421813964844, 1.886867880821228, 1.888930320739746, 1.8779858350753784, 1.8807456493377686, 1.876813292503357, 1.8837417364120483, 1.8810350894927979, 1.8729861974716187, 1.875402569770813, 1.8704910278320312, 1.8805391788482666, 1.8861963748931885, 1.8820379972457886, 1.8767069578170776, 1.8851557970046997, 1.881644368171692, 1.8880188465118408, 1.884346842765808, 1.892446756362915, 1.8749334812164307, 1.880905032157898, 1.885416865348816, 1.8821502923965454, 1.8898259401321411, 1.8815432786941528, 1.8857485055923462, 1.8741029500961304, 1.8773657083511353, 1.8829272985458374, 1.873327612876892, 1.8814631700515747, 1.873468279838562, 1.8852314949035645, 1.875616431236267, 1.8764441013336182, 1.8712644577026367, 1.8921040296554565, 1.8923168182373047, 1.887407660484314, 1.8837542533874512, 1.877403974533081, 1.8710997104644775, 1.8748292922973633, 1.8648957014083862, 1.8745787143707275, 1.8706978559494019, 1.8727452754974365, 1.874306321144104, 1.875447154045105, 1.877227544784546, 1.8759994506835938, 1.8802359104156494, 1.8798044919967651, 1.8883895874023438, 1.893542766571045, 1.882414698600769, 1.8807255029678345, 1.882340431213379, 1.8796148300170898, 1.8805832862854004, 1.8925502300262451, 1.9009473323822021, 1.894446849822998, 1.8862603902816772, 1.8936517238616943, 1.8873116970062256, 1.8777631521224976, 1.8788576126098633, 1.8825795650482178, 1.8787373304367065, 1.885694980621338, 1.8780317306518555, 1.8726434707641602, 1.8728196620941162, 1.8788127899169922, 1.871035099029541, 1.8830088376998901, 1.8677386045455933, 1.8736950159072876, 1.8636709451675415, 1.8718756437301636, 1.8716175556182861, 1.8740012645721436, 1.8774253129959106, 1.8788481950759888, 1.8862144947052002, 1.8895305395126343, 1.8800945281982422, 1.8874034881591797, 1.8918579816818237, 1.8828935623168945, 1.8832086324691772, 1.8729113340377808, 1.8751394748687744, 1.891273856163025, 1.8785117864608765, 1.8827619552612305, 1.877950668334961, 1.8813261985778809, 1.885265588760376, 1.8817007541656494, 1.874064564704895, 1.8791816234588623, 1.8724281787872314, 1.8788644075393677, 1.8767900466918945, 1.8725148439407349, 1.8763059377670288, 1.8773599863052368, 1.8777066469192505, 1.877331018447876, 1.8740994930267334, 1.8800281286239624, 1.8732348680496216, 1.8800867795944214, 1.8715367317199707, 1.8760937452316284, 1.881964921951294, 1.8947010040283203, 1.8950713872909546, 1.8897625207901, 1.8831279277801514, 1.8812339305877686, 1.8752613067626953, 1.8726645708084106, 1.8755500316619873, 1.87562894821167, 1.8732174634933472, 1.8725509643554688, 1.867881178855896, 1.8725773096084595, 1.8681577444076538, 1.87037193775177, 1.8714191913604736, 1.8730754852294922, 1.8722641468048096, 1.8674805164337158, 1.8724528551101685, 1.872881531715393, 1.8810834884643555, 1.8728184700012207, 1.873682975769043, 1.8773823976516724, 1.8737026453018188, 1.8727658987045288, 1.8766777515411377, 1.8780368566513062, 1.8933135271072388, 1.8850693702697754, 1.876063585281372, 1.8702514171600342, 1.8721781969070435, 1.8754667043685913, 1.8707301616668701, 1.8696080446243286, 1.8727055788040161, 1.8691024780273438, 1.8785563707351685, 1.8692195415496826, 1.8743151426315308, 1.8750134706497192, 1.8745450973510742, 1.8765984773635864, 1.8786439895629883, 1.8753583431243896, 1.871573567390442, 1.8755987882614136, 1.869649887084961, 1.8707082271575928, 1.870774269104004, 1.8761889934539795, 1.8691335916519165, 1.8662960529327393, 1.8807895183563232, 1.877415657043457, 1.8780364990234375, 1.876945972442627, 1.8912508487701416, 1.8778066635131836, 1.8870128393173218, 1.8776850700378418, 1.8769187927246094, 1.8805307149887085, 1.8899714946746826, 1.8775194883346558, 1.877457618713379, 1.8770488500595093, 1.8836112022399902, 1.8715846538543701, 1.8864601850509644, 1.8808311223983765, 1.8791558742523193, 1.8736331462860107, 1.8724980354309082, 1.8743153810501099, 1.8638544082641602, 1.87131929397583, 1.866249442100525, 1.8712857961654663, 1.8673206567764282, 1.8763618469238281, 1.874018669128418, 1.8748682737350464, 1.8737715482711792, 1.8886092901229858, 1.8856264352798462, 1.8770841360092163, 1.8846678733825684, 1.8815926313400269, 1.8777408599853516, 1.86522376537323, 1.8767108917236328, 1.8883965015411377, 1.8772836923599243, 1.876310110092163, 1.8796732425689697, 1.8849084377288818, 1.880164384841919, 1.8847644329071045, 1.8814088106155396, 1.8837993144989014, 1.8790210485458374, 1.882813572883606, 1.8873369693756104, 1.8865097761154175, 1.8899919986724854, 1.8853957653045654, 1.8839656114578247, 1.8837774991989136, 1.8870980739593506, 1.892134189605713, 1.8810076713562012, 1.8778107166290283, 1.8758182525634766, 1.877906084060669, 1.8852105140686035, 1.8823577165603638, 1.8902583122253418, 1.8771826028823853, 1.8717765808105469, 1.8735206127166748, 1.8665423393249512, 1.882947325706482, 1.8666155338287354, 1.880402684211731, 1.8766372203826904, 1.879148006439209, 1.8923496007919312, 1.8652633428573608, 1.8774323463439941, 1.8648395538330078, 1.8806644678115845, 1.8714208602905273, 1.8728028535842896, 1.864906668663025, 1.8702696561813354, 1.869093894958496, 1.8666138648986816, 1.8667620420455933, 1.8663617372512817, 1.8629405498504639, 1.8605211973190308, 1.8599039316177368, 1.8571704626083374, 1.8588320016860962, 1.8611093759536743, 1.8578662872314453, 1.853708267211914, 1.8616801500320435, 1.8564850091934204, 1.8736398220062256, 1.8561021089553833, 1.8661867380142212, 1.8787977695465088, 1.8673557043075562, 1.8655602931976318, 1.8762905597686768, 1.8708003759384155, 1.8669462203979492, 1.8652461767196655, 1.8610552549362183, 1.858052372932434, 1.859668254852295, 1.8614511489868164, 1.8606394529342651, 1.8659584522247314, 1.8666255474090576, 1.8722304105758667, 1.873469591140747, 1.8743784427642822, 1.8653452396392822, 1.8675585985183716, 1.88177490234375, 1.8732120990753174, 1.87367582321167, 1.880771517753601, 1.8837958574295044, 1.8779244422912598, 1.8653017282485962, 1.867523193359375, 1.8952441215515137, 1.89292573928833, 1.8786051273345947, 1.8683631420135498, 1.867247462272644, 1.8607779741287231, 1.8620811700820923, 1.8634852170944214, 1.8535313606262207, 1.8622419834136963, 1.855379581451416, 1.866792917251587, 1.8660036325454712, 1.8751052618026733, 1.8807772397994995, 1.8832875490188599, 1.8772393465042114, 1.8767001628875732, 1.8592534065246582, 1.8653616905212402, 1.8499979972839355, 1.8602231740951538, 1.8649637699127197, 1.8713924884796143, 1.8775790929794312, 1.870055913925171, 1.8612052202224731, 1.8731807470321655, 1.8846640586853027, 1.9033195972442627, 1.8934673070907593, 1.8954037427902222, 1.8786466121673584, 1.8828024864196777, 1.8889532089233398, 1.8740981817245483, 1.8694711923599243, 1.867980718612671, 1.8734689950942993, 1.8736896514892578, 1.8737167119979858, 1.8728584051132202, 1.8798424005508423, 1.86464524269104, 1.8677564859390259, 1.8623281717300415, 1.8663431406021118, 1.8628469705581665, 1.8593610525131226, 1.860131859779358, 1.855995535850525, 1.8557980060577393, 1.8661432266235352, 1.8566957712173462, 1.8638248443603516, 1.858549952507019, 1.8590151071548462, 1.865622878074646, 1.8653854131698608, 1.879428744316101, 1.8671640157699585, 1.8731708526611328, 1.8686249256134033, 1.8623591661453247, 1.8680329322814941, 1.860403299331665, 1.8696545362472534, 1.857659101486206, 1.8606245517730713, 1.8633551597595215, 1.8600560426712036, 1.8610032796859741, 1.8595621585845947, 1.8574188947677612, 1.857602596282959, 1.8670403957366943, 1.8629570007324219, 1.8644009828567505, 1.8745490312576294, 1.8695776462554932, 1.8660823106765747, 1.8701434135437012, 1.8780544996261597, 1.8741892576217651, 1.8948653936386108, 1.873518943786621, 1.8617781400680542, 1.8583056926727295, 1.8676271438598633, 1.8626134395599365, 1.8598673343658447, 1.8650281429290771, 1.857913613319397, 1.8768709897994995, 1.8624823093414307, 1.8757096529006958, 1.860886573791504, 1.8684240579605103, 1.8606425523757935, 1.8698002099990845, 1.8578715324401855, 1.8601235151290894, 1.8581101894378662, 1.8743813037872314, 1.8692785501480103, 1.8641520738601685, 1.871131420135498, 1.8634064197540283, 1.8721460103988647, 1.8654162883758545, 1.8732759952545166, 1.888916254043579, 1.8986966609954834, 1.89983069896698, 1.8917055130004883, 1.880266547203064, 1.8879259824752808, 1.8907718658447266, 1.8755451440811157, 1.8746188879013062, 1.8734370470046997, 1.8644946813583374, 1.8637266159057617, 1.8610671758651733, 1.8680237531661987, 1.8619883060455322, 1.8677366971969604, 1.8678776025772095, 1.8783092498779297, 1.8660823106765747, 1.8671307563781738, 1.8697876930236816, 1.8663830757141113, 1.865281105041504, 1.8610100746154785, 1.8650648593902588, 1.8662301301956177, 1.8790721893310547, 1.8748425245285034, 1.870064377784729, 1.8620327711105347, 1.8599600791931152, 1.8631253242492676, 1.869820475578308, 1.8672045469284058, 1.8721795082092285, 1.8714724779129028, 1.8775306940078735, 1.8655692338943481, 1.8587088584899902, 1.8520804643630981, 1.8558323383331299, 1.85298752784729, 1.851098895072937, 1.8584576845169067, 1.8619673252105713, 1.8628515005111694, 1.8616498708724976, 1.862122654914856, 1.8612632751464844, 1.8637205362319946, 1.8603999614715576, 1.877314805984497, 1.8674304485321045, 1.862021803855896, 1.8596895933151245, 1.8621442317962646, 1.862714409828186, 1.8675239086151123, 1.8894871473312378, 1.8901337385177612, 1.8954112529754639, 1.9049465656280518, 1.8978047370910645, 1.8901747465133667, 1.8735989332199097, 1.8793519735336304, 1.8757705688476562, 1.8631319999694824, 1.8586812019348145, 1.8580174446105957, 1.8605355024337769, 1.8618130683898926, 1.8579884767532349, 1.8548134565353394, 1.8587937355041504, 1.8619086742401123, 1.854610562324524, 1.858205795288086, 1.862566351890564, 1.8612953424453735, 1.8626794815063477, 1.8634103536605835, 1.8718633651733398, 1.869644284248352, 1.8565832376480103, 1.871405839920044, 1.8557404279708862, 1.8648481369018555, 1.8722097873687744, 1.859360933303833, 1.8718470335006714, 1.8596333265304565, 1.868892788887024, 1.8861817121505737, 1.8837366104125977, 1.8843721151351929, 1.8773233890533447, 1.8802378177642822, 1.8742985725402832, 1.8802744150161743, 1.8658946752548218, 1.871054768562317, 1.8688039779663086, 1.8762028217315674, 1.8759026527404785, 1.8804088830947876, 1.8783742189407349, 1.8742049932479858, 1.8785457611083984, 1.8833019733428955, 1.8658983707427979, 1.8733845949172974, 1.8648591041564941, 1.8714972734451294, 1.8699983358383179, 1.866810917854309, 1.8616632223129272, 1.8590750694274902, 1.8622759580612183, 1.8660521507263184, 1.8673502206802368, 1.8708586692810059, 1.8698674440383911, 1.866904377937317, 1.8641669750213623, 1.875710129737854, 1.861820936203003, 1.8748396635055542, 1.8676894903182983, 1.8613746166229248, 1.8652974367141724, 1.8595844507217407, 1.8721286058425903, 1.8628255128860474, 1.8550461530685425, 1.8667078018188477, 1.8676235675811768, 1.8679277896881104, 1.8691465854644775, 1.865448236465454, 1.8626452684402466, 1.856452465057373, 1.856982707977295, 1.8639109134674072, 1.8591502904891968, 1.8601109981536865, 1.8623101711273193, 1.8654783964157104, 1.8723276853561401, 1.8736823797225952, 1.861481785774231, 1.8638979196548462, 1.8628300428390503, 1.86943519115448, 1.8604623079299927, 1.8648836612701416, 1.857539415359497, 1.8585290908813477, 1.863243818283081, 1.860697627067566, 1.8627073764801025, 1.855403184890747, 1.8616628646850586, 1.8556756973266602, 1.865471601486206, 1.8699164390563965, 1.8697702884674072, 1.868191123008728, 1.8611035346984863, 1.8579177856445312, 1.859057903289795, 1.8662738800048828, 1.8738048076629639, 1.8659451007843018, 1.8791327476501465, 1.8717039823532104, 1.8966718912124634, 1.863423228263855, 1.8739032745361328, 1.8575974702835083, 1.8566439151763916, 1.8525645732879639, 1.8568880558013916, 1.861916184425354, 1.855544924736023, 1.8588396310806274, 1.8553849458694458, 1.8596185445785522, 1.8519396781921387, 1.858087182044983, 1.852852463722229, 1.853395700454712, 1.8538836240768433, 1.8542526960372925, 1.8601154088974, 1.8597408533096313, 1.8591550588607788, 1.859752893447876, 1.860920786857605, 1.85722017288208, 1.8601700067520142, 1.865604281425476, 1.8697038888931274, 1.8793078660964966, 1.8800208568572998, 1.869680404663086, 1.8598133325576782, 1.8693182468414307, 1.8683534860610962, 1.863646388053894, 1.862331509590149, 1.8535655736923218, 1.862404227256775, 1.8560487031936646, 1.8572793006896973, 1.8653923273086548, 1.8715931177139282, 1.8655812740325928, 1.876582384109497, 1.8620598316192627, 1.885637640953064, 1.8831928968429565, 1.8724274635314941, 1.8595608472824097, 1.8604406118392944, 1.8691980838775635, 1.8718289136886597, 1.8683011531829834, 1.8647476434707642, 1.8664659261703491, 1.8526052236557007, 1.8543654680252075, 1.8653658628463745, 1.8605306148529053, 1.8584797382354736, 1.8553656339645386, 1.853088617324829, 1.862732172012329, 1.8491371870040894, 1.848605990409851, 1.8468729257583618, 1.8512033224105835, 1.8608667850494385, 1.8636860847473145, 1.8612802028656006, 1.8622056245803833, 1.8611167669296265, 1.857042670249939, 1.864664912223816, 1.8602403402328491, 1.8662861585617065, 1.850411295890808, 1.8596996068954468, 1.851049542427063, 1.8539273738861084, 1.866215705871582, 1.8581050634384155, 1.854130744934082, 1.8563435077667236, 1.8546795845031738, 1.8481448888778687, 1.8457798957824707, 1.8416157960891724, 1.8456660509109497, 1.84263277053833, 1.8430527448654175, 1.8422917127609253, 1.8559223413467407, 1.8512388467788696, 1.868967056274414, 1.8603529930114746, 1.867638349533081, 1.8620537519454956, 1.8661508560180664, 1.8699239492416382, 1.8669257164001465, 1.855480670928955, 1.8605937957763672, 1.8505754470825195, 1.8519611358642578, 1.8535490036010742, 1.8538379669189453, 1.8513829708099365, 1.8471468687057495, 1.8552234172821045, 1.8600577116012573, 1.8592674732208252, 1.8537062406539917, 1.8708195686340332, 1.8532432317733765, 1.854623556137085, 1.8508292436599731, 1.851470947265625, 1.8466984033584595, 1.8543106317520142, 1.8602079153060913, 1.8535215854644775, 1.8611565828323364, 1.8507330417633057, 1.8495986461639404, 1.8555339574813843, 1.8629965782165527, 1.8636808395385742, 1.865342140197754, 1.8551996946334839, 1.8562099933624268, 1.8562192916870117, 1.8504265546798706, 1.8508838415145874, 1.852527141571045, 1.8547158241271973, 1.853943109512329, 1.8576399087905884, 1.848562479019165, 1.8531615734100342, 1.8509970903396606, 1.8542431592941284, 1.846109390258789, 1.8551315069198608, 1.849894642829895, 1.8575013875961304, 1.8684933185577393, 1.85875403881073, 1.8647903203964233, 1.8632149696350098, 1.8572545051574707, 1.8534373044967651, 1.85586416721344, 1.8679730892181396, 1.85977303981781, 1.8607182502746582, 1.8713178634643555, 1.8817497491836548, 1.8701717853546143, 1.8643267154693604, 1.860846996307373, 1.8546020984649658, 1.8595155477523804, 1.8605865240097046, 1.8554493188858032, 1.854143738746643, 1.8593337535858154, 1.8732458353042603, 1.8588013648986816, 1.863213300704956, 1.8510280847549438, 1.8518390655517578, 1.848852276802063, 1.8535174131393433, 1.8476959466934204, 1.8585841655731201, 1.851606011390686, 1.8560742139816284, 1.8524341583251953, 1.8536310195922852, 1.853555679321289, 1.8584020137786865, 1.8595911264419556, 1.8662958145141602, 1.861508846282959, 1.8599743843078613, 1.8558955192565918, 1.8641998767852783, 1.8622167110443115, 1.8648210763931274, 1.8824208974838257, 1.8649803400039673, 1.8575249910354614, 1.8516806364059448, 1.8531947135925293, 1.8586944341659546, 1.8548719882965088, 1.8594350814819336, 1.853635549545288, 1.8547232151031494, 1.8505809307098389, 1.8570449352264404, 1.853546142578125, 1.8512418270111084, 1.8539113998413086, 1.8555943965911865, 1.8461828231811523, 1.8467637300491333, 1.8500914573669434, 1.860148310661316, 1.8472285270690918, 1.8475022315979004, 1.8508131504058838, 1.8553657531738281, 1.8546693325042725, 1.851114273071289, 1.859190821647644, 1.852891206741333, 1.8568384647369385, 1.853351354598999, 1.8549549579620361, 1.8586645126342773, 1.8532440662384033, 1.8617393970489502, 1.8495010137557983, 1.85223388671875, 1.8477063179016113, 1.8535141944885254, 1.8527002334594727, 1.851746678352356, 1.8556222915649414, 1.8494601249694824, 1.8509663343429565, 1.8626744747161865, 1.8620787858963013, 1.8671730756759644, 1.8693174123764038, 1.8587524890899658, 1.8581442832946777, 1.8591351509094238, 1.8544448614120483, 1.852882742881775, 1.853775143623352, 1.8457422256469727, 1.8458276987075806, 1.8512604236602783, 1.848585844039917, 1.8501112461090088, 1.8487694263458252, 1.8550448417663574, 1.8511476516723633, 1.851114273071289, 1.8483073711395264, 1.8457239866256714, 1.8478285074234009, 1.8530726432800293, 1.8522144556045532, 1.8541593551635742, 1.8574161529541016, 1.8506252765655518, 1.845823049545288, 1.8541004657745361, 1.850785255432129, 1.8608722686767578, 1.8559614419937134, 1.8477957248687744, 1.8573741912841797, 1.8563895225524902, 1.8606239557266235, 1.8697636127471924, 1.8661243915557861, 1.870132327079773, 1.8690863847732544, 1.8570427894592285, 1.8564578294754028, 1.8559699058532715, 1.849665880203247, 1.8465944528579712, 1.8487201929092407, 1.8461024761199951, 1.8467066287994385, 1.8424084186553955, 1.8431180715560913, 1.8409570455551147, 1.8457974195480347, 1.8441879749298096, 1.8370977640151978, 1.8391923904418945, 1.8442410230636597, 1.8395593166351318, 1.854006290435791, 1.8439536094665527, 1.8457139730453491, 1.8473987579345703, 1.853226900100708, 1.845793604850769, 1.8613190650939941, 1.8555030822753906, 1.8509759902954102, 1.8584703207015991, 1.8574186563491821, 1.850388765335083, 1.8575620651245117, 1.8837900161743164, 1.8607864379882812, 1.8719522953033447, 1.8693434000015259, 1.8579914569854736, 1.8549644947052002, 1.844730019569397, 1.8513754606246948, 1.8434902429580688, 1.8474656343460083, 1.8485698699951172, 1.8496274948120117, 1.8508270978927612, 1.8577364683151245, 1.84760582447052, 1.8548249006271362, 1.8465077877044678, 1.853398084640503, 1.840808391571045, 1.8559492826461792, 1.8467005491256714, 1.8590548038482666, 1.8482266664505005, 1.852574348449707, 1.863376259803772, 1.858081340789795, 1.8560528755187988, 1.8533709049224854, 1.847903847694397, 1.8409767150878906, 1.837949275970459, 1.8427881002426147, 1.838393211364746, 1.8391425609588623, 1.8433033227920532, 1.8518019914627075, 1.8456525802612305, 1.8435931205749512, 1.855102300643921, 1.8470458984375, 1.8505924940109253, 1.8736027479171753, 1.8606683015823364, 1.8562967777252197, 1.862457036972046, 1.8673242330551147, 1.8642351627349854, 1.8593131303787231, 1.8559759855270386, 1.848818302154541, 1.8587664365768433, 1.862472653388977, 1.8595921993255615, 1.8552051782608032, 1.8544564247131348, 1.8488786220550537, 1.844870924949646, 1.8422694206237793, 1.852365493774414, 1.861214280128479, 1.8655550479888916, 1.8622266054153442, 1.8586320877075195, 1.8650102615356445, 1.8528709411621094, 1.8638393878936768, 1.8568391799926758, 1.850982666015625, 1.8514364957809448, 1.8447344303131104, 1.8470282554626465, 1.8432055711746216, 1.862683653831482, 1.8542088270187378, 1.8515666723251343, 1.8436588048934937, 1.848755955696106, 1.8395992517471313, 1.8450630903244019, 1.8487801551818848, 1.8454700708389282, 1.8459792137145996, 1.83945631980896, 1.8385906219482422, 1.8381845951080322, 1.84555983543396, 1.8647270202636719, 1.8642905950546265, 1.8798884153366089, 1.901466727256775, 1.8883684873580933, 1.87783682346344, 1.8765641450881958, 1.8620669841766357, 1.8619576692581177, 1.854944109916687, 1.8519513607025146, 1.855749249458313, 1.8490996360778809, 1.8499161005020142, 1.8483631610870361, 1.84149968624115, 1.842002511024475, 1.8398187160491943, 1.838521122932434, 1.8428226709365845, 1.8491191864013672, 1.8535114526748657, 1.8555493354797363, 1.8549448251724243, 1.843738079071045, 1.8491129875183105, 1.8495975732803345, 1.8406267166137695, 1.8509057760238647, 1.8534135818481445, 1.838621735572815, 1.843420386314392, 1.8526073694229126, 1.8552935123443604, 1.8435254096984863, 1.8467504978179932, 1.8405035734176636, 1.8496954441070557, 1.842273473739624, 1.844833493232727, 1.8387336730957031, 1.852549433708191, 1.8429951667785645, 1.8430956602096558, 1.8448045253753662, 1.8481403589248657, 1.8475414514541626, 1.8489056825637817, 1.8561155796051025, 1.8641748428344727, 1.8574095964431763, 1.8603136539459229, 1.8575365543365479, 1.851847767829895, 1.849671483039856, 1.8603096008300781, 1.858888864517212, 1.8714628219604492, 1.8564389944076538, 1.8545289039611816, 1.8442034721374512, 1.850695013999939, 1.8501938581466675, 1.8447030782699585, 1.8403711318969727, 1.842602014541626, 1.8385964632034302, 1.8451440334320068, 1.8380616903305054, 1.8403239250183105, 1.8436739444732666, 1.8465882539749146, 1.8416917324066162, 1.8433250188827515, 1.858089804649353, 1.8471083641052246, 1.8448017835617065, 1.8462833166122437, 1.8506780862808228, 1.8451907634735107, 1.8504759073257446, 1.8431096076965332, 1.8418954610824585, 1.8427979946136475, 1.8485169410705566, 1.8513060808181763, 1.850937843322754, 1.8503516912460327, 1.8523248434066772, 1.8506016731262207, 1.858436107635498, 1.8484649658203125, 1.8542083501815796, 1.8461233377456665, 1.8418469429016113, 1.8440232276916504, 1.851931095123291, 1.848874807357788, 1.851708173751831, 1.8523192405700684, 1.843878149986267, 1.8474730253219604, 1.8525705337524414, 1.861406683921814, 1.8484803438186646, 1.8590553998947144, 1.8518407344818115, 1.8487861156463623, 1.8512282371520996, 1.8459421396255493, 1.8460952043533325, 1.84388267993927, 1.8501288890838623, 1.8503128290176392, 1.848684549331665, 1.8696010112762451, 1.8542677164077759, 1.8541990518569946, 1.8668503761291504, 1.8554960489273071, 1.8614970445632935, 1.8620957136154175, 1.849397897720337, 1.847719669342041, 1.8430113792419434, 1.8385227918624878, 1.8367173671722412, 1.8345481157302856, 1.836624026298523, 1.8373643159866333, 1.8375306129455566, 1.84690523147583, 1.843981385231018, 1.8457953929901123], train_accuracies=[tensor(0.0939), tensor(0.1921), tensor(0.2000), tensor(0.2002), tensor(0.2024), tensor(0.2067), tensor(0.2100), tensor(0.2090), tensor(0.2131), tensor(0.2166), tensor(0.2192), tensor(0.2303), tensor(0.2302), tensor(0.2346), tensor(0.2399), tensor(0.2466), tensor(0.2510), tensor(0.2500), tensor(0.2542), tensor(0.2535), tensor(0.2566), tensor(0.2545), tensor(0.2545), tensor(0.2573), tensor(0.2607), tensor(0.2592), tensor(0.2616), tensor(0.2631), tensor(0.2644), tensor(0.2651), tensor(0.2659), tensor(0.2676), tensor(0.2651), tensor(0.2699), tensor(0.2639), tensor(0.2704), tensor(0.2661), tensor(0.2719), tensor(0.2690), tensor(0.2742), tensor(0.2723), tensor(0.2732), tensor(0.2704), tensor(0.2767), tensor(0.2699), tensor(0.2787), tensor(0.2733), tensor(0.2783), tensor(0.2749), tensor(0.2743), tensor(0.2715), tensor(0.2729), tensor(0.2736), tensor(0.2775), tensor(0.2749), tensor(0.2783), tensor(0.2754), tensor(0.2786), tensor(0.2756), tensor(0.2769), tensor(0.2753), tensor(0.2778), tensor(0.2778), tensor(0.2778), tensor(0.2814), tensor(0.2789), tensor(0.2776), tensor(0.2769), tensor(0.2778), tensor(0.2827), tensor(0.2805), tensor(0.2810), tensor(0.2828), tensor(0.2812), tensor(0.2842), tensor(0.2844), tensor(0.2832), tensor(0.2817), tensor(0.2830), tensor(0.2844), tensor(0.2865), tensor(0.2872), tensor(0.2830), tensor(0.2841), tensor(0.2844), tensor(0.2877), tensor(0.2883), tensor(0.2871), tensor(0.2845), tensor(0.2871), tensor(0.2865), tensor(0.2860), tensor(0.2876), tensor(0.2847), tensor(0.2861), tensor(0.2878), tensor(0.2841), tensor(0.2933), tensor(0.2889), tensor(0.2911), tensor(0.2898), tensor(0.2926), tensor(0.2909), tensor(0.2938), tensor(0.2913), tensor(0.2911), tensor(0.2870), tensor(0.2877), tensor(0.2878), tensor(0.2878), tensor(0.2845), tensor(0.2860), tensor(0.2830), tensor(0.2827), tensor(0.2823), tensor(0.2875), tensor(0.2880), tensor(0.2908), tensor(0.2917), tensor(0.2886), tensor(0.2933), tensor(0.2893), tensor(0.2920), tensor(0.2894), tensor(0.2911), tensor(0.2903), tensor(0.2900), tensor(0.2887), tensor(0.2941), tensor(0.2906), tensor(0.2938), tensor(0.2926), tensor(0.2892), tensor(0.2935), tensor(0.2925), tensor(0.2898), tensor(0.2935), tensor(0.2939), tensor(0.2938), tensor(0.2944), tensor(0.2948), tensor(0.2952), tensor(0.2939), tensor(0.2958), tensor(0.2974), tensor(0.2939), tensor(0.2935), tensor(0.2933), tensor(0.2958), tensor(0.2969), tensor(0.2950), tensor(0.2954), tensor(0.2938), tensor(0.2943), tensor(0.2927), tensor(0.2911), tensor(0.2957), tensor(0.2936), tensor(0.2960), tensor(0.2957), tensor(0.2964), tensor(0.2938), tensor(0.2955), tensor(0.2944), tensor(0.2948), tensor(0.2980), tensor(0.2968), tensor(0.2966), tensor(0.2964), tensor(0.2958), tensor(0.2941), tensor(0.2931), tensor(0.2936), tensor(0.2935), tensor(0.2902), tensor(0.2932), tensor(0.2927), tensor(0.2932), tensor(0.2931), tensor(0.2939), tensor(0.2897), tensor(0.2927), tensor(0.2888), tensor(0.2968), tensor(0.2948), tensor(0.2958), tensor(0.2961), tensor(0.2944), tensor(0.2970), tensor(0.2961), tensor(0.2943), tensor(0.2983), tensor(0.3011), tensor(0.3005), tensor(0.2994), tensor(0.2976), tensor(0.3003), tensor(0.2987), tensor(0.3024), tensor(0.2976), tensor(0.2988), tensor(0.3000), tensor(0.3013), tensor(0.2994), tensor(0.3015), tensor(0.3008), tensor(0.2993), tensor(0.3003), tensor(0.2993), tensor(0.3007), tensor(0.2955), tensor(0.2958), tensor(0.2965), tensor(0.3005), tensor(0.3019), tensor(0.2999), tensor(0.3008), tensor(0.3027), tensor(0.3008), tensor(0.3013), tensor(0.2972), tensor(0.2998), tensor(0.2983), tensor(0.3013), tensor(0.3011), tensor(0.2979), tensor(0.3021), tensor(0.3010), tensor(0.2996), tensor(0.3048), tensor(0.3002), tensor(0.3011), tensor(0.3015), tensor(0.3014), tensor(0.3013), tensor(0.3036), tensor(0.2992), tensor(0.3019), tensor(0.3025), tensor(0.3040), tensor(0.3044), tensor(0.3024), tensor(0.3032), tensor(0.3027), tensor(0.3031), tensor(0.3013), tensor(0.3033), tensor(0.3048), tensor(0.3013), tensor(0.3015), tensor(0.3029), tensor(0.3052), tensor(0.3047), tensor(0.3024), tensor(0.3068), tensor(0.3051), tensor(0.3027), tensor(0.3047), tensor(0.3057), tensor(0.3069), tensor(0.3080), tensor(0.3057), tensor(0.3040), tensor(0.3044), tensor(0.3081), tensor(0.3064), tensor(0.3059), tensor(0.3036), tensor(0.3075), tensor(0.3042), tensor(0.3057), tensor(0.2999), tensor(0.3049), tensor(0.3076), tensor(0.3046), tensor(0.3073), tensor(0.3030), tensor(0.3036), tensor(0.3036), tensor(0.3040), tensor(0.3060), tensor(0.3044), tensor(0.3076), tensor(0.3083), tensor(0.3087), tensor(0.3052), tensor(0.3083), tensor(0.3101), tensor(0.3079), tensor(0.3064), tensor(0.3112), tensor(0.3054), tensor(0.3091), tensor(0.3066), tensor(0.3083), tensor(0.3080), tensor(0.3102), tensor(0.3097), tensor(0.3077), tensor(0.3104), tensor(0.3093), tensor(0.3119), tensor(0.3092), tensor(0.3079), tensor(0.3094), tensor(0.3055), tensor(0.3109), tensor(0.3102), tensor(0.3086), tensor(0.3083), tensor(0.3109), tensor(0.3098), tensor(0.3104), tensor(0.3047), tensor(0.3076), tensor(0.3082), tensor(0.3099), tensor(0.3086), tensor(0.3060), tensor(0.3069), tensor(0.3060), tensor(0.3083), tensor(0.3054), tensor(0.3087), tensor(0.3048), tensor(0.3000), tensor(0.3032), tensor(0.3065), tensor(0.3063), tensor(0.3046), tensor(0.3071), tensor(0.3088), tensor(0.3081), tensor(0.3068), tensor(0.3090), tensor(0.3121), tensor(0.3059), tensor(0.3101), tensor(0.3030), tensor(0.3066), tensor(0.3054), tensor(0.3057), tensor(0.3093), tensor(0.3065), tensor(0.3086), tensor(0.3037), tensor(0.3049), tensor(0.3040), tensor(0.3074), tensor(0.3049), tensor(0.3047), tensor(0.3079), tensor(0.3071), tensor(0.3052), tensor(0.3080), tensor(0.3098), tensor(0.3071), tensor(0.3049), tensor(0.3082), tensor(0.3033), tensor(0.3069), tensor(0.3098), tensor(0.3093), tensor(0.3105), tensor(0.3071), tensor(0.3090), tensor(0.3079), tensor(0.3094), tensor(0.3069), tensor(0.3059), tensor(0.3086), tensor(0.3064), tensor(0.3096), tensor(0.3096), tensor(0.3109), tensor(0.3118), tensor(0.3124), tensor(0.3094), tensor(0.3159), tensor(0.3156), tensor(0.3159), tensor(0.3123), tensor(0.3162), tensor(0.3115), tensor(0.3152), tensor(0.3134), tensor(0.3140), tensor(0.3140), tensor(0.3145), tensor(0.3152), tensor(0.3125), tensor(0.3127), tensor(0.3136), tensor(0.3114), tensor(0.3129), tensor(0.3148), tensor(0.3171), tensor(0.3116), tensor(0.3146), tensor(0.3141), tensor(0.3151), tensor(0.3131), tensor(0.3138), tensor(0.3119), tensor(0.3090), tensor(0.3120), tensor(0.3134), tensor(0.3124), tensor(0.3135), tensor(0.3134), tensor(0.3140), tensor(0.3145), tensor(0.3148), tensor(0.3097), tensor(0.3121), tensor(0.3141), tensor(0.3142), tensor(0.3146), tensor(0.3131), tensor(0.3153), tensor(0.3140), tensor(0.3157), tensor(0.3148), tensor(0.3138), tensor(0.3159), tensor(0.3143), tensor(0.3126), tensor(0.3121), tensor(0.3163), tensor(0.3169), tensor(0.3170), tensor(0.3123), tensor(0.3134), tensor(0.3134), tensor(0.3130), tensor(0.3120), tensor(0.3148), tensor(0.3098), tensor(0.3103), tensor(0.3136), tensor(0.3118), tensor(0.3152), tensor(0.3102), tensor(0.3181), tensor(0.3149), tensor(0.3173), tensor(0.3142), tensor(0.3163), tensor(0.3127), tensor(0.3164), tensor(0.3185), tensor(0.3171), tensor(0.3184), tensor(0.3146), tensor(0.3123), tensor(0.3138), tensor(0.3165), tensor(0.3158), tensor(0.3149), tensor(0.3143), tensor(0.3157), tensor(0.3149), tensor(0.3159), tensor(0.3149), tensor(0.3169), tensor(0.3171), tensor(0.3119), tensor(0.3126), tensor(0.3107), tensor(0.3143), tensor(0.3143), tensor(0.3168), tensor(0.3120), tensor(0.3159), tensor(0.3126), tensor(0.3159), tensor(0.3118), tensor(0.3114), tensor(0.3156), tensor(0.3123), tensor(0.3113), tensor(0.3125), tensor(0.3115), tensor(0.3153), tensor(0.3145), tensor(0.3137), tensor(0.3137), tensor(0.3145), tensor(0.3101), tensor(0.3157), tensor(0.3157), tensor(0.3149), tensor(0.3137), tensor(0.3167), tensor(0.3142), tensor(0.3138), tensor(0.3141), tensor(0.3152), tensor(0.3126), tensor(0.3121), tensor(0.3103), tensor(0.3101), tensor(0.3082), tensor(0.3069), tensor(0.3075), tensor(0.3088), tensor(0.3088), tensor(0.3123), tensor(0.3075), tensor(0.3137), tensor(0.3131), tensor(0.3131), tensor(0.3131), tensor(0.3145), tensor(0.3105), tensor(0.3129), tensor(0.3116), tensor(0.3134), tensor(0.3096), tensor(0.3153), tensor(0.3135), tensor(0.3145), tensor(0.3137), tensor(0.3165), tensor(0.3187), tensor(0.3147), tensor(0.3165), tensor(0.3141), tensor(0.3131), tensor(0.3136), tensor(0.3146), tensor(0.3136), tensor(0.3114), tensor(0.3119), tensor(0.3130), tensor(0.3132), tensor(0.3142), tensor(0.3113), tensor(0.3169), tensor(0.3085), tensor(0.3170), tensor(0.3119), tensor(0.3146), tensor(0.3112), tensor(0.3160), tensor(0.3171), tensor(0.3153), tensor(0.3140), tensor(0.3135), tensor(0.3152), tensor(0.3173), tensor(0.3162), tensor(0.3169), tensor(0.3124), tensor(0.3140), tensor(0.3179), tensor(0.3158), tensor(0.3157), tensor(0.3156), tensor(0.3164), tensor(0.3167), tensor(0.3118), tensor(0.3188), tensor(0.3127), tensor(0.3145), tensor(0.3135), tensor(0.3147), tensor(0.3151), tensor(0.3175), tensor(0.3186), tensor(0.3163), tensor(0.3153), tensor(0.3143), tensor(0.3167), tensor(0.3167), tensor(0.3142), tensor(0.3174), tensor(0.3152), tensor(0.3147), tensor(0.3170), tensor(0.3157), tensor(0.3169), tensor(0.3185), tensor(0.3208), tensor(0.3176), tensor(0.3196), tensor(0.3209), tensor(0.3148), tensor(0.3181), tensor(0.3184), tensor(0.3201), tensor(0.3173), tensor(0.3180), tensor(0.3152), tensor(0.3192), tensor(0.3153), tensor(0.3145), tensor(0.3121), tensor(0.3160), tensor(0.3179), tensor(0.3190), tensor(0.3165), tensor(0.3153), tensor(0.3127), tensor(0.3175), tensor(0.3119), tensor(0.3114), tensor(0.3162), tensor(0.3158), tensor(0.3126), tensor(0.3159), tensor(0.3134), tensor(0.3151), tensor(0.3158), tensor(0.3121), tensor(0.3159), tensor(0.3101), tensor(0.3193), tensor(0.3182), tensor(0.3165), tensor(0.3164), tensor(0.3207), tensor(0.3191), tensor(0.3182), tensor(0.3201), tensor(0.3177), tensor(0.3229), tensor(0.3162), tensor(0.3190), tensor(0.3179), tensor(0.3134), tensor(0.3138), tensor(0.3203), tensor(0.3164), tensor(0.3181), tensor(0.3191), tensor(0.3225), tensor(0.3180), tensor(0.3186), tensor(0.3151), tensor(0.3197), tensor(0.3175), tensor(0.3184), tensor(0.3186), tensor(0.3209), tensor(0.3153), tensor(0.3184), tensor(0.3186), tensor(0.3192), tensor(0.3191), tensor(0.3202), tensor(0.3162), tensor(0.3192), tensor(0.3184), tensor(0.3182), tensor(0.3188), tensor(0.3195), tensor(0.3181), tensor(0.3168), tensor(0.3160), tensor(0.3191), tensor(0.3168), tensor(0.3186), tensor(0.3195), tensor(0.3160), tensor(0.3151), tensor(0.3156), tensor(0.3180), tensor(0.3156), tensor(0.3181), tensor(0.3145), tensor(0.3116), tensor(0.3170), tensor(0.3157), tensor(0.3149), tensor(0.3148), tensor(0.3149), tensor(0.3129), tensor(0.3202), tensor(0.3174), tensor(0.3151), tensor(0.3179), tensor(0.3201), tensor(0.3190), tensor(0.3214), tensor(0.3223), tensor(0.3232), tensor(0.3228), tensor(0.3201), tensor(0.3195), tensor(0.3221), tensor(0.3207), tensor(0.3208), tensor(0.3206), tensor(0.3175), tensor(0.3199), tensor(0.3199), tensor(0.3215), tensor(0.3206), tensor(0.3225), tensor(0.3187), tensor(0.3230), tensor(0.3176), tensor(0.3228), tensor(0.3209), tensor(0.3204), tensor(0.3226), tensor(0.3192), tensor(0.3198), tensor(0.3186), tensor(0.3173), tensor(0.3168), tensor(0.3184), tensor(0.3148), tensor(0.3185), tensor(0.3215), tensor(0.3201), tensor(0.3226), tensor(0.3196), tensor(0.3219), tensor(0.3199), tensor(0.3201), tensor(0.3212), tensor(0.3239), tensor(0.3217), tensor(0.3207), tensor(0.3204), tensor(0.3207), tensor(0.3207), tensor(0.3197), tensor(0.3196), tensor(0.3225), tensor(0.3208), tensor(0.3210), tensor(0.3207), tensor(0.3199), tensor(0.3186), tensor(0.3180), tensor(0.3175), tensor(0.3235), tensor(0.3223), tensor(0.3198), tensor(0.3204), tensor(0.3176), tensor(0.3201), tensor(0.3179), tensor(0.3191), tensor(0.3158), tensor(0.3141), tensor(0.3148), tensor(0.3129), tensor(0.3140), tensor(0.3167), tensor(0.3145), tensor(0.3124), tensor(0.3140), tensor(0.3229), tensor(0.3197), tensor(0.3214), tensor(0.3207), tensor(0.3187), tensor(0.3198), tensor(0.3203), tensor(0.3202), tensor(0.3192), tensor(0.3186), tensor(0.3191), tensor(0.3229), tensor(0.3196), tensor(0.3201), tensor(0.3213), tensor(0.3206), tensor(0.3257), tensor(0.3246), tensor(0.3198), tensor(0.3203), tensor(0.3210), tensor(0.3191), tensor(0.3212), tensor(0.3174), tensor(0.3246), tensor(0.3226), tensor(0.3247), tensor(0.3230), tensor(0.3204), tensor(0.3208), tensor(0.3223), tensor(0.3252), tensor(0.3221), tensor(0.3231), tensor(0.3237), tensor(0.3199), tensor(0.3260), tensor(0.3267), tensor(0.3215), tensor(0.3257), tensor(0.3243), tensor(0.3273), tensor(0.3241), tensor(0.3225), tensor(0.3270), tensor(0.3280), tensor(0.3271), tensor(0.3237), tensor(0.3209), tensor(0.3257), tensor(0.3207), tensor(0.3212), tensor(0.3188), tensor(0.3256), tensor(0.3182), tensor(0.3234), tensor(0.3243), tensor(0.3263), tensor(0.3243), tensor(0.3241), tensor(0.3192), tensor(0.3231), tensor(0.3245), tensor(0.3240), tensor(0.3269), tensor(0.3231), tensor(0.3252), tensor(0.3248), tensor(0.3271), tensor(0.3257), tensor(0.3250), tensor(0.3258), tensor(0.3210), tensor(0.3253), tensor(0.3246), tensor(0.3213), tensor(0.3219), tensor(0.3207), tensor(0.3218), tensor(0.3195), tensor(0.3241), tensor(0.3239), tensor(0.3248), tensor(0.3214), tensor(0.3241), tensor(0.3241), tensor(0.3246), tensor(0.3229), tensor(0.3197), tensor(0.3201), tensor(0.3217), tensor(0.3224), tensor(0.3208), tensor(0.3241), tensor(0.3204), tensor(0.3215), tensor(0.3208), tensor(0.3218), tensor(0.3217), tensor(0.3223), tensor(0.3204), tensor(0.3232), tensor(0.3182), tensor(0.3239), tensor(0.3250), tensor(0.3265), tensor(0.3268), tensor(0.3242), tensor(0.3223), tensor(0.3247), tensor(0.3254), tensor(0.3254), tensor(0.3231), tensor(0.3270), tensor(0.3258), tensor(0.3252), tensor(0.3240), tensor(0.3232), tensor(0.3240), tensor(0.3276), tensor(0.3242), tensor(0.3286), tensor(0.3251), tensor(0.3248), tensor(0.3240), tensor(0.3245), tensor(0.3254), tensor(0.3251), tensor(0.3265), tensor(0.3265), tensor(0.3258), tensor(0.3252), tensor(0.3231), tensor(0.3267), tensor(0.3240), tensor(0.3247), tensor(0.3207), tensor(0.3295), tensor(0.3265), tensor(0.3296), tensor(0.3257), tensor(0.3281), tensor(0.3276), tensor(0.3295), tensor(0.3280), tensor(0.3259), tensor(0.3270), tensor(0.3259), tensor(0.3292), tensor(0.3256), tensor(0.3239), tensor(0.3268), tensor(0.3258), tensor(0.3242), tensor(0.3251), tensor(0.3259), tensor(0.3270), tensor(0.3278), tensor(0.3230), tensor(0.3252), tensor(0.3242), tensor(0.3285), tensor(0.3278), tensor(0.3264), tensor(0.3282), tensor(0.3251), tensor(0.3278), tensor(0.3280), tensor(0.3286), tensor(0.3279), tensor(0.3289), tensor(0.3275), tensor(0.3221), tensor(0.3260), tensor(0.3264), tensor(0.3302), tensor(0.3278), tensor(0.3300), tensor(0.3318), tensor(0.3276), tensor(0.3280), tensor(0.3290), tensor(0.3274), tensor(0.3304), tensor(0.3293), tensor(0.3276), tensor(0.3269), tensor(0.3252), tensor(0.3275), tensor(0.3273), tensor(0.3267), tensor(0.3284), tensor(0.3268), tensor(0.3276), tensor(0.3271), tensor(0.3242), tensor(0.3220), tensor(0.3219), tensor(0.3263), tensor(0.3250), tensor(0.3236), tensor(0.3285), tensor(0.3268), tensor(0.3308), tensor(0.3271), tensor(0.3252), tensor(0.3263), tensor(0.3251), tensor(0.3307), tensor(0.3263), tensor(0.3291), tensor(0.3282), tensor(0.3279), tensor(0.3319), tensor(0.3304), tensor(0.3306), tensor(0.3301), tensor(0.3297), tensor(0.3284), tensor(0.3290), tensor(0.3301), tensor(0.3232), tensor(0.3224), tensor(0.3269), tensor(0.3248), tensor(0.3292), tensor(0.3236), tensor(0.3268), tensor(0.3274), tensor(0.3278), tensor(0.3280), tensor(0.3262), tensor(0.3270), tensor(0.3268), tensor(0.3278), tensor(0.3242), tensor(0.3273), tensor(0.3243), tensor(0.3267), tensor(0.3297), tensor(0.3256), tensor(0.3313), tensor(0.3276), tensor(0.3279), tensor(0.3280), tensor(0.3281), tensor(0.3318), tensor(0.3280), tensor(0.3320), tensor(0.3269), tensor(0.3298), tensor(0.3269), tensor(0.3264), tensor(0.3243), tensor(0.3240), tensor(0.3262), tensor(0.3250), tensor(0.3270), tensor(0.3263), tensor(0.3287), tensor(0.3324), tensor(0.3301), tensor(0.3286), tensor(0.3282), tensor(0.3300), tensor(0.3289), tensor(0.3292), tensor(0.3291), tensor(0.3291), tensor(0.3304), tensor(0.3268), tensor(0.3304), tensor(0.3254), tensor(0.3291), tensor(0.3240), tensor(0.3289), tensor(0.3275), tensor(0.3287), tensor(0.3293), tensor(0.3264), tensor(0.3278), tensor(0.3250), tensor(0.3239), tensor(0.3229), tensor(0.3267), tensor(0.3263), tensor(0.3292), tensor(0.3236), tensor(0.3287), tensor(0.3287), tensor(0.3258), tensor(0.3334), tensor(0.3298), tensor(0.3328), tensor(0.3281), tensor(0.3296), tensor(0.3290), tensor(0.3309), tensor(0.3269), tensor(0.3267), tensor(0.3259), tensor(0.3278), tensor(0.3307), tensor(0.3275), tensor(0.3322), tensor(0.3292), tensor(0.3304), tensor(0.3286), tensor(0.3269), tensor(0.3289), tensor(0.3282), tensor(0.3254), tensor(0.3273), tensor(0.3267), tensor(0.3231), tensor(0.3274), tensor(0.3263), tensor(0.3273), tensor(0.3240), tensor(0.3273), tensor(0.3264), tensor(0.3240), tensor(0.3278), tensor(0.3292), tensor(0.3259), tensor(0.3276), tensor(0.3284), tensor(0.3268), tensor(0.3290), tensor(0.3275), tensor(0.3306), tensor(0.3271), tensor(0.3285), tensor(0.3304), tensor(0.3303), tensor(0.3242), tensor(0.3241), tensor(0.3285), tensor(0.3297), tensor(0.3241), tensor(0.3246), tensor(0.3258), tensor(0.3274), tensor(0.3291), tensor(0.3289), tensor(0.3296), tensor(0.3297), tensor(0.3252), tensor(0.3287), tensor(0.3276), tensor(0.3291), tensor(0.3254), tensor(0.3254), tensor(0.3303), tensor(0.3308), tensor(0.3284), tensor(0.3270), tensor(0.3268), tensor(0.3243), tensor(0.3236), tensor(0.3252), tensor(0.3251), tensor(0.3268), tensor(0.3270), tensor(0.3260), tensor(0.3276), tensor(0.3267), tensor(0.3274), tensor(0.3285), tensor(0.3271), tensor(0.3293), tensor(0.3273), tensor(0.3258), tensor(0.3319), tensor(0.3304), tensor(0.3296), tensor(0.3309), tensor(0.3311), tensor(0.3323), tensor(0.3287), tensor(0.3278), tensor(0.3309), tensor(0.3254), tensor(0.3315), tensor(0.3282), tensor(0.3356), tensor(0.3279), tensor(0.3295), tensor(0.3320), tensor(0.3336), tensor(0.3317), tensor(0.3301), tensor(0.3337), tensor(0.3314), tensor(0.3364), tensor(0.3315), tensor(0.3326), tensor(0.3354), tensor(0.3386), tensor(0.3329), tensor(0.3325), tensor(0.3367), tensor(0.3359), tensor(0.3350), tensor(0.3322), tensor(0.3353), tensor(0.3352), tensor(0.3320), tensor(0.3298), tensor(0.3324), tensor(0.3323), tensor(0.3315), tensor(0.3273), tensor(0.3331), tensor(0.3297), tensor(0.3317), tensor(0.3334), tensor(0.3344), tensor(0.3361), tensor(0.3344), tensor(0.3319), tensor(0.3289), tensor(0.3326), tensor(0.3292), tensor(0.3302), tensor(0.3337), tensor(0.3293), tensor(0.3267), tensor(0.3297), tensor(0.3292), tensor(0.3293), tensor(0.3250), tensor(0.3267), tensor(0.3328), tensor(0.3330), tensor(0.3286), tensor(0.3246), tensor(0.3279), tensor(0.3296), tensor(0.3286), tensor(0.3308), tensor(0.3329), tensor(0.3352), tensor(0.3325), tensor(0.3336), tensor(0.3345), tensor(0.3323), tensor(0.3340), tensor(0.3295), tensor(0.3246), tensor(0.3286), tensor(0.3292), tensor(0.3303), tensor(0.3322), tensor(0.3315), tensor(0.3345), tensor(0.3323), tensor(0.3314), tensor(0.3304), tensor(0.3295), tensor(0.3290), tensor(0.3341), tensor(0.3241), tensor(0.3279), tensor(0.3219), tensor(0.3274), tensor(0.3239), tensor(0.3228), tensor(0.3284), tensor(0.3231), tensor(0.3263), tensor(0.3276), tensor(0.3297), tensor(0.3286), tensor(0.3306), tensor(0.3280), tensor(0.3291), tensor(0.3285), tensor(0.3306), tensor(0.3322), tensor(0.3340), tensor(0.3320), tensor(0.3347), tensor(0.3314), tensor(0.3345), tensor(0.3336), tensor(0.3363), tensor(0.3342), tensor(0.3348), tensor(0.3317), tensor(0.3337), tensor(0.3333), tensor(0.3331), tensor(0.3348), tensor(0.3279), tensor(0.3298), tensor(0.3298), tensor(0.3333), tensor(0.3344), tensor(0.3313), tensor(0.3334), tensor(0.3324), tensor(0.3312), tensor(0.3329), tensor(0.3318), tensor(0.3334), tensor(0.3314), tensor(0.3342), tensor(0.3341), tensor(0.3319), tensor(0.3318), tensor(0.3315), tensor(0.3329), tensor(0.3296), tensor(0.3330), tensor(0.3304), tensor(0.3311), tensor(0.3301), tensor(0.3281), tensor(0.3276), tensor(0.3276), tensor(0.3311), tensor(0.3308), tensor(0.3346), tensor(0.3334), tensor(0.3331), tensor(0.3326), tensor(0.3342), tensor(0.3347), tensor(0.3345), tensor(0.3323), tensor(0.3308), tensor(0.3364), tensor(0.3314), tensor(0.3311), tensor(0.3323), tensor(0.3322), tensor(0.3350), tensor(0.3309), tensor(0.3309), tensor(0.3308), tensor(0.3295), tensor(0.3313), tensor(0.3322), tensor(0.3317), tensor(0.3291), tensor(0.3263), tensor(0.3214), tensor(0.3262), tensor(0.3245), tensor(0.3293), tensor(0.3262), tensor(0.3260), tensor(0.3275), tensor(0.3318), tensor(0.3291), tensor(0.3350), tensor(0.3323), tensor(0.3295), tensor(0.3346), tensor(0.3285), tensor(0.3374), tensor(0.3341), tensor(0.3342), tensor(0.3335), tensor(0.3328), tensor(0.3336), tensor(0.3325), tensor(0.3312), tensor(0.3340), tensor(0.3320), tensor(0.3352), tensor(0.3264), tensor(0.3295), tensor(0.3311), tensor(0.3329), tensor(0.3335), tensor(0.3319), tensor(0.3334), tensor(0.3330), tensor(0.3345), tensor(0.3317), tensor(0.3342), tensor(0.3291), tensor(0.3365), tensor(0.3365), tensor(0.3328), tensor(0.3341), tensor(0.3368), tensor(0.3353), tensor(0.3344), tensor(0.3381), tensor(0.3367), tensor(0.3314), tensor(0.3339), tensor(0.3317), tensor(0.3336), tensor(0.3284), tensor(0.3340), tensor(0.3317), tensor(0.3348), tensor(0.3356), tensor(0.3344), tensor(0.3337), tensor(0.3326), tensor(0.3248), tensor(0.3264), tensor(0.3275), tensor(0.3241), tensor(0.3290), tensor(0.3318), tensor(0.3295), tensor(0.3341), tensor(0.3313), tensor(0.3353), tensor(0.3326), tensor(0.3369), tensor(0.3323), tensor(0.3357), tensor(0.3378), tensor(0.3344), tensor(0.3350), tensor(0.3346), tensor(0.3352), tensor(0.3357), tensor(0.3356), tensor(0.3344), tensor(0.3331), tensor(0.3290), tensor(0.3361), tensor(0.3319), tensor(0.3346), tensor(0.3376), tensor(0.3357), tensor(0.3351), tensor(0.3357), tensor(0.3326), tensor(0.3357), tensor(0.3303), tensor(0.3298), tensor(0.3301), tensor(0.3300), tensor(0.3295), tensor(0.3297), tensor(0.3319), tensor(0.3318), tensor(0.3329), tensor(0.3304), tensor(0.3329), tensor(0.3320), tensor(0.3292), tensor(0.3302), tensor(0.3300), tensor(0.3286), tensor(0.3333), tensor(0.3308), tensor(0.3286), tensor(0.3313), tensor(0.3287), tensor(0.3323), tensor(0.3318), tensor(0.3314), tensor(0.3317), tensor(0.3318), tensor(0.3319), tensor(0.3363), tensor(0.3306), tensor(0.3340), tensor(0.3354), tensor(0.3354), tensor(0.3314), tensor(0.3319), tensor(0.3317), tensor(0.3331), tensor(0.3334), tensor(0.3348), tensor(0.3335), tensor(0.3334), tensor(0.3375), tensor(0.3345), tensor(0.3339), tensor(0.3330), tensor(0.3298), tensor(0.3339), tensor(0.3286), tensor(0.3302), tensor(0.3369), tensor(0.3353), tensor(0.3322), tensor(0.3352), tensor(0.3362), tensor(0.3317), tensor(0.3359), tensor(0.3350), tensor(0.3346), tensor(0.3368), tensor(0.3363), tensor(0.3370), tensor(0.3367), tensor(0.3341), tensor(0.3342), tensor(0.3344), tensor(0.3364), tensor(0.3364), tensor(0.3358), tensor(0.3356), tensor(0.3337), tensor(0.3370), tensor(0.3400), tensor(0.3340), tensor(0.3330), tensor(0.3342), tensor(0.3353), tensor(0.3351), tensor(0.3347), tensor(0.3351), tensor(0.3374), tensor(0.3357), tensor(0.3325), tensor(0.3354), tensor(0.3337), tensor(0.3352), tensor(0.3334), tensor(0.3328), tensor(0.3330), tensor(0.3369), tensor(0.3369), tensor(0.3406), tensor(0.3412), tensor(0.3351), tensor(0.3375), tensor(0.3396), tensor(0.3375), tensor(0.3330), tensor(0.3378), tensor(0.3369), tensor(0.3395), tensor(0.3392), tensor(0.3374), tensor(0.3386), tensor(0.3361), tensor(0.3401), tensor(0.3396), tensor(0.3324), tensor(0.3353), tensor(0.3364), tensor(0.3353), tensor(0.3344), tensor(0.3347), tensor(0.3315), tensor(0.3318), tensor(0.3325), tensor(0.3363), tensor(0.3350), tensor(0.3344), tensor(0.3370), tensor(0.3365), tensor(0.3378), tensor(0.3362), tensor(0.3364), tensor(0.3367), tensor(0.3364), tensor(0.3350), tensor(0.3363), tensor(0.3330), tensor(0.3340), tensor(0.3300), tensor(0.3329), tensor(0.3341), tensor(0.3358), tensor(0.3333), tensor(0.3346), tensor(0.3346), tensor(0.3342), tensor(0.3330), tensor(0.3318), tensor(0.3326), tensor(0.3347), tensor(0.3336), tensor(0.3368), tensor(0.3333), tensor(0.3373), tensor(0.3353), tensor(0.3370), tensor(0.3409), tensor(0.3390), tensor(0.3387), tensor(0.3411), tensor(0.3365), tensor(0.3352), tensor(0.3342), tensor(0.3326), tensor(0.3363), tensor(0.3372), tensor(0.3367), tensor(0.3373), tensor(0.3369), tensor(0.3367), tensor(0.3387), tensor(0.3428), tensor(0.3390), tensor(0.3375), tensor(0.3376), tensor(0.3392), tensor(0.3381), tensor(0.3372), tensor(0.3411), tensor(0.3435), tensor(0.3412), tensor(0.3434), tensor(0.3428), tensor(0.3435), tensor(0.3416), tensor(0.3383), tensor(0.3412), tensor(0.3356), tensor(0.3331), tensor(0.3328), tensor(0.3348), tensor(0.3290), tensor(0.3373), tensor(0.3365), tensor(0.3362), tensor(0.3342), tensor(0.3351), tensor(0.3378), tensor(0.3357), tensor(0.3386), tensor(0.3375), tensor(0.3380), tensor(0.3381), tensor(0.3383), tensor(0.3402), tensor(0.3412), tensor(0.3363), tensor(0.3367), tensor(0.3356), tensor(0.3372), tensor(0.3381), tensor(0.3409), tensor(0.3390), tensor(0.3386), tensor(0.3356), tensor(0.3369), tensor(0.3419), tensor(0.3407), tensor(0.3380), tensor(0.3379), tensor(0.3340), tensor(0.3350), tensor(0.3373), tensor(0.3387), tensor(0.3352), tensor(0.3368), tensor(0.3379), tensor(0.3387), tensor(0.3372), tensor(0.3378), tensor(0.3364), tensor(0.3358), tensor(0.3363), tensor(0.3357), tensor(0.3390), tensor(0.3373), tensor(0.3395), tensor(0.3365), tensor(0.3383), tensor(0.3352), tensor(0.3409), tensor(0.3353), tensor(0.3364), tensor(0.3373), tensor(0.3402), tensor(0.3374), tensor(0.3348), tensor(0.3370), tensor(0.3406), tensor(0.3340), tensor(0.3367), tensor(0.3356), tensor(0.3361), tensor(0.3407), tensor(0.3408), tensor(0.3397), tensor(0.3341), tensor(0.3379), tensor(0.3383), tensor(0.3345), tensor(0.3376), tensor(0.3370), tensor(0.3381), tensor(0.3409), tensor(0.3400), tensor(0.3395), tensor(0.3409), tensor(0.3374), tensor(0.3367), tensor(0.3398), tensor(0.3402), tensor(0.3395), tensor(0.3370), tensor(0.3402), tensor(0.3353), tensor(0.3402), tensor(0.3400), tensor(0.3335), tensor(0.3361), tensor(0.3361), tensor(0.3341), tensor(0.3331), tensor(0.3336), tensor(0.3313), tensor(0.3333), tensor(0.3344), tensor(0.3350), tensor(0.3379), tensor(0.3356), tensor(0.3396), tensor(0.3389), tensor(0.3354), tensor(0.3339), tensor(0.3370), tensor(0.3368), tensor(0.3374), tensor(0.3402), tensor(0.3364), tensor(0.3375), tensor(0.3365), tensor(0.3381), tensor(0.3381), tensor(0.3335), tensor(0.3391), tensor(0.3409), tensor(0.3389), tensor(0.3378), tensor(0.3376), tensor(0.3341), tensor(0.3384), tensor(0.3372), tensor(0.3375), tensor(0.3369), tensor(0.3380), tensor(0.3379), tensor(0.3378), tensor(0.3367), tensor(0.3381), tensor(0.3391), tensor(0.3361), tensor(0.3395), tensor(0.3383), tensor(0.3398), tensor(0.3376), tensor(0.3386), tensor(0.3383), tensor(0.3348), tensor(0.3326), tensor(0.3340), tensor(0.3330), tensor(0.3341), tensor(0.3375), tensor(0.3368), tensor(0.3359), tensor(0.3375), tensor(0.3369), tensor(0.3394), tensor(0.3408), tensor(0.3372), tensor(0.3380), tensor(0.3394), tensor(0.3395), tensor(0.3353), tensor(0.3359), tensor(0.3392), tensor(0.3386), tensor(0.3351), tensor(0.3406), tensor(0.3376), tensor(0.3362), tensor(0.3363), tensor(0.3367), tensor(0.3405), tensor(0.3406), tensor(0.3400), tensor(0.3396), tensor(0.3394), tensor(0.3406), tensor(0.3444), tensor(0.3351), tensor(0.3395), tensor(0.3389), tensor(0.3375), tensor(0.3356), tensor(0.3322), tensor(0.3383), tensor(0.3385), tensor(0.3307), tensor(0.3383), tensor(0.3387), tensor(0.3387), tensor(0.3391), tensor(0.3413), tensor(0.3403), tensor(0.3392), tensor(0.3402), tensor(0.3420), tensor(0.3403), tensor(0.3412), tensor(0.3413), tensor(0.3409), tensor(0.3419), tensor(0.3391), tensor(0.3405), tensor(0.3405), tensor(0.3396), tensor(0.3405), tensor(0.3370), tensor(0.3420), tensor(0.3365), tensor(0.3376), tensor(0.3392), tensor(0.3396), tensor(0.3378), tensor(0.3394), tensor(0.3361), tensor(0.3345), tensor(0.3357), tensor(0.3344), tensor(0.3346), tensor(0.3362), tensor(0.3368), tensor(0.3412), tensor(0.3378), tensor(0.3398), tensor(0.3427), tensor(0.3385), tensor(0.3409), tensor(0.3398), tensor(0.3367), tensor(0.3373), tensor(0.3405), tensor(0.3408), tensor(0.3405), tensor(0.3401), tensor(0.3395), tensor(0.3406), tensor(0.3380), tensor(0.3378), tensor(0.3347), tensor(0.3376), tensor(0.3339), tensor(0.3333), tensor(0.3373), tensor(0.3379), tensor(0.3440), tensor(0.3422), tensor(0.3439), tensor(0.3418), tensor(0.3409), tensor(0.3446), tensor(0.3394), tensor(0.3396), tensor(0.3391), tensor(0.3383), tensor(0.3406), tensor(0.3398), tensor(0.3328), tensor(0.3374), tensor(0.3315), tensor(0.3354), tensor(0.3362), tensor(0.3381), tensor(0.3354), tensor(0.3394), tensor(0.3413), tensor(0.3348), tensor(0.3364), tensor(0.3357), tensor(0.3359), tensor(0.3370), tensor(0.3401), tensor(0.3402), tensor(0.3416), tensor(0.3402), tensor(0.3376), tensor(0.3335), tensor(0.3352), tensor(0.3365), tensor(0.3331), tensor(0.3353), tensor(0.3383), tensor(0.3356), tensor(0.3385), tensor(0.3356), tensor(0.3390), tensor(0.3417), tensor(0.3412), tensor(0.3408), tensor(0.3383), tensor(0.3420), tensor(0.3433), tensor(0.3424), tensor(0.3431), tensor(0.3442), tensor(0.3411), tensor(0.3422), tensor(0.3436), tensor(0.3423), tensor(0.3406), tensor(0.3412), tensor(0.3411), tensor(0.3425), tensor(0.3385), tensor(0.3348), tensor(0.3307), tensor(0.3350), tensor(0.3329), tensor(0.3320), tensor(0.3329), tensor(0.3379), tensor(0.3384), tensor(0.3381), tensor(0.3389), tensor(0.3386), tensor(0.3400), tensor(0.3412), tensor(0.3430), tensor(0.3416), tensor(0.3420), tensor(0.3420), tensor(0.3440), tensor(0.3368), tensor(0.3409), tensor(0.3384), tensor(0.3376), tensor(0.3398), tensor(0.3402), tensor(0.3368), tensor(0.3401), tensor(0.3408), tensor(0.3379), tensor(0.3392), tensor(0.3420), tensor(0.3380), tensor(0.3395), tensor(0.3385), tensor(0.3428), tensor(0.3400), tensor(0.3403), tensor(0.3441), tensor(0.3385), tensor(0.3441), tensor(0.3405), tensor(0.3392), tensor(0.3411), tensor(0.3417), tensor(0.3390), tensor(0.3391), tensor(0.3384), tensor(0.3383), tensor(0.3392), tensor(0.3373), tensor(0.3397), tensor(0.3386), tensor(0.3381), tensor(0.3398), tensor(0.3413), tensor(0.3374), tensor(0.3395), tensor(0.3359), tensor(0.3374), tensor(0.3391), tensor(0.3362), tensor(0.3383), tensor(0.3413), tensor(0.3413), tensor(0.3397), tensor(0.3392), tensor(0.3392), tensor(0.3427), tensor(0.3390), tensor(0.3397), tensor(0.3392), tensor(0.3400), tensor(0.3411), tensor(0.3407), tensor(0.3430), tensor(0.3405), tensor(0.3398), tensor(0.3389), tensor(0.3381), tensor(0.3398), tensor(0.3429), tensor(0.3416), tensor(0.3397), tensor(0.3441), tensor(0.3368), tensor(0.3401), tensor(0.3385), tensor(0.3351), tensor(0.3400), tensor(0.3374), tensor(0.3362), tensor(0.3384), tensor(0.3402), tensor(0.3405), tensor(0.3413), tensor(0.3380), tensor(0.3401), tensor(0.3394), tensor(0.3376), tensor(0.3370), tensor(0.3414), tensor(0.3365), tensor(0.3383), tensor(0.3379), tensor(0.3385), tensor(0.3364), tensor(0.3389), tensor(0.3403), tensor(0.3386), tensor(0.3401), tensor(0.3379), tensor(0.3392), tensor(0.3395), tensor(0.3389), tensor(0.3336), tensor(0.3373), tensor(0.3373), tensor(0.3323), tensor(0.3376), tensor(0.3344), tensor(0.3358), tensor(0.3386), tensor(0.3396), tensor(0.3392), tensor(0.3422), tensor(0.3405), tensor(0.3433), tensor(0.3419), tensor(0.3433), tensor(0.3376), tensor(0.3419), tensor(0.3395), tensor(0.3407)], val_accuracies=[tensor(0.1172), tensor(0.1250), tensor(0.1510), tensor(0.1562), tensor(0.1589), tensor(0.1432), tensor(0.1458), tensor(0.1510), tensor(0.1510), tensor(0.1562), tensor(0.1562), tensor(0.1615), tensor(0.1693), tensor(0.1667), tensor(0.1849), tensor(0.1823), tensor(0.1875), tensor(0.1927), tensor(0.1875), tensor(0.1849), tensor(0.1901), tensor(0.1979), tensor(0.2005), tensor(0.1849), tensor(0.1901), tensor(0.1823), tensor(0.1979), tensor(0.1875), tensor(0.2005), tensor(0.1901), tensor(0.2005), tensor(0.2031), tensor(0.2031), tensor(0.2005), tensor(0.1901), tensor(0.1979), tensor(0.1849), tensor(0.2005), tensor(0.1953), tensor(0.2031), tensor(0.2031), tensor(0.2005), tensor(0.2005), tensor(0.2057), tensor(0.1901), tensor(0.2057), tensor(0.1875), tensor(0.2161), tensor(0.2083), tensor(0.1927), tensor(0.1979), tensor(0.1849), tensor(0.1979), tensor(0.2057), tensor(0.1979), tensor(0.2135), tensor(0.2057), tensor(0.2031), tensor(0.2031), tensor(0.2083), tensor(0.1927), tensor(0.2083), tensor(0.1953), tensor(0.2057), tensor(0.2005), tensor(0.2161), tensor(0.2161), tensor(0.1979), tensor(0.2083), tensor(0.1953), tensor(0.2083), tensor(0.1953), tensor(0.2135), tensor(0.1953), tensor(0.2161), tensor(0.1979), tensor(0.2109), tensor(0.1979), tensor(0.2031), tensor(0.1979), tensor(0.2135), tensor(0.1979), tensor(0.2109), tensor(0.1927), tensor(0.2135), tensor(0.1979), tensor(0.2083), tensor(0.1927), tensor(0.2188), tensor(0.1979), tensor(0.2161), tensor(0.2031), tensor(0.2031), tensor(0.2031), tensor(0.1979), tensor(0.2031), tensor(0.2135), tensor(0.2057), tensor(0.2109), tensor(0.2109), tensor(0.2214), tensor(0.2109), tensor(0.2109), tensor(0.2109), tensor(0.2005), tensor(0.2031), tensor(0.2031), tensor(0.2005), tensor(0.1927), tensor(0.2083), tensor(0.1979), tensor(0.1901), tensor(0.2031), tensor(0.2083), tensor(0.1979), tensor(0.2031), tensor(0.1979), tensor(0.1953), tensor(0.2109), tensor(0.1901), tensor(0.2057), tensor(0.2005), tensor(0.2083), tensor(0.2057), tensor(0.2057), tensor(0.1927), tensor(0.1901), tensor(0.1953), tensor(0.1875), tensor(0.2057), tensor(0.1927), tensor(0.2031), tensor(0.1927), tensor(0.1901), tensor(0.2005), tensor(0.2057), tensor(0.1953), tensor(0.1927), tensor(0.1849), tensor(0.1901), tensor(0.2005), tensor(0.1953), tensor(0.1927), tensor(0.1797), tensor(0.2005), tensor(0.1901), tensor(0.1979), tensor(0.1927), tensor(0.1953), tensor(0.1901), tensor(0.1927), tensor(0.1927), tensor(0.2031), tensor(0.1979), tensor(0.2005), tensor(0.1875), tensor(0.2005), tensor(0.1979), tensor(0.2135), tensor(0.1849), tensor(0.2005), tensor(0.1823), tensor(0.2005), tensor(0.1979), tensor(0.1953), tensor(0.2005), tensor(0.2031), tensor(0.1901), tensor(0.1953), tensor(0.1953), tensor(0.2031), tensor(0.1953), tensor(0.2031), tensor(0.1953), tensor(0.2031), tensor(0.1901), tensor(0.1953), tensor(0.2005), tensor(0.1901), tensor(0.1849), tensor(0.1849), tensor(0.1823), tensor(0.1797), tensor(0.1823), tensor(0.1953), tensor(0.1849), tensor(0.1953), tensor(0.1901), tensor(0.1927), tensor(0.1849), tensor(0.1927), tensor(0.1901), tensor(0.1849), tensor(0.1953), tensor(0.1875), tensor(0.1927), tensor(0.1797), tensor(0.1901), tensor(0.1953), tensor(0.1953), tensor(0.1953), tensor(0.1901), tensor(0.1979), tensor(0.2005), tensor(0.1849), tensor(0.2005), tensor(0.1953), tensor(0.1953), tensor(0.2083), tensor(0.1927), tensor(0.2005), tensor(0.2005), tensor(0.1797), tensor(0.1953), tensor(0.1901), tensor(0.1953), tensor(0.2005), tensor(0.1953), tensor(0.1953), tensor(0.1953), tensor(0.1927), tensor(0.1927), tensor(0.1927), tensor(0.1953), tensor(0.2005), tensor(0.2005), tensor(0.1953), tensor(0.1979), tensor(0.1901), tensor(0.1927), tensor(0.2083), tensor(0.1849), tensor(0.2057), tensor(0.1849), tensor(0.2057), tensor(0.1823), tensor(0.1927), tensor(0.1979), tensor(0.2161), tensor(0.2031), tensor(0.1953), tensor(0.2031), tensor(0.1901), tensor(0.1849), tensor(0.1901), tensor(0.1901), tensor(0.2005), tensor(0.1927), tensor(0.1953), tensor(0.2031), tensor(0.1979), tensor(0.1875), tensor(0.2031), tensor(0.1927), tensor(0.1953), tensor(0.1953), tensor(0.1979), tensor(0.2057), tensor(0.2005), tensor(0.1875), tensor(0.1979), tensor(0.1953), tensor(0.1953), tensor(0.1953), tensor(0.2109), tensor(0.1875), tensor(0.2031), tensor(0.1953), tensor(0.2031), tensor(0.2109), tensor(0.1979), tensor(0.2005), tensor(0.1927), tensor(0.1953), tensor(0.2031), tensor(0.1927), tensor(0.2031), tensor(0.1953), tensor(0.2083), tensor(0.2057), tensor(0.1849), tensor(0.1901), tensor(0.1927), tensor(0.1979), tensor(0.1927), tensor(0.1953), tensor(0.2031), tensor(0.2005), tensor(0.1979), tensor(0.1927), tensor(0.1979), tensor(0.2005), tensor(0.2109), tensor(0.1953), tensor(0.1901), tensor(0.2031), tensor(0.2031), tensor(0.1927), tensor(0.1927), tensor(0.1849), tensor(0.2005), tensor(0.1953), tensor(0.1979), tensor(0.2005), tensor(0.2031), tensor(0.1927), tensor(0.1875), tensor(0.2031), tensor(0.2005), tensor(0.1953), tensor(0.2005), tensor(0.1953), tensor(0.1953), tensor(0.1953), tensor(0.1979), tensor(0.1849), tensor(0.1979), tensor(0.1901), tensor(0.1953), tensor(0.1927), tensor(0.1875), tensor(0.1797), tensor(0.1849), tensor(0.1823), tensor(0.1927), tensor(0.1953), tensor(0.1849), tensor(0.1953), tensor(0.1901), tensor(0.1875), tensor(0.1901), tensor(0.1875), tensor(0.1875), tensor(0.1927), tensor(0.1953), tensor(0.1979), tensor(0.1901), tensor(0.1979), tensor(0.1849), tensor(0.1927), tensor(0.1927), tensor(0.1875), tensor(0.1901), tensor(0.1927), tensor(0.1901), tensor(0.2031), tensor(0.1849), tensor(0.1927), tensor(0.1875), tensor(0.1797), tensor(0.1875), tensor(0.1823), tensor(0.1901), tensor(0.1901), tensor(0.1953), tensor(0.1953), tensor(0.1927), tensor(0.1797), tensor(0.1875), tensor(0.1849), tensor(0.1979), tensor(0.1979), tensor(0.1849), tensor(0.1875), tensor(0.1953), tensor(0.1927), tensor(0.1823), tensor(0.1901), tensor(0.1849), tensor(0.1849), tensor(0.2005), tensor(0.1771), tensor(0.1979), tensor(0.1901), tensor(0.1953), tensor(0.1979), tensor(0.1875), tensor(0.1953), tensor(0.1901), tensor(0.1979), tensor(0.1823), tensor(0.1823), tensor(0.1927), tensor(0.1875), tensor(0.1953), tensor(0.1979), tensor(0.1901), tensor(0.1875), tensor(0.1823), tensor(0.1823), tensor(0.1849), tensor(0.1823), tensor(0.1901), tensor(0.1901), tensor(0.1745), tensor(0.1823), tensor(0.1953), tensor(0.1927), tensor(0.1875), tensor(0.1797), tensor(0.1849), tensor(0.1849), tensor(0.1797), tensor(0.1979), tensor(0.1771), tensor(0.2031), tensor(0.1771), tensor(0.1953), tensor(0.1849), tensor(0.1953), tensor(0.2005), tensor(0.1875), tensor(0.1953), tensor(0.1849), tensor(0.1927), tensor(0.1979), tensor(0.1901), tensor(0.2031), tensor(0.1953), tensor(0.2005), tensor(0.1979), tensor(0.1849), tensor(0.1901), tensor(0.1849), tensor(0.1875), tensor(0.1979), tensor(0.1953), tensor(0.1901), tensor(0.2057), tensor(0.1953), tensor(0.1849), tensor(0.1901), tensor(0.1901), tensor(0.1927), tensor(0.1901), tensor(0.2005), tensor(0.1797), tensor(0.1823), tensor(0.1875), tensor(0.1875), tensor(0.1979), tensor(0.1797), tensor(0.1875), tensor(0.1953), tensor(0.1849), tensor(0.1901), tensor(0.1927), tensor(0.1927), tensor(0.1979), tensor(0.1797), tensor(0.2031), tensor(0.1927), tensor(0.1901), tensor(0.1823), tensor(0.1953), tensor(0.1797), tensor(0.2005), tensor(0.1823), tensor(0.1953), tensor(0.1849), tensor(0.2057), tensor(0.1953), tensor(0.1927), tensor(0.1875), tensor(0.1875), tensor(0.1901), tensor(0.2005), tensor(0.1953), tensor(0.1927), tensor(0.1927), tensor(0.1875), tensor(0.1797), tensor(0.1875), tensor(0.1875), tensor(0.1849), tensor(0.1875), tensor(0.1745), tensor(0.1875), tensor(0.1927), tensor(0.1823), tensor(0.1797), tensor(0.2031), tensor(0.1927), tensor(0.1797), tensor(0.1849), tensor(0.2005), tensor(0.1901), tensor(0.1901), tensor(0.1875), tensor(0.1979), tensor(0.1849), tensor(0.1901), tensor(0.1953), tensor(0.1849), tensor(0.1979), tensor(0.1979), tensor(0.1901), tensor(0.1823), tensor(0.1927), tensor(0.1745), tensor(0.1901), tensor(0.1849), tensor(0.1927), tensor(0.1875), tensor(0.1927), tensor(0.2005), tensor(0.2031), tensor(0.2031), tensor(0.1979), tensor(0.1901), tensor(0.1979), tensor(0.1953), tensor(0.1953), tensor(0.2005), tensor(0.1953), tensor(0.1927), tensor(0.1979), tensor(0.1901), tensor(0.1979), tensor(0.1953), tensor(0.2031), tensor(0.2031), tensor(0.1953), tensor(0.1875), tensor(0.2057), tensor(0.2109), tensor(0.2005), tensor(0.1927), tensor(0.1979), tensor(0.2057), tensor(0.1979), tensor(0.2031), tensor(0.2005), tensor(0.2031), tensor(0.1745), tensor(0.1979), tensor(0.1927), tensor(0.2057), tensor(0.1901), tensor(0.1927), tensor(0.1979), tensor(0.2031), tensor(0.1901), tensor(0.2005), tensor(0.1979), tensor(0.1901), tensor(0.1953), tensor(0.1823), tensor(0.1849), tensor(0.1927), tensor(0.1927), tensor(0.1927), tensor(0.1953), tensor(0.1927), tensor(0.1927), tensor(0.1875), tensor(0.1979), tensor(0.1953), tensor(0.1927), tensor(0.1979), tensor(0.1875), tensor(0.1979), tensor(0.1797), tensor(0.2005), tensor(0.1875), tensor(0.1979), tensor(0.1875), tensor(0.1901), tensor(0.1797), tensor(0.1953), tensor(0.1875), tensor(0.1901), tensor(0.1719), tensor(0.1927), tensor(0.1927), tensor(0.1797), tensor(0.1849), tensor(0.1719), tensor(0.1849), tensor(0.1849), tensor(0.1927), tensor(0.1979), tensor(0.1901), tensor(0.1901), tensor(0.1849), tensor(0.1797), tensor(0.1979), tensor(0.1719), tensor(0.1693), tensor(0.1927), tensor(0.1823), tensor(0.1875), tensor(0.1849), tensor(0.1927), tensor(0.1745), tensor(0.1979), tensor(0.1745), tensor(0.1875), tensor(0.1823), tensor(0.1875), tensor(0.1849), tensor(0.1901), tensor(0.1927), tensor(0.1849), tensor(0.2005), tensor(0.1927), tensor(0.1875), tensor(0.2005), tensor(0.1849), tensor(0.1771), tensor(0.1745), tensor(0.1823), tensor(0.1953), tensor(0.1927), tensor(0.1693), tensor(0.1953), tensor(0.2057), tensor(0.1927), tensor(0.1979), tensor(0.1797), tensor(0.1875), tensor(0.1823), tensor(0.1823), tensor(0.1823), tensor(0.1875), tensor(0.1875), tensor(0.1823), tensor(0.1771), tensor(0.1901), tensor(0.1901), tensor(0.1849), tensor(0.1771), tensor(0.1823), tensor(0.1849), tensor(0.2005), tensor(0.1823), tensor(0.1849), tensor(0.1849), tensor(0.1875), tensor(0.1901), tensor(0.1745), tensor(0.1797), tensor(0.1641), tensor(0.1797), tensor(0.1641), tensor(0.1797), tensor(0.1745), tensor(0.1927), tensor(0.1797), tensor(0.1745), tensor(0.1927), tensor(0.1771), tensor(0.1719), tensor(0.1719), tensor(0.1875), tensor(0.1875), tensor(0.1823), tensor(0.1875), tensor(0.1823), tensor(0.1875), tensor(0.1771), tensor(0.1745), tensor(0.1849), tensor(0.1771), tensor(0.2005), tensor(0.1849), tensor(0.1927), tensor(0.1693), tensor(0.1875), tensor(0.1797), tensor(0.1927), tensor(0.1719), tensor(0.1849), tensor(0.1927), tensor(0.1797), tensor(0.2031), tensor(0.1797), tensor(0.1745), tensor(0.1797), tensor(0.1927), tensor(0.1901), tensor(0.1771), tensor(0.1667), tensor(0.1771), tensor(0.1823), tensor(0.1719), tensor(0.1719), tensor(0.1901), tensor(0.1771), tensor(0.1719), tensor(0.1693), tensor(0.1771), tensor(0.1771), tensor(0.1797), tensor(0.1823), tensor(0.1719), tensor(0.1693), tensor(0.1745), tensor(0.1745), tensor(0.1745), tensor(0.1719), tensor(0.1771), tensor(0.1823), tensor(0.1771), tensor(0.1797), tensor(0.1797), tensor(0.1849), tensor(0.1927), tensor(0.1953), tensor(0.1953), tensor(0.1953), tensor(0.1797), tensor(0.1849), tensor(0.1719), tensor(0.1745), tensor(0.1771), tensor(0.1797), tensor(0.1745), tensor(0.1849), tensor(0.1849), tensor(0.1849), tensor(0.1823), tensor(0.1823), tensor(0.1901), tensor(0.1719), tensor(0.1875), tensor(0.1771), tensor(0.1901), tensor(0.1771), tensor(0.1875), tensor(0.1901), tensor(0.1797), tensor(0.1849), tensor(0.1927), tensor(0.1901), tensor(0.1797), tensor(0.1849), tensor(0.1771), tensor(0.1771), tensor(0.1823), tensor(0.1797), tensor(0.1745), tensor(0.1797), tensor(0.1719), tensor(0.1771), tensor(0.1797), tensor(0.1797), tensor(0.1745), tensor(0.1771), tensor(0.1797), tensor(0.1797), tensor(0.1745), tensor(0.1771), tensor(0.1797), tensor(0.1771), tensor(0.1823), tensor(0.1797), tensor(0.1979), tensor(0.1875), tensor(0.1745), tensor(0.1823), tensor(0.1927), tensor(0.1849), tensor(0.1953), tensor(0.1901), tensor(0.1823), tensor(0.1875), tensor(0.1901), tensor(0.1953), tensor(0.1953), tensor(0.2005), tensor(0.1771), tensor(0.1875), tensor(0.1901), tensor(0.1875), tensor(0.1797), tensor(0.1823), tensor(0.1797), tensor(0.1849), tensor(0.1823), tensor(0.1719), tensor(0.1875), tensor(0.1849), tensor(0.1927), tensor(0.1927), tensor(0.1849), tensor(0.1823), tensor(0.1901), tensor(0.1771), tensor(0.1875), tensor(0.1849), tensor(0.1823), tensor(0.1823), tensor(0.1823), tensor(0.1849), tensor(0.1953), tensor(0.1927), tensor(0.1849), tensor(0.1771), tensor(0.1823), tensor(0.1875), tensor(0.1823), tensor(0.1875), tensor(0.1719), tensor(0.1745), tensor(0.1849), tensor(0.1901), tensor(0.1927), tensor(0.1953), tensor(0.1875), tensor(0.1771), tensor(0.1849), tensor(0.1849), tensor(0.1823), tensor(0.1745), tensor(0.1823), tensor(0.1875), tensor(0.1797), tensor(0.1771), tensor(0.1797), tensor(0.1771), tensor(0.1901), tensor(0.1823), tensor(0.1901), tensor(0.1849), tensor(0.1901), tensor(0.1771), tensor(0.1953), tensor(0.1797), tensor(0.1953), tensor(0.1667), tensor(0.1849), tensor(0.1849), tensor(0.1927), tensor(0.1979), tensor(0.1953), tensor(0.1823), tensor(0.1823), tensor(0.1719), tensor(0.1927), tensor(0.1771), tensor(0.1823), tensor(0.1849), tensor(0.1797), tensor(0.1927), tensor(0.1771), tensor(0.1849), tensor(0.1771), tensor(0.1823), tensor(0.1823), tensor(0.1901), tensor(0.1875), tensor(0.1849), tensor(0.1849), tensor(0.1849), tensor(0.1797), tensor(0.1849), tensor(0.1875), tensor(0.1927), tensor(0.1849), tensor(0.1901), tensor(0.1875), tensor(0.1823), tensor(0.1901), tensor(0.1927), tensor(0.1849), tensor(0.1823), tensor(0.1823), tensor(0.1745), tensor(0.1771), tensor(0.1771), tensor(0.1927), tensor(0.1771), tensor(0.1797), tensor(0.1875), tensor(0.1745), tensor(0.1849), tensor(0.1797), tensor(0.1797), tensor(0.1849), tensor(0.1927), tensor(0.1927), tensor(0.1823), tensor(0.2005), tensor(0.1693), tensor(0.1875), tensor(0.1849), tensor(0.1823), tensor(0.1875), tensor(0.1797), tensor(0.1745), tensor(0.1927), tensor(0.1901), tensor(0.1849), tensor(0.1849), tensor(0.1823), tensor(0.1953), tensor(0.1901), tensor(0.1797), tensor(0.1849), tensor(0.1823), tensor(0.1901), tensor(0.1823), tensor(0.1797), tensor(0.1901), tensor(0.1901), tensor(0.1875), tensor(0.1849), tensor(0.1823), tensor(0.1849), tensor(0.1901), tensor(0.1927), tensor(0.1823), tensor(0.1953), tensor(0.1927), tensor(0.1875), tensor(0.1927), tensor(0.1901), tensor(0.1849), tensor(0.1797), tensor(0.1797), tensor(0.1953), tensor(0.1953), tensor(0.1927), tensor(0.1849), tensor(0.1927), tensor(0.1979), tensor(0.1875), tensor(0.1979), tensor(0.1901), tensor(0.1901), tensor(0.1797), tensor(0.1745), tensor(0.1823), tensor(0.1901), tensor(0.1771), tensor(0.1901), tensor(0.1771), tensor(0.1849), tensor(0.1927), tensor(0.2031), tensor(0.1901), tensor(0.1927), tensor(0.2057), tensor(0.1979), tensor(0.1953), tensor(0.2005), tensor(0.1849), tensor(0.1875), tensor(0.2005), tensor(0.1797), tensor(0.1875), tensor(0.1745), tensor(0.1823), tensor(0.1927), tensor(0.1719), tensor(0.1849), tensor(0.1901), tensor(0.1771), tensor(0.1797), tensor(0.1719), tensor(0.2005), tensor(0.1875), tensor(0.1953), tensor(0.1797), tensor(0.1849), tensor(0.1849), tensor(0.1979), tensor(0.1849), tensor(0.1875), tensor(0.1901), tensor(0.1745), tensor(0.1823), tensor(0.1875), tensor(0.1875), tensor(0.1797), tensor(0.1849), tensor(0.1927), tensor(0.1979), tensor(0.1901), tensor(0.1875), tensor(0.1849), tensor(0.1849), tensor(0.2005), tensor(0.1823), tensor(0.1901), tensor(0.1823), tensor(0.1849), tensor(0.1875), tensor(0.1901), tensor(0.1901), tensor(0.1797), tensor(0.1771), tensor(0.1745), tensor(0.1849), tensor(0.1771), tensor(0.1797), tensor(0.1719), tensor(0.1745), tensor(0.1875), tensor(0.1901), tensor(0.1979), tensor(0.1797), tensor(0.1875), tensor(0.1901), tensor(0.1875), tensor(0.1927), tensor(0.1849), tensor(0.1901), tensor(0.1901), tensor(0.1797), tensor(0.1797), tensor(0.2083), tensor(0.1823), tensor(0.1901), tensor(0.1797), tensor(0.2109), tensor(0.1927), tensor(0.1927), tensor(0.1901), tensor(0.1875), tensor(0.1953), tensor(0.1875), tensor(0.1927), tensor(0.1901), tensor(0.2005), tensor(0.1875), tensor(0.1849), tensor(0.1849), tensor(0.1849), tensor(0.1901), tensor(0.1927), tensor(0.1823), tensor(0.1849), tensor(0.1927), tensor(0.1875), tensor(0.2005), tensor(0.1953), tensor(0.1979), tensor(0.1901), tensor(0.2005), tensor(0.1927), tensor(0.2031), tensor(0.1875), tensor(0.1979), tensor(0.2005), tensor(0.1849), tensor(0.1953), tensor(0.1875), tensor(0.1771), tensor(0.1797), tensor(0.1901), tensor(0.1927), tensor(0.1875), tensor(0.1953), tensor(0.2031), tensor(0.1875), tensor(0.1901), tensor(0.1901), tensor(0.1979), tensor(0.1797), tensor(0.1875), tensor(0.1797), tensor(0.1797), tensor(0.1823), tensor(0.1849), tensor(0.2005), tensor(0.1771), tensor(0.1979), tensor(0.1927), tensor(0.1953), tensor(0.1927), tensor(0.1953), tensor(0.2005), tensor(0.1953), tensor(0.1901), tensor(0.1771), tensor(0.2005), tensor(0.1901), tensor(0.1901), tensor(0.2057), tensor(0.1927), tensor(0.2135), tensor(0.2005), tensor(0.1953), tensor(0.1927), tensor(0.1953), tensor(0.2005), tensor(0.2005), tensor(0.2031), tensor(0.1875), tensor(0.1927), tensor(0.1953), tensor(0.2031), tensor(0.1901), tensor(0.1927), tensor(0.1953), tensor(0.1979), tensor(0.2109), tensor(0.2005), tensor(0.2005), tensor(0.1901), tensor(0.2057), tensor(0.2005), tensor(0.1849), tensor(0.2005), tensor(0.2005), tensor(0.1927), tensor(0.1849), tensor(0.1953), tensor(0.1875), tensor(0.1979), tensor(0.1927), tensor(0.1849), tensor(0.1797), tensor(0.1771), tensor(0.1979), tensor(0.1797), tensor(0.1979), tensor(0.1875), tensor(0.1901), tensor(0.1927), tensor(0.1823), tensor(0.1979), tensor(0.1901), tensor(0.1875), tensor(0.2005), tensor(0.1901), tensor(0.1953), tensor(0.1927), tensor(0.2031), tensor(0.2005), tensor(0.1901), tensor(0.1875), tensor(0.1927), tensor(0.1901), tensor(0.1901), tensor(0.1875), tensor(0.1901), tensor(0.1953), tensor(0.1823), tensor(0.1849), tensor(0.1823), tensor(0.1849), tensor(0.1901), tensor(0.1823), tensor(0.1875), tensor(0.1823), tensor(0.1849), tensor(0.1953), tensor(0.1875), tensor(0.1875), tensor(0.1849), tensor(0.1901), tensor(0.1927), tensor(0.1875), tensor(0.1849), tensor(0.1927), tensor(0.2057), tensor(0.1849), tensor(0.1797), tensor(0.1979), tensor(0.1901), tensor(0.1901), tensor(0.1901), tensor(0.2005), tensor(0.1849), tensor(0.1823), tensor(0.1979), tensor(0.1901), tensor(0.1901), tensor(0.1901), tensor(0.1797), tensor(0.1875), tensor(0.1927), tensor(0.1797), tensor(0.1901), tensor(0.1875), tensor(0.1823), tensor(0.1849), tensor(0.1901), tensor(0.1797), tensor(0.1953), tensor(0.1901), tensor(0.1927), tensor(0.1875), tensor(0.1953), tensor(0.1849), tensor(0.1901), tensor(0.1797), tensor(0.1927), tensor(0.2083), tensor(0.1875), tensor(0.1823), tensor(0.1849), tensor(0.1927), tensor(0.1953), tensor(0.1849), tensor(0.1953), tensor(0.1875), tensor(0.1953), tensor(0.1797), tensor(0.1901), tensor(0.1849), tensor(0.1823), tensor(0.1849), tensor(0.1927), tensor(0.1823), tensor(0.1849), tensor(0.1823), tensor(0.1875), tensor(0.1979), tensor(0.2005), tensor(0.2031), tensor(0.1953), tensor(0.1875), tensor(0.1823), tensor(0.1927), tensor(0.1875), tensor(0.1927), tensor(0.1797), tensor(0.1901), tensor(0.1849), tensor(0.1745), tensor(0.1849), tensor(0.1849), tensor(0.1901), tensor(0.1901), tensor(0.1979), tensor(0.1875), tensor(0.1927), tensor(0.1875), tensor(0.1875), tensor(0.1771), tensor(0.1953), tensor(0.1901), tensor(0.1927), tensor(0.1849), tensor(0.1927), tensor(0.1927), tensor(0.1823), tensor(0.1875), tensor(0.1953), tensor(0.1849), tensor(0.1849), tensor(0.1823), tensor(0.1901), tensor(0.1901), tensor(0.1797), tensor(0.1901), tensor(0.1901), tensor(0.1875), tensor(0.1797), tensor(0.1823), tensor(0.1927), tensor(0.1823), tensor(0.1901), tensor(0.1849), tensor(0.1953), tensor(0.1901), tensor(0.1927), tensor(0.1927), tensor(0.2031), tensor(0.1875), tensor(0.1979), tensor(0.1901), tensor(0.1875), tensor(0.1875), tensor(0.1953), tensor(0.1927), tensor(0.2005), tensor(0.2005), tensor(0.1901), tensor(0.1901), tensor(0.1719), tensor(0.1875), tensor(0.1849), tensor(0.1953), tensor(0.1849), tensor(0.1797), tensor(0.1953), tensor(0.1875), tensor(0.1953), tensor(0.1875), tensor(0.1953), tensor(0.1927), tensor(0.2005), tensor(0.1771), tensor(0.2031), tensor(0.1849), tensor(0.1927), tensor(0.1849), tensor(0.1849), tensor(0.1797), tensor(0.1823), tensor(0.1771), tensor(0.1927), tensor(0.1875), tensor(0.1849), tensor(0.1901), tensor(0.2031), tensor(0.1823), tensor(0.1875), tensor(0.1979), tensor(0.1823), tensor(0.1823), tensor(0.1979), tensor(0.1953), tensor(0.1875), tensor(0.1797), tensor(0.1771), tensor(0.1901), tensor(0.1875), tensor(0.1901), tensor(0.1901), tensor(0.2031), tensor(0.2005), tensor(0.1927), tensor(0.1849), tensor(0.1979), tensor(0.1901), tensor(0.2005), tensor(0.1875), tensor(0.1901), tensor(0.1823), tensor(0.1823), tensor(0.1927), tensor(0.1979), tensor(0.2057), tensor(0.1901), tensor(0.2005), tensor(0.1849), tensor(0.2031), tensor(0.1953), tensor(0.1797), tensor(0.1849), tensor(0.1797), tensor(0.1901), tensor(0.1901), tensor(0.1901), tensor(0.1901), tensor(0.2005), tensor(0.1797), tensor(0.1849), tensor(0.1927), tensor(0.1927), tensor(0.2031), tensor(0.1901), tensor(0.1901), tensor(0.1927), tensor(0.1927), tensor(0.1927), tensor(0.1875), tensor(0.1901), tensor(0.1927), tensor(0.1797), tensor(0.1849), tensor(0.1979), tensor(0.1875), tensor(0.1927), tensor(0.1849), tensor(0.1797), tensor(0.1901), tensor(0.1927), tensor(0.1745), tensor(0.1979), tensor(0.1901), tensor(0.1927), tensor(0.1901), tensor(0.1849), tensor(0.2057), tensor(0.1927), tensor(0.2057), tensor(0.1927), tensor(0.1953), tensor(0.1953), tensor(0.1979), tensor(0.2057), tensor(0.1875), tensor(0.1901), tensor(0.1901), tensor(0.1823), tensor(0.1797), tensor(0.1927), tensor(0.1849), tensor(0.2031), tensor(0.1901), tensor(0.1927), tensor(0.1979), tensor(0.1953), tensor(0.1901), tensor(0.1927), tensor(0.1953), tensor(0.1823), tensor(0.1745), tensor(0.1823), tensor(0.2005), tensor(0.1953), tensor(0.1797), tensor(0.1771), tensor(0.1745), tensor(0.1797), tensor(0.1745), tensor(0.1953), tensor(0.1901), tensor(0.1979), tensor(0.1823), tensor(0.1823), tensor(0.1849), tensor(0.2005), tensor(0.1953), tensor(0.1927), tensor(0.1849), tensor(0.1849), tensor(0.1979), tensor(0.1901), tensor(0.1875), tensor(0.1875), tensor(0.1927), tensor(0.1823), tensor(0.1901), tensor(0.1875), tensor(0.1901), tensor(0.1797), tensor(0.1745), tensor(0.1875), tensor(0.1745), tensor(0.1823), tensor(0.1771), tensor(0.1797), tensor(0.1875), tensor(0.1823), tensor(0.1823), tensor(0.1693), tensor(0.1719), tensor(0.1745), tensor(0.1771), tensor(0.1875), tensor(0.1797), tensor(0.1745), tensor(0.1953), tensor(0.1771), tensor(0.1901), tensor(0.1875), tensor(0.1823), tensor(0.1745), tensor(0.1823), tensor(0.1823), tensor(0.1745), tensor(0.1953), tensor(0.1797), tensor(0.1797), tensor(0.1745), tensor(0.1823), tensor(0.1771), tensor(0.1771), tensor(0.1745), tensor(0.1797), tensor(0.1927), tensor(0.1771), tensor(0.1745), tensor(0.1823), tensor(0.1771), tensor(0.1927), tensor(0.1927), tensor(0.1615), tensor(0.1823), tensor(0.1823), tensor(0.1823), tensor(0.1771), tensor(0.1797), tensor(0.1901), tensor(0.1719), tensor(0.1693), tensor(0.1745), tensor(0.1719), tensor(0.1797), tensor(0.1901), tensor(0.1719), tensor(0.1745), tensor(0.1953), tensor(0.1823), tensor(0.1693), tensor(0.1797), tensor(0.1823), tensor(0.1745), tensor(0.1823), tensor(0.1693), tensor(0.1823), tensor(0.1693), tensor(0.1693), tensor(0.1771), tensor(0.1745), tensor(0.1823), tensor(0.1693), tensor(0.1771), tensor(0.1745), tensor(0.1667), tensor(0.1875), tensor(0.1823), tensor(0.1849), tensor(0.1823), tensor(0.1797), tensor(0.1771), tensor(0.1849), tensor(0.1719), tensor(0.1615), tensor(0.1667), tensor(0.1745), tensor(0.1771), tensor(0.1745), tensor(0.1823), tensor(0.1771), tensor(0.1797), tensor(0.1979), tensor(0.1823), tensor(0.1875), tensor(0.1849), tensor(0.1823), tensor(0.1875), tensor(0.1823), tensor(0.1771), tensor(0.1823), tensor(0.1823), tensor(0.1875), tensor(0.1875), tensor(0.1875), tensor(0.1849), tensor(0.1745), tensor(0.1771), tensor(0.1745), tensor(0.1875), tensor(0.1797), tensor(0.1797), tensor(0.1719), tensor(0.1849), tensor(0.1771), tensor(0.1927), tensor(0.1901), tensor(0.1849), tensor(0.1901), tensor(0.1927), tensor(0.1875), tensor(0.1875), tensor(0.1823), tensor(0.1875), tensor(0.1849), tensor(0.1823), tensor(0.1823), tensor(0.1901), tensor(0.1797), tensor(0.1797), tensor(0.1875), tensor(0.1953), tensor(0.1745), tensor(0.1901), tensor(0.1953), tensor(0.1823), tensor(0.1875), tensor(0.1901), tensor(0.1771), tensor(0.1849), tensor(0.1771), tensor(0.1849), tensor(0.1953), tensor(0.1771), tensor(0.1797), tensor(0.1823), tensor(0.1771), tensor(0.1875), tensor(0.1875), tensor(0.1953), tensor(0.1875), tensor(0.1901), tensor(0.1797), tensor(0.1953), tensor(0.1953), tensor(0.2031), tensor(0.1849), tensor(0.1927), tensor(0.1875), tensor(0.1875), tensor(0.1823), tensor(0.1927), tensor(0.1901), tensor(0.1927), tensor(0.1849), tensor(0.1901), tensor(0.1901), tensor(0.1849), tensor(0.1953), tensor(0.1849), tensor(0.1875), tensor(0.1875), tensor(0.1875), tensor(0.1875), tensor(0.1901), tensor(0.1901), tensor(0.1901), tensor(0.1875), tensor(0.1927), tensor(0.1875), tensor(0.1771), tensor(0.1953), tensor(0.1823), tensor(0.1771), tensor(0.1875), tensor(0.1901), tensor(0.1771), tensor(0.1745), tensor(0.1901), tensor(0.1875), tensor(0.1875), tensor(0.1771), tensor(0.1901), tensor(0.1849), tensor(0.1823), tensor(0.1849), tensor(0.1927), tensor(0.1797), tensor(0.1823), tensor(0.1927), tensor(0.1823), tensor(0.1823), tensor(0.1797), tensor(0.1745), tensor(0.1927), tensor(0.1823), tensor(0.1875), tensor(0.1771), tensor(0.1901), tensor(0.1901), tensor(0.1927), tensor(0.1797), tensor(0.1823), tensor(0.1953), tensor(0.1953), tensor(0.1927), tensor(0.1719), tensor(0.1927), tensor(0.1771), tensor(0.1953), tensor(0.1771), tensor(0.1823), tensor(0.1797), tensor(0.1901), tensor(0.1901), tensor(0.1797), tensor(0.1849), tensor(0.1693), tensor(0.1745), tensor(0.1875), tensor(0.1875), tensor(0.1927), tensor(0.1823), tensor(0.1849), tensor(0.1797), tensor(0.1875), tensor(0.1797), tensor(0.1823), tensor(0.1823), tensor(0.1849), tensor(0.1875), tensor(0.1719), tensor(0.1823), tensor(0.1875), tensor(0.1797), tensor(0.1875), tensor(0.1797), tensor(0.1771), tensor(0.1797), tensor(0.1849), tensor(0.1849), tensor(0.1849), tensor(0.1823), tensor(0.1771), tensor(0.1875), tensor(0.1797), tensor(0.1771), tensor(0.1797), tensor(0.1875), tensor(0.1823), tensor(0.1927), tensor(0.1745), tensor(0.1953), tensor(0.1823), tensor(0.1693), tensor(0.1901), tensor(0.1875), tensor(0.1823), tensor(0.1849), tensor(0.1823), tensor(0.1901), tensor(0.1849), tensor(0.1849), tensor(0.1849), tensor(0.1745), tensor(0.1849), tensor(0.1875), tensor(0.1901), tensor(0.1901), tensor(0.1927), tensor(0.1927), tensor(0.1771), tensor(0.1927), tensor(0.1849), tensor(0.1823), tensor(0.1797), tensor(0.1823), tensor(0.1771), tensor(0.1823), tensor(0.1849), tensor(0.1797), tensor(0.1823), tensor(0.1849), tensor(0.1797), tensor(0.1849), tensor(0.1849), tensor(0.1771), tensor(0.1927), tensor(0.1771), tensor(0.1823), tensor(0.1849), tensor(0.1849), tensor(0.1823), tensor(0.1771), tensor(0.1771), tensor(0.1823), tensor(0.1771), tensor(0.1797), tensor(0.1719), tensor(0.1693), tensor(0.1719), tensor(0.1771), tensor(0.1771), tensor(0.1875), tensor(0.1823), tensor(0.1823), tensor(0.1823), tensor(0.1719), tensor(0.1901), tensor(0.1875), tensor(0.1875), tensor(0.1823), tensor(0.1771), tensor(0.1797), tensor(0.1797), tensor(0.1771), tensor(0.1823), tensor(0.1719), tensor(0.1979), tensor(0.1823), tensor(0.1823), tensor(0.1615), tensor(0.1849), tensor(0.1719), tensor(0.1823), tensor(0.1823), tensor(0.1823), tensor(0.1823), tensor(0.1719), tensor(0.1641), tensor(0.1823), tensor(0.1719), tensor(0.1823), tensor(0.1797), tensor(0.1823), tensor(0.1771), tensor(0.1745), tensor(0.1823), tensor(0.1771), tensor(0.1667), tensor(0.1797), tensor(0.1849), tensor(0.1901), tensor(0.1849), tensor(0.1771), tensor(0.1797), tensor(0.1745), tensor(0.1823), tensor(0.1875), tensor(0.1797), tensor(0.1823), tensor(0.1771), tensor(0.1875), tensor(0.1901), tensor(0.1875), tensor(0.1927), tensor(0.1875), tensor(0.2031), tensor(0.1927), tensor(0.1979), tensor(0.1771), tensor(0.1849), tensor(0.1979), tensor(0.1849), tensor(0.1771), tensor(0.1849), tensor(0.1797), tensor(0.1901), tensor(0.1875), tensor(0.1901), tensor(0.1823), tensor(0.1875), tensor(0.1849), tensor(0.1823), tensor(0.1745), tensor(0.1927), tensor(0.1771), tensor(0.1927), tensor(0.1797), tensor(0.1875), tensor(0.1745), tensor(0.1823), tensor(0.1823), tensor(0.1771), tensor(0.1719), tensor(0.1875), tensor(0.1797), tensor(0.1745), tensor(0.1849), tensor(0.1797), tensor(0.1693), tensor(0.1901), tensor(0.1745), tensor(0.1849), tensor(0.1849), tensor(0.1901), tensor(0.1823), tensor(0.1797), tensor(0.1771), tensor(0.1953), tensor(0.1849), tensor(0.1745), tensor(0.1875), tensor(0.1849), tensor(0.1953), tensor(0.1875), tensor(0.1797), tensor(0.1901), tensor(0.1875), tensor(0.1771), tensor(0.1875), tensor(0.1927), tensor(0.1953), tensor(0.1693), tensor(0.1823), tensor(0.1797), tensor(0.1771), tensor(0.1745), tensor(0.1823), tensor(0.1849), tensor(0.1849), tensor(0.1849), tensor(0.1953), tensor(0.1849), tensor(0.1797), tensor(0.1771), tensor(0.1797), tensor(0.1719), tensor(0.1849), tensor(0.1875), tensor(0.1823), tensor(0.1693), tensor(0.1901), tensor(0.1797), tensor(0.1875), tensor(0.1849), tensor(0.1797), tensor(0.1641), tensor(0.1901), tensor(0.1953), tensor(0.1745), tensor(0.1875), tensor(0.1771), tensor(0.1979), tensor(0.1797), tensor(0.1849), tensor(0.1901), tensor(0.1719), tensor(0.1849), tensor(0.1771), tensor(0.1771), tensor(0.1927), tensor(0.1745), tensor(0.1901), tensor(0.1849), tensor(0.1797), tensor(0.1849), tensor(0.1797), tensor(0.1823), tensor(0.1823), tensor(0.1901), tensor(0.1901), tensor(0.1927), tensor(0.1979), tensor(0.1875), tensor(0.1901), tensor(0.1771), tensor(0.1823), tensor(0.1823), tensor(0.1901), tensor(0.1901), tensor(0.1849), tensor(0.1875), tensor(0.1849), tensor(0.1823), tensor(0.1797), tensor(0.1849), tensor(0.1823), tensor(0.1875), tensor(0.1719), tensor(0.1901), tensor(0.1719), tensor(0.1953), tensor(0.1797), tensor(0.1823), tensor(0.1849), tensor(0.1797), tensor(0.1797), tensor(0.1823), tensor(0.1875), tensor(0.1875), tensor(0.1719), tensor(0.1823), tensor(0.1771), tensor(0.1901), tensor(0.1849), tensor(0.1745), tensor(0.1771), tensor(0.1745), tensor(0.1771), tensor(0.1771), tensor(0.1875), tensor(0.1875), tensor(0.1771), tensor(0.1875), tensor(0.1849), tensor(0.1797), tensor(0.1875), tensor(0.1797), tensor(0.1927), tensor(0.1875), tensor(0.1901), tensor(0.1849), tensor(0.1771), tensor(0.1797), tensor(0.1823), tensor(0.1901), tensor(0.1797), tensor(0.2005), tensor(0.1797), tensor(0.1771), tensor(0.1797), tensor(0.1953), tensor(0.1927), tensor(0.1797), tensor(0.1849), tensor(0.1823), tensor(0.1823), tensor(0.1693), tensor(0.1797), tensor(0.1823), tensor(0.1953), tensor(0.1771), tensor(0.1797), tensor(0.1823), tensor(0.1901), tensor(0.1927), tensor(0.1901), tensor(0.1823), tensor(0.1901), tensor(0.1797), tensor(0.1927), tensor(0.1667), tensor(0.1875), tensor(0.1719), tensor(0.1901), tensor(0.1823), tensor(0.1849), tensor(0.1849), tensor(0.1771), tensor(0.1797), tensor(0.1823), tensor(0.1797), tensor(0.1823), tensor(0.1615), tensor(0.1797), tensor(0.1771), tensor(0.1875), tensor(0.1745), tensor(0.1849), tensor(0.1823), tensor(0.1823), tensor(0.1693), tensor(0.1901), tensor(0.1797), tensor(0.1875), tensor(0.1771), tensor(0.1849)])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training\n",
    "\n",
    "n_epochs = 2_000\n",
    "\n",
    "# Optimization\n",
    "lr = 1e-3\n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrainingHistory:\n",
    "    losses: list[float]\n",
    "    train_accuracies: list[float]\n",
    "    val_accuracies: list[float]\n",
    "\n",
    "def train_model(model: transformer_lens.HookedTransformer) -> TrainingHistory:\n",
    "    losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for i, (X, y) in enumerate(train_dataloader):\n",
    "            optim.zero_grad()\n",
    "\n",
    "            logits = model(X)\n",
    "            loss = loss_fn(logits, y)\n",
    "\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                losses.append(loss.item())\n",
    "                train_batch_acc = accuracy(logits, y)\n",
    "                train_accuracies.append(train_batch_acc.cpu())\n",
    "                val_acc = accuracy(model(X_val), y_val)\n",
    "                val_accuracies.append(val_acc.cpu())\n",
    "\n",
    "                print(f\"({epoch}) loss = {loss.item():.4f}, batch accuracy = {train_batch_acc}, val acc = {val_acc}\")\n",
    "\n",
    "    return TrainingHistory(losses, train_accuracies, val_accuracies)\n",
    "\n",
    "history = train_model(model)\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc92e924fd0>]"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAABN2klEQVR4nO3deVhU1f8H8PewDYusIquouC+ouWvmjmubLb+vmWWW2YalLWZaWVmJ7X3bbFXbTLNS+5pLbuCGmiYqLigIggqIKMMOA3N+fwxzmcvMwIDABeb9eh6eh7nLzLmMMm/O/ZxzVEIIASIiIiKF2CndACIiIrJtDCNERESkKIYRIiIiUhTDCBERESmKYYSIiIgUxTBCREREimIYISIiIkUxjBAREZGiHJRugDV0Oh0uX74Md3d3qFQqpZtDREREVhBCIDc3F0FBQbCzs9z/0STCyOXLlxESEqJ0M4iIiKgWUlNT0bp1a4v7m0QYcXd3B6C/GA8PD4VbQ0RERNbIyclBSEiI9DluSZMII4ZbMx4eHgwjRERETUx1JRYsYCUiIiJFMYwQERGRohhGiIiISFEMI0RERKQohhEiIiJSFMMIERERKYphhIiIiBTFMEJERESKYhghIiIiRTGMEBERkaIYRoiIiEhRDCNERESkqCaxUF59+e3IRcRd0mBCWAAGt2+pdHOIiIhskk33jESfzcTK/ck4dTlH6aYQERHZLJsOIwZC6QYQERHZMJsOIyqlG0BERES2HUaIiIhIeQwjAITgjRoiIiKl2HQYUfE+DRERkeJsOowQERGR8mw6jLBjhIiISHk2HUaIiIhIeQwjAFi/SkREpBybDiMqVrASEREpzqbDiIHgHKxERESKsekwwn4RIiIi5dl0GCEiIiLl2XYYKe8aYQErERGRcmw7jBAREZHiGEYAlq8SEREpyKbDiIolrERERIqz6TBCREREyrPpMKJiASsREZHibDqMEBERkfIYRsAZWImIiJRk02GE5atERETKs+kwQkRERMqz6TDCAlYiIiLl2XQYISIiIuUxjBAREZGibDqMcAZWIiIi5dl0GDEQLBohIiJSjE2HERU7RoiIiBRn02GEiIiIlGfTYYRDe4mIiJRn02GEiIiIlMcwAnBlGiIiIgXZeBhhBSsREZHSahRGIiMjMWDAALi7u8PPzw+TJ09GfHx8tedlZ2cjIiICgYGBUKvV6Ny5MzZt2lTrRhMREVHz4VCTg6OjoxEREYEBAwagtLQUCxcuxLhx43Dq1Cm4ubmZPaekpARjx46Fn58ffvvtNwQHB+PChQvw8vKqi/bfEBawEhERKa9GYWTLli2yxytXroSfnx+OHDmC4cOHmz1n+fLluHbtGvbv3w9HR0cAQLt27WrXWiIiImp2bqhmRKPRAAB8fHwsHvPnn39iyJAhiIiIgL+/P8LCwrBkyRKUlZVZPKe4uBg5OTmyr/okWMJKRESkmFqHEZ1Oh7lz52Lo0KEICwuzeNz58+fx22+/oaysDJs2bcKrr76KDz74AG+99ZbFcyIjI+Hp6Sl9hYSE1LaZVWL5KhERkfJqHUYiIiIQFxeH1atXV3mcTqeDn58fvv76a/Tr1w9TpkzByy+/jC+//NLiOQsWLIBGo5G+UlNTa9tMIiIiauRqVDNiMHv2bGzcuBG7d+9G69atqzw2MDAQjo6OsLe3l7Z169YN6enpKCkpgZOTk8k5arUaarW6Nk2rERawEhERKa9GPSNCCMyePRvr1q3Dzp07ERoaWu05Q4cORUJCAnQ6nbTt7NmzCAwMNBtEiIiIyLbUKIxERETgp59+wqpVq+Du7o709HSkp6ejsLBQOmb69OlYsGCB9PjJJ5/EtWvXMGfOHJw9exZ//fUXlixZgoiIiLq7ilpSlVeNsGOEiIhIOTW6TbNs2TIAwMiRI2XbV6xYgRkzZgAAUlJSYGdXkXFCQkKwdetWPPvss+jVqxeCg4MxZ84czJ8//8ZaTkRERM1CjcKIsKK4IioqymTbkCFDcODAgZq8FBEREdkIm16bxlDAygpWIiIi5dh0GCEiIiLl2XQYkTpGFG0FERGRbbPpMEJERETKYxghIiIiRdl0GFGVV7CyfpWIiEg5Nh1GiIiISHkMIwAES1iJiIgUwzBCREREimIYAWtGiIiIlGTTYUSagZWIiIgUY9NhhIiIiJRn02FEVT4HK+/SEBERKcemwwgREREpz6bDiKFmhAWsREREyrHpMEJERETKYxghIiIiRdl0GDGM7OUMrERERMqx6TBCREREyrPpMKKq6BohIiIihdh0GCEiIiLlMYwQERGRomw6jKhUnIGViIhIaTYdRoiIiEh5Nh1GpPpVTsFKRESkGJsOI0RERKQ82w4jquoPISIiovpl22GkHO/SEBERKYdhhIiIiBRl02FEBQ7tJSIiUppNhxEiIiJSnk2HERULWImIiBRn02HEgAWsREREymEYISIiIkXZdBiRZmBlCSsREZFibDqMEBERkfJsOoywgJWIiEh5Nh1GDFjASkREpByGESIiIlKUTYcRFVfKIyIiUpxNhxEiIiJSnk2HEUMBq2DRCBERkWJsOowQERGR8mw6jLBihIiISHk2HUYMeJOGiIhIOQwjREREpCjbDiPlFaysXyUiIlKObYcRIiIiUpxNhxEWsBIRESnPpsOIgWAJKxERkWIYRoiIiEhRNh1GKmZgVbYdREREtsymwwgREREpz6bDCFftJSIiUp5NhxED3qUhIiJSjk2HERU7RoiIiBRn02HEgAWsREREymEYISIiIkXZdBjhXRoiIiLl2XQYqcD7NEREREqx6TDCAlYiIiLl2XQYMWABKxERkXIYRoiIiEhRNh1GVLxPQ0REpDibDiMGvE1DRESkHIYRIiIiUhTDCADBob1ERESKqVEYiYyMxIABA+Du7g4/Pz9MnjwZ8fHxVp+/evVqqFQqTJ48uabtJCIiomaqRmEkOjoaEREROHDgALZt2watVotx48YhPz+/2nOTk5PxwgsvYNiwYbVubF1j/SoREZHyHGpy8JYtW2SPV65cCT8/Pxw5cgTDhw+3eF5ZWRmmTZuGN954A3v27EF2dnatGltfWMBKRESknBuqGdFoNAAAHx+fKo9bvHgx/Pz8MHPmTKuet7i4GDk5ObKv+qDi6jRERESKq3UY0el0mDt3LoYOHYqwsDCLx+3duxffffcdvvnmG6ufOzIyEp6entJXSEhIbZtpFXaMEBERKafWYSQiIgJxcXFYvXq1xWNyc3Px4IMP4ptvvoGvr6/Vz71gwQJoNBrpKzU1tbbNrBJrRoiIiJRXo5oRg9mzZ2Pjxo3YvXs3WrdubfG4xMREJCcn4/bbb5e26XQ6/Qs7OCA+Ph4dOnQwOU+tVkOtVtemabXCmhEiIiLl1CiMCCHw9NNPY926dYiKikJoaGiVx3ft2hUnTpyQbXvllVeQm5uL//73v/V++4WIiIgavxqFkYiICKxatQobNmyAu7s70tPTAQCenp5wcXEBAEyfPh3BwcGIjIyEs7OzST2Jl5cXAFRZZ9JQeJeGiIhIeTUKI8uWLQMAjBw5UrZ9xYoVmDFjBgAgJSUFdnZNa2JXzsBKRESknBrfpqlOVFRUlftXrlxZk5esVyxgJSIiUl7T6sKoL+wYISIiUgzDCBERESnKpsMIZ2AlIiJSnk2HEQPepSEiIlKOTYcRFrASEREpz6bDiIE1o4SIiIiofjCMEBERkaIYRoiIiEhRDCNgASsREZGSbDqMqFjBSkREpDibDiMGrF8lIiJSjk2HEfaLEBERKc+mwwgREREpj2EELGAlIiJSkk2HEdavEhERKc+mw4gBZ2AlIiJSjk2HEXaMEBERKc+mwwgREREpz6bDiJ2dvm9Ex9s0REREirHpMOJgp798bRnDCBERkVJsO4zY63tGynQMI0REREqx7TBSfptGW6ZTuCVERES2y7bDiL3+8kt5m4aIiEgxNh1GHMt7Rkp17BkhIiJSik2HEalnhDUjREREirHtMGLoGeFtGiIiIsXYdhixZwErERGR0mw7jNjxNg0REZHSbDqMOHKeESIiIsXZdBix5zwjREREirPpMOLIeUaIiIgUZ9NhxFDAynlGiIiIlGPbYUSa9Iw9I0REREqx8TDC2zRERERKs+0wwnlGiIiIFGfTYcSR08ETEREpzqbDiGFob5lOQAgGEiIiIiXYdBhxtKu4fPaOEBERKcOmw4ihZgRgESsREZFSGEbKaTnXCBERkSJsO4wY3aYpY88IERGRImw6jNjbqaAq7xxhzwgREZEybDqMABXDe7XsGSEiIlKEzYcRZwf9j6BIW6ZwS4iIiGyTzYcRFyd7AEBhCcMIERGREmw+jDg76sMIe0aIiIiUYfNhxEUKIyxgJSIiUoLNhxF1eRgpZM8IERGRImw+jLiV14zkFWsVbgkREZFtsvkw4ueuBgBcySlWuCVERES2yebDiL+HMwAgPadI4ZYQERHZJoaR8jDCnhEiIiJlMIyUh5EM9owQEREpwubDiLebIwAgu5AFrEREREqw+TDi4awPIzkMI0RERIqw+TDi6VIeRooYRoiIiJRg82HEozyMFGl1KC7lxGdEREQNzebDiLvaASqV/vucwlJlG0NERGSDbD6M2Nmp0ELtAIC3aoiIiJRg82EEYBErERGRkhhGYFzEyts0REREDY1hBICHS/ltGvaMEBERNTiGERjdpmHNCBERUYNjGEHF8F4Ne0aIiIgaHMMIjGpGOLSXiIiowTGMgLdpiIiIlFSjMBIZGYkBAwbA3d0dfn5+mDx5MuLj46s855tvvsGwYcPg7e0Nb29vhIeH49ChQzfU6LpmKGDlbRoiIqKGV6MwEh0djYiICBw4cADbtm2DVqvFuHHjkJ+fb/GcqKgoTJ06Fbt27UJMTAxCQkIwbtw4XLp06YYbX1cMk57lF/M2DRERUUNzqMnBW7ZskT1euXIl/Pz8cOTIEQwfPtzsOT///LPs8bfffovff/8dO3bswPTp02vY3Prh4mQPACgo4do0REREDe2GakY0Gg0AwMfHx+pzCgoKoNVqa3ROfXMtDyNFWoYRIiKihlajnhFjOp0Oc+fOxdChQxEWFmb1efPnz0dQUBDCw8MtHlNcXIzi4mLpcU5OTm2baRUXR/2PgT0jREREDa/WPSMRERGIi4vD6tWrrT5n6dKlWL16NdatWwdnZ2eLx0VGRsLT01P6CgkJqW0zrWLoGSlkGCEiImpwtQojs2fPxsaNG7Fr1y60bt3aqnPef/99LF26FH///Td69epV5bELFiyARqORvlJTU2vTTKu5SjUjLGAlIiJqaDW6TSOEwNNPP41169YhKioKoaGhVp337rvv4u2338bWrVvRv3//ao9Xq9VQq9U1adoNYQErERGRcmoURiIiIrBq1Sps2LAB7u7uSE9PBwB4enrCxcUFADB9+nQEBwcjMjISAPDOO+9g0aJFWLVqFdq1ayed06JFC7Ro0aIur6XWXJ30P4biUh3KdAL2diqFW0RERGQ7anSbZtmyZdBoNBg5ciQCAwOlrzVr1kjHpKSkIC0tTXZOSUkJ7r33Xtk577//ft1dxQ1ycbSXvi/kiBoiIqIGVePbNNWJioqSPU5OTq7JSyjC2dEOKhUghL6I1TAJGhEREdU/rk0DQKVSSb0jHFFDRETUsBhGykkjarQcUUNERNSQGEbKcUQNERGRMhhGyrmWz8LK2zREREQNi2GknDN7RoiIiBTBMFLO1ZGzsBIRESmBYaSch4v+Nk1OEcMIERFRQ2IYKefj5gQAuJZXonBLiIiIbAvDSDlDGLlewDBCRETUkBhGynm76sNIZm6xwi0hIiKyLQwj5dyd9TUjf51IQ2mZTuHWEBER2Q6GkXJe5T0jAJB6vVDBlhAREdkWhpFyY7v5S9+nXitQsCVERES2hWGknJ2dCv3begMA8os5vJeIiKihMIwYcVXr60byOQsrERFRg2EYMdJCrZ+FlT0jREREDYdhxIibk75nJI9hhIiIqMEwjBjxNszCms+Jz4iIiBoKw4iRAA9nAECahkN7iYiIGgrDiBFfdzUA4Hq+VuGWEBER2Q6GESNuTvoC1oIS1owQERE1FIYRIy7lYeTYRQ3OZ+Yp3BoiIiLbwDBixDCaBgB+OpCiYEuIiIhsB8OIBY72KqWbQEREZBMYRox0D/KQvi8u5cq9REREDYFhxIijvR0WTuoKANAUckQNERFRQ2AYqcTTxREAwwgREVFDYRipxNNFPwtruqZI4ZYQERHZBoaRSrxc9T0jp9JysHDdCYVbQ0RE1PwxjFRiuE0DAKsOcngvERFRfWMYqcTQM0JEREQNg2GkEq/ymhGDExc1CrWEiIjINjCMVGKYEt7g9s/2KtQSIiIi28AwYoWUrAKlm0BERNRsMYyY8ept3WWPc4o45wgREVF9YRgxI7ybn+yxisvUEBER1RuGETPa+LjKHguhUEOIiIhsAMOIGapKXSEFJWUKtYSIiKj5YxixwpsbTyndBCIiomaLYcQKJy5xrhEiIqL6wjBiQcyC0bLHOh0LR4iIiOoDw4gFgZ4ussfFpTqFWkJERNS8MYxUYerANtL31wpKFGwJERFR88UwUoV+bb2l74cu3YkNsZeQrilSsEVERETND8NIFe7qEyx7PGd1LCZ9skeh1hARETVPDCNVsLcznXr1Wj5v1xAREdUlhhEiIiJSFMNILVzIyle6CURERM0Gw0g19rw4ymSbppCr+BIREdUVhpFqtPZ2MdlWpNXhbEYurrN+hIiI6IY5KN2Axq7yonkA8J+vYgAAzo52OPPmxIZuEhERUbPCnpEbUKTlrKxEREQ3imHECn3aeCndBCIiomaLYcQKXz3Qz+K+PecyG7AlREREzQ/DiBX8PJxxYMEYzB7V0WTfg98dQgkX0SMiIqo1hhErBXg644XxXRA9b6TJvvWxlxq+QURERM0Ew0gNhXi7mmwzHuKbU6RF6rWChmwSERFRk8ahvTVkZ2a9mj3nrqJ7kAd2nL6CH2KSoRP6ydJCfFyh0wm8siEOXQPcMX1Iu4ZvMBERUSPHMFILj49oj6+iz0uP9yZcxd6Eq7JjYs5nIcTHFbvPZWLVwRQAYBghIiIyg7dpamHBxG44+cb4Ko8RQgAAMnKKTLYRERFRBYaRWnJTO2D7c8Mt7j+XkQcAKCwpk7ZxkjQiIiJTDCM3oKOfu8V9sanZAIB8ozCSV1xa300iIiJqchhG6snl7EIAQFZexUibfIYRIiIiEwwj9aRAq+8RycwrlraxZ4SIiMgUw8gNWv3YYEzpH2KyvaC4PIzkVhSwsmeEiIjIFIf23qDB7VticPuWeHFCF6zYl4xxPfxxx2f7UFKmQ0mpDsdSNdKx+SUMI0RERJWxZ6SOtGyhxgvju6BrgIe0bW9CJgq1FQWs1/K1eOLHI/jxwAUlmkhERNQo1SiMREZGYsCAAXB3d4efnx8mT56M+Pj4as9bu3YtunbtCmdnZ/Ts2RObNm2qdYMbOycHO3TyawEAeGTlYdm+F9Yew5aT6Xh1fZwSTSMiImqUahRGoqOjERERgQMHDmDbtm3QarUYN24c8vPzLZ6zf/9+TJ06FTNnzsTRo0cxefJkTJ48GXFxzfcDeVLPQKWbQERE1GSoxA1MC5qZmQk/Pz9ER0dj+HDzE4BNmTIF+fn52Lhxo7Rt8ODBuOmmm/Dll19a9To5OTnw9PSERqOBh4dH9Sco7NfDqXjxt+PS464B7jiTnis75vySSWbXuTGm0wl8vec8Bob6oG8b73ppKxERUX2x9vP7hmpGNBp9caaPj4/FY2JiYhAeHi7bNn78eMTExFg8p7i4GDk5ObKvpqRj+W0ag7v7BpscsykurdrniT6XiaWbz+DuL/ZzKnkiImq2ah1GdDod5s6di6FDhyIsLMzicenp6fD395dt8/f3R3p6usVzIiMj4enpKX2FhJgOnW3MegV7St87OdihbUs3k2NmrzqKRRviMGf1UZSUmp8m3ngo8PUCbd03lIiIqBGodRiJiIhAXFwcVq9eXZftAQAsWLAAGo1G+kpNTa3z16hPDvYVP9aSUh38PZzNHvdDzAVsiL2MtUfMX58KFbdxrhpNnkZERNSc1CqMzJ49Gxs3bsSuXbvQunXrKo8NCAhARkaGbFtGRgYCAgIsnqNWq+Hh4SH7ampaujkBAAaG+qB3a0+8OKELugaYX8vmw7/PYsW+JNm2hCt5iFj1r/T4ai7DCBERNU81CiNCCMyePRvr1q3Dzp07ERoaWu05Q4YMwY4dO2Tbtm3bhiFDhtSspU3M948MxKBQHzwxoj1UKhWeGtkRW+YOx4oZA0yOzcovwRv/O4Xr+RXr2CyLSpQdk8meESIiaqZqFEYiIiLw008/YdWqVXB3d0d6ejrS09NRWFgoHTN9+nQsWLBAejxnzhxs2bIFH3zwAc6cOYPXX38dhw8fxuzZs+vuKhqhsGBPrHl8CEZ3ldfLjOrqh+3PjTB7Tp83t+FCVj6EEPj934uyfVeNFtwjIiJqTmoURpYtWwaNRoORI0ciMDBQ+lqzZo10TEpKCtLSKkaK3HzzzVi1ahW+/vpr9O7dG7/99hvWr19fZdFrc9fRr4XFWzbzfz+Oc1fyTLZnVeoZ0ekElu9NQtwljcmxRERETckNzTPSUJraPCPWOHLhOu5Ztr9G5yQvvVX6ft3Ri3h2zTGT7URERI1Fg8wzQrXn28Kpxuf8dqTi1s2ZtNwqjiQiImo6GEYU0sbHFbfWcNr4F9YeQ1H5wntqR3tpu7bM/DwlRERETQHDiEJUKhU+n9YXSZGT8OfsoRaLWiPv7il73GfxNqw+lIK1hyvmJrmSW2xx4jQiIqLGjmFEYSqVCr1ae6FtS1eTfSO7tMLUgW1k2wq1ZXjpjxNI0xRJ2x7/8TB6vLYF205lVH4KIiKiRo9hpJFwtJe/FW9ODsOnU/sAAFY9OqjKc+Mu5UBbJvDq+upXQk7XFGHJptNIvVZQ+8YSERHVIYaRRqRdee9ISzcnPDi4LdydHQEAN3f0hbvaodrz03OKsOafFPx1PA1939yGL6MTTY55aPkhfL37PGb/crRuG09ERFRL1X/CUYP5ceYgfL8/GY/cYjqzbbC3C86kVz+CZv7vJ6Tvl24+g/sGhMDLtWLkTnyG/jmOpWbfeIOJiIjqAHtGGpEQH1e8clt3BHm5mOx79bbuaN/KDfPGd5G2/fe+m6p9zsRM0wnUDDKtXO/G2qloruQUWX0sERGRAcNIEzG0oy92Pj8ST43sgFWzBuHoq2Nx503B1Z53z7IYiyv+Tv58X7XnX8jKx6AlO0zWyjEmhMD+hKsYuGQH5v9+vNrnJCIiMsYw0sSoVCrc3MEX3uWrAo/v4V/NGcAzvxzF4v+dwue7EmTbL2UXIvlqfpXnvv/3WVzJLcY7W85AU6gFABSWlOGDv+NxOi0HPx+8gP5vbcc7W84AAH49fLGqpyMiIjLBmpEm7oHBbbH1ZNVDevcnZmF/YpbZfcv3JeGHmAtwsFPh5OLxUDvYy/bnFmml79/ceArv/19vRG4+jR9iLuC7vUkoKNFPwpaVz4X8iIiodtgz0sQN69QKKx8egD0vjsKMm9vV+PwfYi4AAEp1Ao9+f1jafvB8Fj7beQ5JRj0nMYlZEELgz2OXAUAKIkRERDeCPSPNwMgufgCA//QPwQ8xydDVsoZ0z7mr0vdTvj5gsj8zrxivbohDdoHWZB8REVFtMYw0I92DPHDo5XDEp+dic1waVFDhxwMXavQcmkItHOxUZveVlOrw04GUap+jtEwHB3vrO91Ky3T460QabgrxQtuWblafR0REzQPDSDPj20IN345qDO3oiwtZ+TUOI33f3Iay2natlCupYRj5bm8SIjefwcBQH/z6+JAbem0iImp6WDPSjPl7ONf4nBsNIoC+ByU2NVtaYRioeq6SmPP64tpDSdegq4PXJyKipoVhpBlzdrTHVw/2AwC8e28vjOnq1yCvO++345j8+T58FX0eJy9rsOCP4+jz5jYcTblu9vgAo9B0KbvQ4vNm5BTJAg4AaMt0eO7XWLR76S/sOM2FAomImiKVaAJTZubk5MDT0xMajQYeHh5KN6fJyi3SYvvpDHy2MwF3922NlfuTrZ6FtS509GuB7c+NkB4LIaBSqTBv7TGsPaKfn2Te+C6IGNXR5NyEK3kY//FulOkEhnZsie8fHggHezssi0qU5jgBgDWPDUbvEC84O9qbPAcRETUsaz+/2TNiQ9ydHXFXn9bY8fxIRIzqiK1zh2NSzwD0CKpZwDv22jj88MhAODnY4ZnRpsHBkjKdgBAC5zPzMHf1UYx8PwqFJWUoKtVJx7y3Nd7sufHpudItpH0JWTiYdA0AsP7oJdlxU74+gBd/s34W2OSr+VX2xtS3K7mcQp+IiAWsNszHzQlfTNPfxun52lbkFpcCAOaGd8LH289h0zPDEHM+CzeFeCLpagFeWHsMk3oGwNPFEcM7t0Lc6+Ph5GAHtaM98otL8UUVU8YD+lsqy/cl482Np6Rt205noFhb8/lK7MtH/BRoS032/XnsMj6Z2kd6bOiBqaywpAwj348CACQumSQ9Z0P55VAKFvxxAs+N7YxnxnRq0NcmImpMGEYIABCzcAw++Dse/+kfgm6BHpgb3hmAfrgwAPRr64N7+7WWnePkoO9YM9xWua1XECZ9ssfia1y8XigLIgCg0wn8fUpe61FQUgpXJ/k/zZIyeWAxxIa+bbyRes1yz8YTPx7BxewCrHtqKBwrjfAxTG8P6OtRzC1QWJ8W/KFfYfnDbWcZRojIpvE2DQEAWqgd8NrtPdAtsPY1Od1reLsHAOauiTXZlnQ1H2U6IRtZU6zVyY4pLO9N8W2htvjcxaVl2HIyHXGXctDp5c04dTlH2ieEQJnR7ZHLCt6qISKydQwj1Ohcyy/BrZ/swaRP9iDpaj7iLmlMRtH8d8c5nEnPMdlukHAlF1dy5MW5T//yLwBg26kM9H9rO7Yb9chUnlX275Pp+O0IF/0jImoIvE1D9WLWsFCkXCvAtlMZeOeeXvh4+7kqC0Wn9A/B5rg05BSVYuW+ZJxJzwUAjCqv6ajsaEo2Jnxs+ZbQH/9eMhkplJipX2dn1g/6NXhe+/OktM/4lo1OJ/DYj0cAADd3aIlAT2ezNScNpUhbhsPJ1zEw1Ee6NUZE1JzwNxvVqUW3dcfQji3x7NjO+OrB/khcMgn/1z8EKx8eUOV5KhXQI8gTALDjzJUbbofawV4aLmzs810JZo9/fu0xKbzklVQUxT7zy1GELtgk6yXZfioDT/x4BOcyci2+vqZAiylfxWDVweqnz6/O3NWxeOC7g1i6+Uz1BxMRNUEMI1SnHrklFD8/OlgqQDX0KLRwrroT7kpuMdyrOaYm9pzLNLvd0tBhoOI2To5RL8nhC/qJ2l5Yewzzy4cMP/rDYWw5mY6xH+3G+cw8s8/13d7zOJh0DQvXncCV3KJaXQMA/BCTjC0n0wEAy/cl1fp5iIgaM4YRahABHs7oXkVxbOTdPeHu7Fjt8+x/aTRGdG5V7XGGEFETB87r5y45YuHcNYdTTbbtOG2+F+e6UQ3KwLd3IL/YdAhydfKKS7FoQ8WtpAYeeUxE1GAYRqhBqFQqbJozzGT7yTfG48ybE+Dv4WxVz0iQlws+ua8Pts4dLm0bXk04eXJkB6vbeTYjFwvLh9yasyFWPsmaVqdDaZkOQghkF5Tgu71JSNMUQl2ptiPpar7VbTB4p9JtGeOhyRey8hGTmGXx3NWHUjDmgyhcyKr56xIRNTQWsFKDOvf2RHR6ebP02E1d8U/Qt4VTled6uuh7TjxdHeHp6oiDC8fg54MpmDaoDY5f1GDp5tPo08bbZBRMqK8b2rdyw/nM6j+Yz6TnIr/E8iRsc1bHyh7nFpVi+Lu7cFlTcSum8lwqQMXss4bbVtYsSFh5xeWSMh0Ons9CnzbeGPFeFABg85xhZodjv1QeqEa8F4X5E7pWG8iEEMgvKUMLNX8lEFHDY88INShHezusjxiKAe28sfaJIbJ99w1sg94hXibnzBnTCff2a40VlYpg/T2c8dzYzvD3cMbY7v7Y8fxIs4sBast0+PXxIXjjjh7Vtm/RhrgaXc+yqERZELHkn+RrGPbuLrzxP/1tl9wi+VBibVnFPCpF2jIsi0qEe6VgIIR+uvup3xyQtsWnWy6iNTBeu8eSF9YeR9hrW3HysqbaY8mUEAIJV/LqZNXr2iguLcO0bw/gkx3nFHl9ohvFMEIN7qYQL6x94mYMaOcj2+7bQo1Xbu0mPd749C348oF+mD26I97/v97o28a72ueuPMsqANzcwRe+LdR46OZ2su3fTu+Plyd1Q9QLIzH5piAApvONAMCDg9tac1lmdQ1wBwC89ddpXLxeiBX7kpGuKcJNi7fJjks3CjTf70/GO1vOSNPzV66RMa5pMTdpnCVX84qx68wV6QPzen4Jlmw6jQV/nMDv/+p7k76MPm/9xVUhI6dINslcc/fzwRSEfxiNl363fl2kurT5RDr2JWThw21nFXl9qjtbT6ZjV/yNjyhsatgnS42K1mjRvC4B7ggL9qzR+Y6VajX+fnY4Qn3dpMdjuvpJQ4fDu/tL233cLM/k+vKt3bA34apVdR9Bns6ynhIPM0W5gyN3mGw7dyUXr26IQ1R8JipPadKymttX1/NL4O1W9TEA8PK6E9h6MgPzxndBsJeL2SBTV4v2DVqiv8aoF0aindHPv7ky9EisPXIR7/1f7xt6riMXruGN/53Cy5O6YVD7lladYzz5n04nYMdq5yYpu6AEj5fPcXT2rYk2Na+Q7VwpNQmG+gcPZwezvRzVGRTqI93e2PbscHT2d5ftf3FCV3g4O+D23kGy7cHe5telOfxKOJwd7bHz+RFWvf6n9/eVvv/s/j5wU9tbdd7PB1IQFa8fjlw5D7SsJmhUHjm0P+Gq2eO2ntTPOPve1niLPSpnM3LR7qW/0O6lv/BP8jUrWl41SyOTLFHqNoe1Skp1KCnVmWyvyw+Ne5bF6GugrLi9ZmC8yGNOkWnvXl0pLdNhyabTiLLBv9wbgnG9WkFJzUfgNWUMI9SoeLs54dDLY7B/wZhane/saI/Y18YhKXISOlUKIoC+t+XgwnB8ct9Nsu1eLqY9GC9O6CKtfaNSqTCsk2+1r9+rtSfmje+CN+7ogdt6BZmtgTGnqone+rX1sbgPkPdmpGkKcf+3B02OKayiKNfY2YyKeVOetRBYquo9KSgpxa//VAyBPn4x26rXFULgwe8Oov9b25CRI6/BKS4tQ/TZTKuvob4IIXDXF/sw4r1dJm0xDgPmwoq553po+SE8+v0/Fn+eNbnNVWT0mtfyS6w+r6bWHE7F17vPY8aKf+rtNWyZ8XpcVRXSN0cMI9To+Lk739CoDns7VZXTt7s42Zvs7xIgDy7fPdQfjw1rL9v22dSKXo9VswZh3vgusv2LbusOR3s7RIzqKNWnDO1YfYCpSntfN/hU0zPyQ8wF/F0+MdoXuxJN9vu2UOO/tShsvHi9EIv/dwqHjXpI/vj3Inq98bc0rHh/4lUMXboT3+7R15q8/udJvGhUN/F9jHxE0JEL13DXF/vw7JpYpF4rkLZrCrXYc+4qrhdoTXpk3t0Sj4eWH8K8346ZtPFMeg6+jE606q9IIQS+3p2IOz7bi/OZedCW6TBn9VHc9ukeWQGxJWU6gZOXc5CmKTLpGXAy6sWzJgxk5hYj+mwmtp++gpwi820vttALY+n5DK4X1F8YSTF6z6juFZdWBJDazE3UlDGMEAEIC/ZEiI/+Vs2GiKEY080fDpVuE3m6OuL7Rwbi5UndMKR9S0SM6oj/lvewDO/cCo/cEmryvP2qKbrt0KrqeopFt3evto5jb8JVPPbjERRpy8x+qF7NK8aX0aYhxRrL9yXh3i9jEHdJP8rmuV+PIbeoFFO/OYANsZekNYfe+us0AODXw5YXF9QUaHHPshgcTcnGuqOXcO+X+6V9xn8FplcanfTdXv3MsxuPp5k854SP92Dp5jPovmir7BpTrxXgq+hEnLioweYT+vMeWfkPlmw6g+MXNRj9QTTuXbYfG2IvI+5SDo6mZFf7syg1+qvVeC0jALLi6t1nzc/+a8z4HS2tIgh9u9e6gmLjXpSsvPoLI2jcd9GavCKj1cnzbCyMsICVqNyeF0ejpFRX5f3/EZ1byUa33NE7CF0C3GVFssbs7FT47qH+mPn9Ydn2/+vXGv/XPwQ6IXDf1wfMnvv82M4Y2cUPKVmmf41unTsc4z/eLdvW9dUtFtt9ox747iBmDpWHrTmrY+Hnbrnw12D7qQyEd/fHpzvlvTMZOcUo0pbB2dEeBUa/eC9ny8OIs6Od7Je0JUs3n4EKwOMjOmDWD4elxRYBYG54J+yKl4eEYxcrhjHbW1HwWWIUGkoqBQjjxy/+fhz/GRBS9XMZ9XgUVdH78ce/l/DUyI7Vts14SHaBme79v46nIadIi6kD21T7XFXRWVngrNMJ7D6XiR5Bnmhlxb8R0is2+rdQUGxbt2kYRoiM1LQQUaVSoWuA5WnuAWBMN3/se2k07vp8Hzr7u2PFwwOk4lxz3fC39QrE2O7+uPOmYABAm5au+PKBvlA72uOWjr7IKyqFt5sTHhrS1uQ2iDVGdWklfTAfeSUc/d7aXu052QVafGBm2OgVo9sDxiM6jD36w2HMG99FFg4Mnl0Ti2UP9JN9gF6utLqzi6O9VWEEACI3n8HdfVubvNbH26u+TeVoX30Y2XC0YvbdYq0OV/OKsf1UBu64KUjWvW6NkkrzyljSqoV1H+TGH2KVn08IgYhV/0r7HhjcFuv+vYSbO7ZEa2/XmjTbpLjakp1nruDRHw7D29URRxeNq9Fr1KXcIq1Vy0zciE93nEOIjysm9wm+4ecy/nfEnhEiqnPBXi449HK4yXYnBzs8M7oj1hxOxQOD2uLxER3MBqIJYYHS94ZhvDNvaY8fDlyw+AHh4miPQjMfdP+d2gfx6bmwUwEtrfyws0bl2yvGLC1QuDlOX+sSm5otbbus0YeR4tIyfL4zQbbOj7E0TaHZ7Vfzis1ur8o/ydfRq7WXxf3L9yZhsdHMukXaMsxYcQhxl3KQcq3ApKC1ug/BEgvhofJoImdH68KxpecD5LeX3vjfKew9dxU7zlxBkKdzjQvFqxrsVFBSipJSHbxcnaQRXpbeu7oghMDcNbFooXbA23f1NNn/yY5z+HDbWXz1YD+M7xFQL204lpothfS6CSNGPSMcTUNEDem5cV1wcGE4nh7TqUY9M21auiJ20Th89WA/k3133hSE6HkjZdtGdWmFPS+OgoezIwa087E4SsfRXoWEtyci2Mv8cGdLjlk5cqay9gv+wmt/ViwIaOgZ+X5/Mj7ZmSA79oO/47FoQxw+35UgzcdQWcIV8yspV+XNjadkH+If/B0vFeUCkAURQF/jEndJX6fx14k0HDUKUwDQ8/W/sfpQCgBg1cEUDF26E+cyKnprjCfXM+71qfwBpHawx/qjl7D33FU8/+sxRG4+jcKSMly8XmD0XCWyMPJxpWLlyuHEMHLLmpmDK/vf8ctmtxdpyzD6/WgMe2cXcou08HatCGLWFAfXRuq1QmyIvYyfD6aY7V0yTAD38rqazapcE9fquFi42Ojfgq0VsLJnhKgJ83RxRHg3f5Pt/72vj+xxF393rHh4oNnn+PrBfnjsxyN4eVI33NorEEHlIeTP2UMRFZ+J59eajmIxp/K6Pdaq/Nf21bwSHE25bnZo66eVwok5a/4xXV3Z4PHh7fHVbvNFoTlFWjg72iMlq0B6ne5BHri5g+mIKOPQcMFMTQ+gXx/ovoFtsHCdfp2gl9fH4dfH9UsgGC87cM8yfSHvv6+ONfngvnCtwGROmL+Op+Hi9UJsf24EEjPzTEJZ5VmEq7vFdTYjF3/GXsbTYzpC7VD1vDjGo3aMncvIQ3r5kOyL1wtl60MVFJfB07Xu/+6tPPLE2dG6OX3qkvHNPeO1p2rL+JouZdc8LDZl7BkhauKMiy/b+7rhp5mDqjymsnE9ApC89FbMGt5eCiKA/hbOXbXoem7X0hXJS2/FihkDqj/Ygru+2I/1seb/Cq/OXjOTvt3dNxjR80bigSqm9jcMyd14ouJ17//moNkJvnItDMetyqEk/ZDlf1Ou45yZ3pu+b26TZq41OJ1mGsguXtf3HP19Kh2vrjf/V7/hdo9OJ5CVX/Vtq3Ef7cZnuxLQ5ZUt0nnZBSV48bdjsgn0dJVSo/Ff7sZrLeUWlcquL6+ebjcY1xntTbiKr3cnmrRRr/6GABmHD22Z5ddZd/QiFv/vlIX2VTC+TROfbjvLKQAMI0TNwqpZg/DoLaHYNGcYbjGanC3Q0xkAMLa7ae+JNezsVPinUq3L0I5VT1H+y2ODAQCjuvrhmTGdZPuCyttTl758oC8eNTOs2thbk8PQtqUbQnxc8cMj5nuIFpavdPzuFnl9i7kJvi5lm69XMWYu/xVpy/CulTOrVldU6+xgD1cn870BBSWliEnMQvuFmzDh4z0Wn+PxHyuN8iofbj13TSx+PXxRuvb84lJ8ESXvlfo+Jln63njW18q1PBM+2m3VfCnnM/Oknpc//r2IH4ye3xzjAs85q2OxZNMZrI+9VMUZlhlW1TanuLQMvx5ONTuzrfE7VNWMxc+uOYbl+5Kw/XRGle2Q14xYVxT9yY5zmPbtgRoVUet0App6rOepDYYRombg5g6+eOW27iZd1eueGor3/683nhrVodbPbTw084HBbfDzo4NxfskkLL7TdBVkR3sVAj0relcerrQ4YUmZwFcP9sPCSV2lKfY9nCvuFr96W/cat8/FyQHTh7Sr8hhXp4rXGF5p4UGDf1OyrR7BcNGKyb/cnEzvgoe9ttXqESnGP0dzHOxVcDHzGoD+g+zDbeaLho0Zlggw+DclG+EfRktLE5SU6bBiXxJe3RCH9/+Wj6bKM+odyimUBwNjucWlOGWmh8cgu6AE7V76C6M/iMbYj6Kh0wk89+sxLNpw0mRkFQCcy8jF3NVHTcIRIJ9B2FppmkL0f2sbXrHQy9TllS148bfj6PX63yb7jO/KPLyy+llp/61mPptio9qXqkZZGftw21nsS8jC5hPpVh0PADO//we9F/9t1arfDYVhhKgZC/B0xr39WldbC1CdPS+OwvwJXfHihK4A9D0m04e0w+nFE7Dx6Vuk4759SH5rxtvNCc+P7Sw9fmBwG4zvEYDHhndA+1YtkLz0Vhx/fTySIifh2KJxmHlLKAI85L0nu14YiVWPym89TTAaHZFfXIo2LV0R98Z4q6/njkprExnMs7I+xpriz9ziUgx8Wz5sulQncDDJujV/1NUUM+cWlcLFaLSNvZ0K7uXBLr+4FCVV3DaoSuUC4Df+dwp//Gva42Bn9EmcnlP1z2Py5/uk2zqbT6Sh3Ut/odurWxB9NhPL9yVLx2UXaGW9TrlFpcguKMFPBy5II5amfXsQ62MvY19CltnXKtKWYdq35ufuMee9LfG4XqDFzwdTrD7HwPiuizW9P19GJyK7oAQpWQV45pej2FGpp8R4Nl5zI+EqMx59VZOeEcPQ/spzFSmJBaxEVK0QH1c8OdK0d8XFyR49girmWTE3CdrjIzpIwx8dLNSuqFQqeJaPwFgXcTOe//UYHOzt0CvYE6G+bgj1dUPy0ltRWFIGtYMd7OxUuPWTPTibkYv+bfWzn1ZeQmDe+C4WhxQ/O7Yz/jxmWpOyP9H8B5y19rw4ClO+ipHCyhULBZ/WKK2mvmDvuauyv7QNc5LkFpXicnYRjlUa4WOgdrCT3Q6oLeNalEvXq79t1eO1rejbxktqc6G2DA8tP4SnKv27Sr0u73UK/zAaV/NKkF1QgtmjO1X5M/33wnVsiUuXBRVLP8YibRmc7O2wrZpbJ8YqT4qotfBz3H02E78duYg37wyT/l0bfLIjAXvOZeLclTz8eewy/tO/NUZ39cOEsEBp9Weg+n87X+9OlM2fU9vi2dIyncls00pgGCGiG6JSqfD67d2RkVssrbpszMnBDj2DPXHikga39TLfI2Es0NMFq2YNNrvPxahG4o+nbkZ+cZls7Z7Zozris10J+P6Rgbi5Q0uUlgkM62w6GibU1w3xb01A3CUN7lkWI22vPM27se8fGYhjqdnSkFFzWnu7YPPc4ej9hmmXfk1891B//HIoFUlX8y0eE3NeHpy8XB31ASZHP2OuOasfG4ywYE+sPpQiTeFfW78cSkWHVi3g7GiPNYctj2AyZu42ReXh7MbT/hv/5W7NlP2Hkq/hnn7yoms7lX4pAje1PX7/9yIGt28JR3s7jPtoNwa087ZYjKzTCSworyMyKCwpk9r788ELFocNT19+CADgprbHkkpzoCzflyR7/Ovhi/j18EWsjxiKzv4tpFtN2QVaaAq18DSziCcALNkkrz2yqyaMxCRm4ZnVR/H25DDZ9qWbz+CVWtwerWsMI0R0w2YMrbqA9Lcnh0BTqIWfe90VsKod7E1uP70wvgseH9FemnBsTngnc6dK5/dr64PkpbcC0P8FbmmOkoWTukpLAVgKI+1buel7eFwcMW1Qm2q7/dv7uuG8mbDxzfT+GNPNH8HeLlLB48B2PjhURYEkAIzu6off/7W8NhAADG6vLz5+dFj7Gw4jAOrkOVwq1TmtsvBzc7S3w/K9SWb3GZv/uzxAXM0rQe/Ff+PxEe3xVbR8WHflJQI2nUjDpJ6BEEJg8cZTJiErr6RU6umoHEQ8nB3w44ELsrqPU5dzqhxlYyzuksak5iv5ar7VK38bCnC/2X0ednYqzLwlFBey8rFw3Qk8MaIDpi8/BCGAxyoNBf9uX1KjCCPK980QUbOndrCv0yBSldpO/21uvhaDGTdXhK0XJ3Qxe8yWOcOl71+7XV7c+/DQdrLHXQPcLQalIR1alh/jgf0vjcaZNyfgh5kDEfXCSCyY2FVW7+LkYIfwbn64q08wnh7dCfcPtDx0ufKQ7+Uz+iPAwxmrHxuM/S+NNjn+8RHtTbYB5m/FmdOvrTfatax+unlL875UtuVkusnkczVROYiYs+afVPx04AI+3ZmAlfuTTfbvO6cf6myuPiSnqBSvro+TBbT0nCKzRbjmFGnLZJOeAcA3e84jKv4KFq47gSu5Rfho21kcuWA+lBZpy6Ap0OLtTafx5sZTyC3S4rlfj2FfQhYe/O6QxcJpawuq6xt7RoiIADw9uqNs5d/Fd/ZAF393hLZyk91K8HF1Mne67BgnBzv8p39raRXjRbd1x6SegdiXcBWaQi3uvCkYvVt7okOrFkjMzEOXAHf8GXsZA0N9ZLUvxvO+tPN1w+MjOmDnmQyp3qWkVCcrGo4Y1QEfbTftuXlqZAfZkG8AGN3VHwcWmg9gwzu3wvzxXXF7ryDc9uleafuKhwcgv7gUs1cdNTnnrclh2HQiTaq7+XjKTQCAYe/uMvsaBob5XRqD6LOZiK5i1eXos5m4u28wRrxX9TUZZOQUY+xH0VYdm19cZlKEuvF4GuIuaZCcVSD1GP13xzksnNTV5PxTaTkYZ1TYXVBShiPl0/JXxXg0m5IaRyuIiBTmpnbAqlmDMGPFP3hmdEeLw4XHdvfHS+W1BE4OdhZHUbx9V088Oqw92vvqb98MaOeDAe3kU/CHBXsiLNgTANB1QtULLhpU1fPjYG+HgwvH4O4v9qO1twtWPjwQiZl5Zmt5Ktv27HBExWdi+s1tpdtfYcGeiF00Fiv2JWPqwDYI8HRGwhXT4aCG9V/Cgj0x+fN9mDowBCE+rrLJ0OrD3PBOOJ+Zb7YY+fcnb5Zmt60rmXnFuHi9EGk1mErf2ts0Z9JzzBYWJ5uZ4bdyvQigr+F5xOh2aeUJ9CwJ8anZYon1RSUszfTSiOTk5MDT0xMajQYeHtb9hyUiqg1tmU5aVdmSvOJSuDja4/lfY7E+9jK6B3pg05xhDdI+IQTGfBiN85n5WHp3T9w3sE2DvK6x/x27jGv5Jfh8VwJevrWbtMI0oB9W7OpkL43u2HvuKradSpdWmL4pxEu2MKLB8M6tsLuKXonKdjw/AqEt3WBnp0JhSRm6Ldoi7fv+kYEY0bkVPtp2Fl9EJVgdCCyxt1NJw2gNyyfUp0+m9sEzv5j2PlVnxs3tzN5eqkqAhzPevbcXQn3d6iWYWPv5zTBCRFRLOUVarPv3Eib2DGiwmhiDysNMG7O84lKEvbYVgP6D1reFE67kFMvW3dkQMRSXsgvx1M//ys71beGElQ8PxD/J1/DG/ypqRgyFxwYRP/+Lv06kmewTQj+3y31fV8w9svHpW3Dn5/ukgLH2iSH4vy9jYMmLE7qYzMxrYK4w1pJBoT5WzTMT9cJIjHw/yqrndFc7ILeWi+oZhyxAf2utLlYfNmbt5zdv0xAR1ZKHsyMeqjTLbENpKkEE0M8B89WD/RCTmIVJYQFwsLczuYXj6mSPST0D8drt3eHv4Yys/BIcPJ+Fj6bcBEd7O4QFe6JboAce//EIXrm1m8lrfD6tL/5zNhPtfd1k21UqFQaF+mBkl1aIis/Eotu6o0eQB06+MR5R8Zm4pZMvWqgd8MzojvhkZwKGdfLFnnMVa/K8d28v3NO3tcUw8vToTmbDSEs3J2QZ1cPc1isQn93fF9O+PWBxwjbjn0VVAchgyV094e3qiCcrBThrPDi4LaLPZiLFaDZhDxflIgHDCBER1bvxPQIw3qjAsnLti3f5fDEPG9U9PFhpYcPB7VsidtFYixN8jbAw1b9KpcLKhwfKVtZ1drTHhLCK9jw7tjMeGNwWfh7OyC4oQQu1A4pKdVJB8eFXwtH/LfmMuodeHoMWagd8dn8fzF51FGO7+2PbKf1w7JYt5GHklVv1w2dv7RmEfQlZcLRXYdMzwzD2I9NZUL3dnNDSzXyhtLG7+wZbnEjQwLg35unRHdHZ3x3L9yXh6TEdcf5qniyMDO9k/ufXEBhGiIhIEUvu6omF607g7bvC4NvCuiHDtZ1ptLpzVSoV/MqXIvAqHzHVwqh2yLeFGqO6tJLNTWK4NXdbryBMCguETgiM/3g3PFwcZYvozRnTCQHli0ROHRgCAYF+bb3Ryd8dpxaPR/dFW2VtcbS3w+iu/gDkc6YYC+/mJ81LsmXuMNmCiH89cwv+Op6Gm0K8EBbsia+iE/HEyA7Seke3lw8Pb+3lCkDfSzO8cytFZ2JlGCEiIkXcP6gN7h/U8AW4tbV8xgCELtgEAHj//3rL9tnZqWAHFbbOHQ47lQoHk65h6jf6OpVpRteoUqkwbVBFj4+rkwM2PTMM3+w5jx2nM6R5Zlq5q/HuPb3weVQCVswYgPe2xiOvuBSDQn2w/fQVLL6zYibVrgEeCPZywaXsQjw4uC16BHmiR5CntP+NO+Wzrhp0N1rKIdir6oUZ6xsLWImIiKz0T/I15BZpy3suqnYlpwi+LdSwq+ZWikFxqX69nNr2/mgKtPBwcbD6/AtZ+RjxXhQA/S2n+ijCZgErERFRHas8V0xV/Dxq9uF+o6trV16UrzptW7phxcMD4OrYcDMkW8IwQkREZKNGdfFTugkAuDYNERERKYxhhIiIiBTFMEJERESKYhghIiIiRTGMEBERkaIYRoiIiEhRDCNERESkqBqHkd27d+P2229HUFAQVCoV1q9fX+05P//8M3r37g1XV1cEBgbikUceQVZW1asWEhERkW2ocRjJz89H79698fnnn1t1/L59+zB9+nTMnDkTJ0+exNq1a3Ho0CHMmjWrxo0lIiKi5qfGM7BOnDgREydOtPr4mJgYtGvXDs888wwAIDQ0FI8//jjeeeedmr40ERERNUP1XjMyZMgQpKamYtOmTRBCICMjA7/99hsmTZpk8Zzi4mLk5OTIvoiIiKh5qvcwMnToUPz888+YMmUKnJycEBAQAE9Pzypv80RGRsLT01P6CgkJqe9mEhERkULqPYycOnUKc+bMwaJFi3DkyBFs2bIFycnJeOKJJyyes2DBAmg0GukrNTW1vptJRERECqn3VXsjIyMxdOhQzJs3DwDQq1cvuLm5YdiwYXjrrbcQGBhoco5arYZarZYeCyEAgLdriIiImhDD57bhc9ySeg8jBQUFcHCQv4y9vT2A6htnkJubCwC8XUNERNQE5ebmwtPT0+L+GoeRvLw8JCQkSI+TkpIQGxsLHx8ftGnTBgsWLMClS5fwww8/AABuv/12zJo1C8uWLcP48eORlpaGuXPnYuDAgQgKCrLqNYOCgpCamgp3d3eoVKqaNtminJwchISEIDU1FR4eHnX2vI1Jc7/G5n59QPO/Rl5f09fcr7G5Xx9Qf9cohEBubm61n/c1DiOHDx/GqFGjpMfPPfccAOChhx7CypUrkZaWhpSUFGn/jBkzkJubi88++wzPP/88vLy8MHr06BoN7bWzs0Pr1q1r2lSreXh4NNt/YAbN/Rqb+/UBzf8aeX1NX3O/xuZ+fUD9XGNVPSIGNQ4jI0eOrPL2ysqVK022Pf3003j66adr+lJERERkA7g2DRERESnKpsOIWq3Ga6+9Jhu509w092ts7tcHNP9r5PU1fc39Gpv79QHKX6NKWDukhYiIiKge2HTPCBERESmPYYSIiIgUxTBCREREimIYISIiIkXZdBj5/PPP0a5dOzg7O2PQoEE4dOiQ0k2ySmRkJAYMGAB3d3f4+flh8uTJiI+Plx0zcuRIqFQq2VflxQlTUlJw6623wtXVFX5+fpg3bx5KS0sb8lLMev31103a3rVrV2l/UVERIiIi0LJlS7Ro0QL33HMPMjIyZM/RWK/NoF27dibXqFKpEBERAaDpvX+7d+/G7bffjqCgIKhUKqxfv162XwiBRYsWITAwEC4uLggPD8e5c+dkx1y7dg3Tpk2Dh4cHvLy8MHPmTOTl5cmOOX78OIYNGwZnZ2eEhITg3Xffre9LA1D19Wm1WsyfPx89e/aEm5sbgoKCMH36dFy+fFn2HObe86VLl8qOUer6gOrfwxkzZpi0f8KECbJjmup7CMDs/0eVSoX33ntPOqYxv4fWfC7U1e/OqKgo9O3bF2q1Gh07djQ7v1iNCRu1evVq4eTkJJYvXy5OnjwpZs2aJby8vERGRobSTavW+PHjxYoVK0RcXJyIjY0VkyZNEm3atBF5eXnSMSNGjBCzZs0SaWlp0pdGo5H2l5aWirCwMBEeHi6OHj0qNm3aJHx9fcWCBQuUuCSZ1157TfTo0UPW9szMTGn/E088IUJCQsSOHTvE4cOHxeDBg8XNN98s7W/M12Zw5coV2fVt27ZNABC7du0SQjS992/Tpk3i5ZdfFn/88YcAINatWyfbv3TpUuHp6SnWr18vjh07Ju644w4RGhoqCgsLpWMmTJggevfuLQ4cOCD27NkjOnbsKKZOnSrt12g0wt/fX0ybNk3ExcWJX375Rbi4uIivvvpK0evLzs4W4eHhYs2aNeLMmTMiJiZGDBw4UPTr10/2HG3bthWLFy+WvafG/2eVvL7qrlEIIR566CExYcIEWfuvXbsmO6apvodCCNl1paWlieXLlwuVSiUSExOlYxrze2jN50Jd/O48f/68cHV1Fc8995w4deqU+PTTT4W9vb3YsmXLDbXfZsPIwIEDRUREhPS4rKxMBAUFicjISAVbVTtXrlwRAER0dLS0bcSIEWLOnDkWz9m0aZOws7MT6enp0rZly5YJDw8PUVxcXJ/NrdZrr70mevfubXZfdna2cHR0FGvXrpW2nT59WgAQMTExQojGfW2WzJkzR3To0EHodDohRNN+/yr/otfpdCIgIEC899570rbs7GyhVqvFL7/8IoQQ4tSpUwKA+Oeff6RjNm/eLFQqlbh06ZIQQogvvvhCeHt7y65v/vz5okuXLvV8RXLmPsgqO3TokAAgLly4IG1r27at+Oijjyye01iuTwjz1/jQQw+JO++80+I5ze09vPPOO8Xo0aNl25rSe1j5c6Gufne++OKLokePHrLXmjJlihg/fvwNtdcmb9OUlJTgyJEjCA8Pl7bZ2dkhPDwcMTExCrasdjQaDQDAx8dHtv3nn3+Gr68vwsLCsGDBAhQUFEj7YmJi0LNnT/j7+0vbxo8fj5ycHJw8ebJhGl6Fc+fOISgoCO3bt8e0adOk9Y6OHDkCrVYre++6du2KNm3aSO9dY7+2ykpKSvDTTz/hkUcekS0E2ZTfP2NJSUlIT0+XvWeenp4YNGiQ7D3z8vJC//79pWPCw8NhZ2eHgwcPSscMHz4cTk5O0jHjx49HfHw8rl+/3kBXYx2NRgOVSgUvLy/Z9qVLl6Jly5bo06cP3nvvPVn3d1O4vqioKPj5+aFLly548sknkZWVJe1rTu9hRkYG/vrrL8ycOdNkX1N5Dyt/LtTV786YmBjZcxiOudHPzhqvTdMcXL16FWVlZbIfOAD4+/vjzJkzCrWqdnQ6HebOnYuhQ4ciLCxM2n7//fejbdu2CAoKwvHjxzF//nzEx8fjjz/+AACkp6ebvX7DPiUNGjQIK1euRJcuXZCWloY33ngDw4YNQ1xcHNLT0+Hk5GTyS97f319qd2O+NnPWr1+P7OxszJgxQ9rWlN+/ygztMdde4/fMz89Ptt/BwQE+Pj6yY0JDQ02ew7DP29u7XtpfU0VFRZg/fz6mTp0qW3DsmWeeQd++feHj44P9+/djwYIFSEtLw4cffgig8V/fhAkTcPfddyM0NBSJiYlYuHAhJk6ciJiYGNjb2zer9/D777+Hu7s77r77btn2pvIemvtcqKvfnZaOycnJQWFhIVxcXGrVZpsMI81JREQE4uLisHfvXtn2xx57TPq+Z8+eCAwMxJgxY5CYmIgOHTo0dDNrZOLEidL3vXr1wqBBg9C2bVv8+uuvtf6H3ph99913mDhxomyJ7ab8/tkyrVaL//znPxBCYNmyZbJ9hhXOAf2/aycnJzz++OOIjIxsEtOM33fffdL3PXv2RK9evdChQwdERUVhzJgxCras7i1fvhzTpk2Ds7OzbHtTeQ8tfS40ZjZ5m8bX1xf29vYmVcQZGRkICAhQqFU1N3v2bGzcuBG7du1C69atqzx20KBBAICEhAQAQEBAgNnrN+xrTLy8vNC5c2ckJCQgICAAJSUlyM7Olh1j/N41pWu7cOECtm/fjkcffbTK45ry+2doT1X/3wICAnDlyhXZ/tLSUly7dq3JvK+GIHLhwgVs27at2mXYBw0ahNLSUiQnJwNo/NdXWfv27eHr6yv7N9nU30MA2LNnD+Lj46v9Pwk0zvfQ0udCXf3utHSMh4fHDf2xaJNhxMnJCf369cOOHTukbTqdDjt27MCQIUMUbJl1hBCYPXs21q1bh507d5p0C5oTGxsLAAgMDAQADBkyBCdOnJD98jD8Au3evXu9tLu28vLykJiYiMDAQPTr1w+Ojo6y9y4+Ph4pKSnSe9eUrm3FihXw8/PDrbfeWuVxTfn9Cw0NRUBAgOw9y8nJwcGDB2XvWXZ2No4cOSIds3PnTuh0OimIDRkyBLt374ZWq5WO2bZtG7p06aJ4974hiJw7dw7bt29Hy5Ytqz0nNjYWdnZ20q2Nxnx95ly8eBFZWVmyf5NN+T00+O6779CvXz/07t272mMb03tY3edCXf3uHDJkiOw5DMfc8GfnDZW/NmGrV68WarVarFy5Upw6dUo89thjwsvLS1ZF3Fg9+eSTwtPTU0RFRcmGmBUUFAghhEhISBCLFy8Whw8fFklJSWLDhg2iffv2Yvjw4dJzGIZwjRs3TsTGxootW7aIVq1aNYrhr88//7yIiooSSUlJYt++fSI8PFz4+vqKK1euCCH0w9PatGkjdu7cKQ4fPiyGDBkihgwZIp3fmK/NWFlZmWjTpo2YP3++bHtTfP9yc3PF0aNHxdGjRwUA8eGHH4qjR49Ko0mWLl0qvLy8xIYNG8Tx48fFnXfeaXZob58+fcTBgwfF3r17RadOnWTDQrOzs4W/v7948MEHRVxcnFi9erVwdXVtkGGTVV1fSUmJuOOOO0Tr1q1FbGys7P+kYQTC/v37xUcffSRiY2NFYmKi+Omnn0SrVq3E9OnTG8X1VXeNubm54oUXXhAxMTEiKSlJbN++XfTt21d06tRJFBUVSc/RVN9DA41GI1xdXcWyZctMzm/s72F1nwtC1M3vTsPQ3nnz5onTp0+Lzz//nEN7b9Snn34q2rRpI5ycnMTAgQPFgQMHlG6SVQCY/VqxYoUQQoiUlBQxfPhw4ePjI9RqtejYsaOYN2+ebJ4KIYRITk4WEydOFC4uLsLX11c8//zzQqvVKnBFclOmTBGBgYHCyclJBAcHiylTpoiEhARpf2FhoXjqqaeEt7e3cHV1FXfddZdIS0uTPUdjvTZjW7duFQBEfHy8bHtTfP927dpl9t/kQw89JITQD+999dVXhb+/v1Cr1WLMmDEm152VlSWmTp0qWrRoITw8PMTDDz8scnNzZcccO3ZM3HLLLUKtVovg4GCxdOlSxa8vKSnJ4v9Jw7wxR44cEYMGDRKenp7C2dlZdOvWTSxZskT2Qa7k9VV3jQUFBWLcuHGiVatWwtHRUbRt21bMmjXL5I+3pvoeGnz11VfCxcVFZGdnm5zf2N/D6j4XhKi73527du0SN910k3BychLt27eXvUZtqcovgoiIiEgRNlkzQkRERI0HwwgREREpimGEiIiIFMUwQkRERIpiGCEiIiJFMYwQERGRohhGiIiISFEMI0RERKQohhEiIiJSFMMIERERKYphhIiIiBTFMEJERESK+n8CuPdigUZiVQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc92eac22e0>]"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.train_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc92e7fee20>]"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.val_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
